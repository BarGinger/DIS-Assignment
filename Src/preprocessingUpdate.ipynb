{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Leh3A2PxFNSS",
        "outputId": "087586b7-2357-4142-dc3e-3389a90190c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "openjdk-8-jdk-headless is already the newest version (8u422-b05-1~22.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Requirement already satisfied: graphframes in /usr/local/lib/python3.10/dist-packages (0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from graphframes) (1.26.4)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.10/dist-packages (from graphframes) (1.3.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "!pip install graphframes\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import (\n",
        "    col,\n",
        "    udf,\n",
        "    row_number,\n",
        "    countDistinct,\n",
        "    collect_list,\n",
        "    struct,\n",
        "    count,\n",
        "    sum,\n",
        "    avg,\n",
        "    expr,\n",
        "    percentile_approx,\n",
        "    max as spark_max\n",
        ")\n",
        "from pyspark.sql.types import StringType, IntegerType, BinaryType, DoubleType, ArrayType, StructType, StructField\n",
        "from pyspark.sql import Window\n",
        "from datetime import datetime\n",
        "from graphframes import GraphFrame\n",
        "from scipy.sparse import csr_matrix, vstack, hstack\n",
        "from pyspark.sql.functions import least, greatest, col\n",
        "import numpy as np\n",
        "import pickle\n",
        "import base64"
      ],
      "metadata": {
        "id": "f63Y6XuBFSJE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, least, greatest, udf, countDistinct, row_number, collect_list, struct\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import DoubleType, StringType\n",
        "from pyspark.sql import Window\n",
        "from graphframes import GraphFrame\n",
        "from datetime import datetime\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PhoneCallsCommunityDetection\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.1-s_2.12\") \\\n",
        "    .config(\"spark.executor.memory\", \"20G\") \\\n",
        "    .config(\"spark.driver.memory\", \"50G\") \\\n",
        "    .config(\"spark.executor.memoryOverhead\", \"1G\") \\\n",
        "    .config(\"spark.default.parallelism\", \"100\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
        "    .config(\"spark.driver.maxResultSize\", \"2G\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Optional: Set logging level to reduce verbosity\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "# Set a checkpoint directory for Spark\n",
        "spark.sparkContext.setCheckpointDir(\"/tmp/spark-checkpoints\")\n",
        "\n",
        "file_path = '/content/toy_dataset1.csv'  # Adjust this to your file path\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "# Convert YYMMDDHHMM to a proper datetime object\n",
        "def convert_to_datetime(yyMMddHHMM):\n",
        "    return datetime.strptime(str(yyMMddHHMM), '%y%m%d%H%M')\n",
        "\n",
        "# Define UDF for calculating duration in minutes\n",
        "def calculate_duration_minutes(start_time, end_time):\n",
        "    start_dt = convert_to_datetime(start_time)\n",
        "    end_dt = convert_to_datetime(end_time)\n",
        "    duration = end_dt - start_dt\n",
        "    return duration.total_seconds() / 60\n",
        "\n",
        "# Register the UDF for duration in minutes\n",
        "calculate_duration_minutes_udf = udf(calculate_duration_minutes, DoubleType())\n",
        "\n",
        "# Add column for duration in minutes\n",
        "df = df.withColumn('duration_minutes', calculate_duration_minutes_udf(col('Start_Time'), col('End_Time')))\n",
        "\n",
        "# Adjust Client1 and Client2 to ensure Client1 is the smaller value and Client2 the larger\n",
        "df = df.withColumn(\"Client1_min\", least(col(\"Client1\"), col(\"Client2\"))) \\\n",
        "       .withColumn(\"Client2_max\", greatest(col(\"Client1\"), col(\"Client2\"))) \\\n",
        "       .drop(\"Client1\", \"Client2\") \\\n",
        "       .withColumnRenamed(\"Client1_min\", \"Client1\") \\\n",
        "       .withColumnRenamed(\"Client2_max\", \"Client2\")\n",
        "\n",
        "# Aggregate total duration for each unique pair (Client1, Client2)\n",
        "df_aggregated = df.groupBy(\"Client1\", \"Client2\") \\\n",
        "    .agg(F.sum(\"duration_minutes\").alias(\"total_duration_minutes\"))\n",
        "\n",
        "# Join the aggregated total duration back to the original DataFrame\n",
        "df = df.drop(\"duration_minutes\") \\\n",
        "       .join(df_aggregated, on=[\"Client1\", \"Client2\"], how=\"left\")\n",
        "\n",
        "# Create Graph using GraphFrames for community detection\n",
        "vertices = df.selectExpr(\"Client1 as id\").union(df.selectExpr(\"Client2 as id\")).distinct()\n",
        "edges = df.selectExpr(\"Client1 as src\", \"Client2 as dst\", \"total_duration_minutes as weight\")\n",
        "\n",
        "# Cache vertices and edges\n",
        "vertices.cache()\n",
        "edges.cache()\n",
        "\n",
        "# Create a GraphFrame\n",
        "g = GraphFrame(vertices, edges)\n",
        "\n",
        "# Find connected components (communities) using GraphFrames\n",
        "result = g.connectedComponents()\n",
        "\n",
        "# Create a mapping from original community IDs to sequential ones\n",
        "community_mapping = result.select(\"component\").distinct() \\\n",
        "    .orderBy(\"component\") \\\n",
        "    .withColumn(\"new_id\", row_number().over(Window.orderBy(\"component\"))) \\\n",
        "    .cache()\n",
        "\n",
        "# Join the result (community IDs) with the original DataFrame and map to new sequential IDs\n",
        "df_with_communities = df.join(result, df['Client1'] == result['id'], 'inner') \\\n",
        "    .join(community_mapping, result['component'] == community_mapping['component'], 'inner') \\\n",
        "    .drop(result['id']) \\\n",
        "    .drop(community_mapping['component']) \\\n",
        "    .withColumnRenamed('new_id', 'community_id')\n",
        "\n",
        "# Calculate the number of unique clients (community size) per community\n",
        "community_sizes = df_with_communities.select(\"community_id\", \"Client1\").union(df_with_communities.select(\"community_id\", \"Client2\")) \\\n",
        "    .distinct() \\\n",
        "    .groupBy(\"community_id\").agg(countDistinct(\"Client1\").alias(\"community_size\"))\n",
        "\n",
        "# Merge the community sizes into the main DataFrame\n",
        "df_final = df_with_communities.join(community_sizes, 'community_id')\n",
        "\n",
        "# Create community_members with unique tuples for each community\n",
        "community_members = df_final.select(\"community_id\", \"Client1\", \"Client2\", \"total_duration_minutes\") \\\n",
        "    .distinct() \\\n",
        "    .groupBy(\"community_id\") \\\n",
        "    .agg(F.collect_list(F.struct(\n",
        "        F.col(\"Client1\"),\n",
        "        F.col(\"Client2\"),\n",
        "        F.col(\"total_duration_minutes\")\n",
        "    )).alias(\"members\")) \\\n",
        "    .orderBy(\"community_id\")\n",
        "\n",
        "# Show the final DataFrame with community IDs, duration, and community sizes\n",
        "print(\"\\nFinal DataFrame with Sequential Community IDs:\")\n",
        "df_final.select(\n",
        "    'Client1',\n",
        "    'Client2',\n",
        "    'Start_Time',\n",
        "    'End_Time',\n",
        "    'total_duration_minutes',\n",
        "    'community_id',\n",
        "    'community_size'\n",
        ").orderBy(\"community_id\").show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9UKEdnQdEC5",
        "outputId": "2c02e2f7-242c-4afa-ce2c-c8d6a416056a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final DataFrame with Sequential Community IDs:\n",
            "+-------+-------+----------+----------+----------------------+------------+--------------+\n",
            "|Client1|Client2|Start_Time|  End_Time|total_duration_minutes|community_id|community_size|\n",
            "+-------+-------+----------+----------+----------------------+------------+--------------+\n",
            "|      1|      2|2408040000|2408040500|                 420.0|           1|             2|\n",
            "|      1|      2|2408060000|2408060200|                 420.0|           1|             2|\n",
            "|      4|      5|2408090000|2408091500|                1260.0|           2|             2|\n",
            "|      4|      5|2408020000|2408020600|                1260.0|           2|             2|\n",
            "|      6|      7|2408070000|2408070800|                 480.0|           3|             2|\n",
            "|      8|      9|2408070000|2408070500|                 480.0|           4|             2|\n",
            "|      8|      9|2408090000|2408090300|                 480.0|           4|             2|\n",
            "|     10|     11|2408010000|2408010400|                 240.0|           5|             2|\n",
            "|     12|     14|2408020000|2408020800|                 480.0|           6|             3|\n",
            "|     12|     13|2408010000|2408010200|                 120.0|           6|             3|\n",
            "|     13|     14|2408030000|2408030500|                 300.0|           6|             3|\n",
            "+-------+-------+----------+----------+----------------------+------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define community_members to get a list of unique (Client1, Client2, total_duration_minutes) tuples for each community\n",
        "community_members = df_final.select(\"community_id\", \"Client1\", \"Client2\", \"total_duration_minutes\") \\\n",
        "    .distinct() \\\n",
        "    .groupBy(\"community_id\") \\\n",
        "    .agg(F.collect_list(F.struct(\n",
        "        F.col(\"Client1\"),\n",
        "        F.col(\"Client2\"),\n",
        "        F.col(\"total_duration_minutes\")\n",
        "    )).alias(\"members\")) \\\n",
        "    .orderBy(\"community_id\")\n",
        "\n",
        "# Show the list of community members as tuples\n",
        "print(\"\\nCommunity Members with Sequential IDs:\")\n",
        "community_members.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jL1vjgdhUn4",
        "outputId": "17c999cb-2c85-46e5-8ca0-d28a5cd34e66"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Community Members with Sequential IDs:\n",
            "+------------+---------------------------------------------------+\n",
            "|community_id|members                                            |\n",
            "+------------+---------------------------------------------------+\n",
            "|1           |[{1, 2, 420.0}]                                    |\n",
            "|2           |[{4, 5, 1260.0}]                                   |\n",
            "|3           |[{6, 7, 480.0}]                                    |\n",
            "|4           |[{8, 9, 480.0}]                                    |\n",
            "|5           |[{10, 11, 240.0}]                                  |\n",
            "|6           |[{12, 14, 480.0}, {12, 13, 120.0}, {13, 14, 300.0}]|\n",
            "+------------+---------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CvHNqIi2ha4Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}