{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BarGinger/DIS-Assignment/blob/main/Src/dis_notebook_02_11_24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QFDENKMNs-52",
        "outputId": "f2f3c459-c16c-47b0-e8be-62040e003ec3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra fonts-nanum fonts-ipafont-gothic\n",
            "  fonts-ipafont-mincho fonts-wqy-microhei fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 39.6 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 123623 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u422-b05-1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u422-b05-1~22.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u422-b05-1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u422-b05-1~22.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u422-b05-1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u422-b05-1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "Collecting graphframes\n",
            "  Downloading graphframes-0.6-py2.py3-none-any.whl.metadata (934 bytes)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from graphframes) (1.26.4)\n",
            "Collecting nose (from graphframes)\n",
            "  Downloading nose-1.3.7-py3-none-any.whl.metadata (1.7 kB)\n",
            "Downloading graphframes-0.6-py2.py3-none-any.whl (18 kB)\n",
            "Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nose, graphframes\n",
            "Successfully installed graphframes-0.6 nose-1.3.7\n",
            "Collecting sparkmeasure\n",
            "  Downloading sparkmeasure-0.24.0-py2.py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading sparkmeasure-0.24.0-py2.py3-none-any.whl (5.8 kB)\n",
            "Installing collected packages: sparkmeasure\n",
            "Successfully installed sparkmeasure-0.24.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "!pip install graphframes\n",
        "!pip install sparkmeasure\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import (\n",
        "    col,\n",
        "    udf,\n",
        "    row_number,\n",
        "    countDistinct,\n",
        "    collect_list,\n",
        "    struct,\n",
        "    count,\n",
        "    sum,\n",
        "    avg,\n",
        "    expr,\n",
        "    percentile_approx,\n",
        "    max as spark_max,\n",
        "    explode,\n",
        "    round\n",
        ")\n",
        "from pyspark.sql.types import StringType, IntegerType, BinaryType, DoubleType, ArrayType, StructType, StructField\n",
        "from pyspark.sql import Window\n",
        "from datetime import datetime\n",
        "from graphframes import GraphFrame\n",
        "from scipy.sparse import csr_matrix, vstack, hstack\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import base64\n",
        "from sparkmeasure import StageMetrics # for resources monitoring\n",
        "from functools import wraps"
      ],
      "metadata": {
        "id": "gjrP64v5QyEd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Monitor CPU, Memory and running time\n",
        "def format_memory(report_memory_output):\n",
        "  # Initialize a list to store parsed data\n",
        "    parsed_data = []\n",
        "\n",
        "    # Iterate through each line in report_memory_output and parse data\n",
        "    for line in report_memory_output.split('\\n'):\n",
        "        if line.startswith(\"Stage\"):\n",
        "            # Split the line into parts\n",
        "            parts = line.split()\n",
        "\n",
        "            # Extract information\n",
        "            stage = parts[1]\n",
        "            metric = parts[2]\n",
        "            raw_value = int(parts[6])  # Raw value in bytes (integer)\n",
        "\n",
        "            # Extract formatted value and units\n",
        "            formatted_value_with_units = \" \".join(parts[7:]).replace(\"(\", \"\").replace(\")\", \"\")\n",
        "            formatted_value, units = formatted_value_with_units.split(\" \", 1)\n",
        "\n",
        "            # Append the extracted information to parsed_data\n",
        "            parsed_data.append({\n",
        "                \"stageId\": stage,\n",
        "                \"memory_metric\": metric,\n",
        "                \"memory_raw_value_bytes\": raw_value,\n",
        "                \"memory_formatted_value\": formatted_value,\n",
        "                \"memory_units\": units\n",
        "            })\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(parsed_data)\n",
        "    return spark.createDataFrame(df)\n",
        "\n",
        "\n",
        "def track_stage(stage_name):\n",
        "    def decorator(func):\n",
        "        @wraps(func)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            print(f\"Starting {stage_name}\")\n",
        "            stagemetrics.begin()  # Begin collecting metrics for this stage\n",
        "\n",
        "            result = func(*args, **kwargs)  # Run the actual function\n",
        "\n",
        "            stagemetrics.end()  # Stop collecting metrics for this stage\n",
        "\n",
        "            time.sleep(15)\n",
        "\n",
        "            # stagemetrics.print_report()\n",
        "\n",
        "            # Print or retrieve the metrics summary for the stage\n",
        "            df_metrics = stagemetrics.create_stagemetrics_DF()\n",
        "            df_metrics.show(truncate=False)\n",
        "            print(f\"Completed {stage_name}\\n\")\n",
        "\n",
        "            memory_info = stagemetrics.report_memory()\n",
        "            df_memory = format_memory(memory_info)\n",
        "\n",
        "            # Add stage_name column to df_memmory\n",
        "            df_metrics = df_metrics.withColumn(\"stage_name\", pyspark.sql.functions.lit(stage_name))\n",
        "            # Join df_metrics and df_memmory\n",
        "            df_metrics = df_metrics.join(df_memmory, on=['stageId'], how='left') # Assuming 'Stage' is the common column\n",
        "\n",
        "            if \"Stage 1\" in stage_name:\n",
        "                write_mode = \"overwrite\"\n",
        "            else:\n",
        "                write_mode = \"append\"\n",
        "\n",
        "            # If clear_csv is True, overwrite the file; otherwise, append\n",
        "            if write_mode == \"overwrite\":\n",
        "                df_metrics.coalesce(1).write.mode(write_mode).option(\"header\", \"true\").csv(\"stage_metrics\")\n",
        "            else:\n",
        "                df_metrics.write.mode(write_mode).csv(\"stage_metrics\")\n",
        "\n",
        "            df_metrics.show()\n",
        "\n",
        "            return result\n",
        "        return wrapper\n",
        "    return decorator"
      ],
      "metadata": {
        "id": "R9jMJMWzY2LP"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utils functions\n",
        "\n",
        "# Convert YYMMDDHHMM to a proper datetime object\n",
        "def convert_to_datetime(yyMMddHHMM):\n",
        "    return datetime.strptime(str(yyMMddHHMM), '%y%m%d%H%M')\n",
        "\n",
        "# Define UDF for calculating duration in minutes\n",
        "def calculate_duration_minutes(start_time, end_time):\n",
        "    start_dt = convert_to_datetime(start_time)\n",
        "    end_dt = convert_to_datetime(end_time)\n",
        "    duration = end_dt - start_dt\n",
        "    return duration.total_seconds() / 60\n",
        "\n",
        "# Define UDF for calculating duration in DDHHMM format\n",
        "def calculate_duration_string(start_time, end_time):\n",
        "    start_dt = convert_to_datetime(start_time)\n",
        "    end_dt = convert_to_datetime(end_time)\n",
        "    duration = end_dt - start_dt\n",
        "\n",
        "    days = duration.days\n",
        "    hours, remainder = divmod(duration.seconds, 3600)\n",
        "    minutes = remainder // 60\n",
        "    return f'{days:02d}{hours:02d}{minutes:02d}'\n",
        "\n",
        "# prompt: print csr_matrix_result pretty\n",
        "def pretty_print_csr_matrix(csr_matrix_result):\n",
        "  \"\"\"Prints a CSR matrix in a readable format.\"\"\"\n",
        "\n",
        "  rows, cols = csr_matrix_result.nonzero()\n",
        "  data = csr_matrix_result.data\n",
        "\n",
        "  df = pd.DataFrame({\n",
        "      'Row': rows,\n",
        "      'Col': cols,\n",
        "      'Value': data\n",
        "  })\n",
        "\n",
        "  print(df)\n",
        "\n",
        "def create_csr_matrix_from_edges_with_spark(members_df):\n",
        "    \"\"\"\n",
        "    Creates a CSR matrix from a Spark DataFrame based on unique vertices.\n",
        "\n",
        "    Args:\n",
        "        members_df: Spark DataFrame with 'community_id' and 'members' columns.\n",
        "\n",
        "    Returns:\n",
        "        A CSR matrix.\n",
        "    \"\"\"\n",
        "\n",
        "    # Explode the members array to get each connection in separate rows\n",
        "    exploded_df = members_df.select(\n",
        "        \"community_id\",\n",
        "        explode(\"members\").alias(\"member\")\n",
        "    ).select(\n",
        "        \"community_id\",\n",
        "        col(\"member.Client1\").alias(\"Client1\"),\n",
        "        col(\"member.Client2\").alias(\"Client2\"),\n",
        "        col(\"member.duration_minutes\").alias(\"duration_minutes\")\n",
        "    )\n",
        "\n",
        "    # Get unique clients and create a mapping to indices\n",
        "    unique_clients = exploded_df.select(\"Client1\").union(exploded_df.select(\"Client2\")).distinct().rdd.flatMap(lambda x: x).collect()\n",
        "    client_to_index = {client: i for i, client in enumerate(unique_clients)}\n",
        "    num_clients = len(unique_clients)\n",
        "\n",
        "    # Extract data for CSR matrix\n",
        "    rows = exploded_df.select(\"Client1\").rdd.map(lambda row: client_to_index[row[0]]).collect()\n",
        "    cols = exploded_df.select(\"Client2\").rdd.map(lambda row: client_to_index[row[0]]).collect()\n",
        "    data = exploded_df.select(\"duration_minutes\").rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "    # Create CSR matrix\n",
        "    csr = csr_matrix((data, (rows, cols)), shape=(num_clients, num_clients))\n",
        "\n",
        "    return csr\n",
        "\n",
        "# create csr matrix from given members list\n",
        "def create_csr_matrix(members, use_weights=False):\n",
        "    clients = list(set([member['Client1'] for member in members] + [member['Client2'] for member in members]))\n",
        "    client_index = {client: idx for idx, client in enumerate(clients)}\n",
        "\n",
        "    row_indices = []\n",
        "    col_indices = []\n",
        "    data = []\n",
        "\n",
        "    for member in members:\n",
        "        row_indices.append(client_index[member['Client1']])\n",
        "        col_indices.append(client_index[member['Client2']])\n",
        "        if use_weights:\n",
        "            data.append(float(member['duration_minutes']))  # Use duration in minutes as the weight of the edge\n",
        "        else:\n",
        "            data.append(1)  # Use 1 for unweighted similarity\n",
        "\n",
        "    num_clients = len(clients)\n",
        "    csr = csr_matrix((data, (row_indices, col_indices)), shape=(num_clients, num_clients))\n",
        "\n",
        "    # Serialize the CSR matrix\n",
        "    serialized_csr = base64.b64encode(pickle.dumps(csr)).decode('utf-8')\n",
        "    return serialized_csr\n",
        "\n",
        "# compare given two csr matrices (each relating to a community) to get similarity score\n",
        "def compare_weighted_structural_similarity(csr_matrix_1, csr_matrix_2):\n",
        "    # Deserialize CSR matrices\n",
        "    csr_1 = pickle.loads(base64.b64decode(csr_matrix_1))\n",
        "    csr_2 = pickle.loads(base64.b64decode(csr_matrix_2))\n",
        "\n",
        "\n",
        "    # Align matrix dimensions to the largest size\n",
        "    max_rows = max(csr_1.shape[0], csr_2.shape[0])\n",
        "    max_cols = max(csr_1.shape[1], csr_2.shape[1])\n",
        "\n",
        "    # Pad csr_1 to match max dimensions\n",
        "    if csr_1.shape[0] < max_rows or csr_1.shape[1] < max_cols:\n",
        "        csr_1 = vstack([csr_1, csr_matrix((max_rows - csr_1.shape[0], csr_1.shape[1]))]) if csr_1.shape[0] < max_rows else csr_1\n",
        "        csr_1 = hstack([csr_1, csr_matrix((csr_1.shape[0], max_cols - csr_1.shape[1]))]) if csr_1.shape[1] < max_cols else csr_1\n",
        "\n",
        "    # Pad csr_2 to match max dimensions\n",
        "    if csr_2.shape[0] < max_rows or csr_2.shape[1] < max_cols:\n",
        "        csr_2 = vstack([csr_2, csr_matrix((max_rows - csr_2.shape[0], csr_2.shape[1]))]) if csr_2.shape[0] < max_rows else csr_2\n",
        "        csr_2 = hstack([csr_2, csr_matrix((csr_2.shape[0], max_cols - csr_2.shape[1]))]) if csr_2.shape[1] < max_cols else csr_2\n",
        "\n",
        "    # Calculate structural similarity (e.g., using cosine similarity)\n",
        "    dot_product = csr_1.multiply(csr_2).sum()\n",
        "    norm_1 = np.sqrt(csr_1.multiply(csr_1).sum())\n",
        "    norm_2 = np.sqrt(csr_2.multiply(csr_2).sum())\n",
        "    similarity = dot_product / (norm_1 * norm_2) if norm_1 != 0 and norm_2 != 0 else 0\n",
        "    return float(similarity)"
      ],
      "metadata": {
        "id": "mhgOioE4ZU5t"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the\n",
        "@track_stage(\"Stage 1: Reading the calls dataset\")\n",
        "def read_csv_to_dataframe(file_path= 'toy_dataset.csv'):\n",
        "  \"\"\"\n",
        "  Read dataset from given path into a Spark DataFrame.\n",
        "  Parameters:\n",
        "    -----------\n",
        "    file_path : str\n",
        "        The name of the given dataset (unigrams or bigrams or both).\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    df_dataset : DataFrame\n",
        "        A DataFrame of calls with the given dataset info.\n",
        "  \"\"\"\n",
        "  df_dataset = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "  # convert start - end times to duration\n",
        "  # 1st Register the UDFs in Spark\n",
        "  calculate_duration_minutes_udf = udf(calculate_duration_minutes, DoubleType())\n",
        "  calculate_duration_string_udf = udf(calculate_duration_string, StringType())\n",
        "\n",
        "  # 2nd use udfs to add columns for duration in minutes and DDHHMM format\n",
        "  df_dataset = df_dataset.withColumn('duration_minutes', calculate_duration_minutes_udf(col('Start_Time'), col('End_Time')))\n",
        "  df_dataset = df_dataset.withColumn('duration_DDHHMM', calculate_duration_string_udf(col('Start_Time'), col('End_Time')))\n",
        "\n",
        "  print(\"The following dataframe has been read from the CSV file:\")\n",
        "  df_dataset.show()\n",
        "  return df_dataset\n",
        "\n",
        "@track_stage(\"Stage 2: Preprocessing and creating the graph\")\n",
        "def create_graph_from_dataframe(df_dataset):\n",
        "  \"\"\"\n",
        "  Create graph in GraphFrame from the calls in the current dataset.\n",
        "  Parameters:\n",
        "    -----------\n",
        "    df_dataset : DataFrame\n",
        "        A DataFrame of calls with the given dataset info.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    df_dataset : DataFrame\n",
        "        A DataFrame of calls with the given dataset info.\n",
        "  \"\"\"\n",
        "\n",
        "  # Create Graph using GraphFrames for community detection\n",
        "  vertices = df_dataset.selectExpr(\"Client1 as id\").union(df_dataset.selectExpr(\"Client2 as id\")).distinct()\n",
        "  edges = df_dataset.selectExpr(\"Client1 as src\", \"Client2 as dst\", \"duration_minutes as weight\")\n",
        "\n",
        "  # Cache vertices and edges\n",
        "  vertices.cache()\n",
        "  edges.cache()\n",
        "\n",
        "  # Create a GraphFrame\n",
        "  g = GraphFrame(vertices, edges)\n",
        "\n",
        "  # Find connected components (communities) using GraphFrames\n",
        "  connected_components_result = g.connectedComponents()\n",
        "\n",
        "  # Create a mapping from original community IDs to sequential ones\n",
        "  community_mapping = connected_components_result.select(\"component\").distinct() \\\n",
        "      .orderBy(\"component\") \\\n",
        "      .withColumn(\"new_id\", row_number().over(Window.orderBy(\"component\"))) \\\n",
        "      .cache()\n",
        "\n",
        "  # Join the result (community IDs) with the original dataframe and map to new sequential IDs\n",
        "  df_with_communities = df_dataset.join(connected_components_result, df_dataset['Client1'] == connected_components_result['id'], 'inner') \\\n",
        "      .join(community_mapping, connected_components_result['component'] == community_mapping['component'], 'inner') \\\n",
        "      .drop(connected_components_result['id']) \\\n",
        "      .drop(community_mapping['component']) \\\n",
        "      .withColumnRenamed('new_id', 'community_id')\n",
        "\n",
        "  # Calculate the number of unique clients (community size) per community\n",
        "  community_sizes = df_with_communities.select(\"community_id\", \"Client1\").union(df_with_communities.select(\"community_id\", \"Client2\")) \\\n",
        "      .distinct() \\\n",
        "      .groupBy(\"community_id\").agg(countDistinct(\"Client1\").alias(\"community_size\"))\n",
        "\n",
        "  # Merge the community sizes into the main DataFrame\n",
        "  df_final = df_with_communities.join(community_sizes, 'community_id')\n",
        "\n",
        "  # Get list of tuples for each community member by considering both Client1 and Client2\n",
        "  community_members = df_final.select(\"community_id\", \"Client1\", \"Client2\", \"duration_DDHHMM\", \"duration_minutes\") \\\n",
        "      .distinct() \\\n",
        "      .groupBy(\"community_id\") \\\n",
        "      .agg(collect_list(struct(col(\"Client1\"),\n",
        "                            col(\"Client2\"),\n",
        "                            col(\"duration_DDHHMM\"),\n",
        "                            col(\"duration_minutes\"))).alias(\"members\")) \\\n",
        "      .orderBy(\"community_id\")\n",
        "\n",
        "  # Show the final DataFrame with community IDs, duration, and community sizes\n",
        "  print(\"\\nFinal DataFrame with Sequential Community IDs:\")\n",
        "  df_final.select('Client1',\n",
        "                  'Client2',\n",
        "                  'duration_DDHHMM',\n",
        "                  'duration_minutes',\n",
        "                  'community_id',\n",
        "                  'community_size') \\\n",
        "      .orderBy(\"community_id\") \\\n",
        "      .show()\n",
        "\n",
        "  # Show the list of community members as tuples\n",
        "  print(\"\\nCommunity Members with Sequential IDs:\")\n",
        "  community_members.show(truncate=False)\n",
        "\n",
        "  # Save results to CSV files\n",
        "  # Save the main analysis results\n",
        "  df_final.select('Client1',\n",
        "                  'Client2',\n",
        "                  'duration_DDHHMM',\n",
        "                  'duration_minutes',\n",
        "                  'community_id',\n",
        "                  'community_size') \\\n",
        "      .orderBy(\"community_id\") \\\n",
        "      .write.mode(\"overwrite\").csv(\"community_analysis_results\")\n",
        "\n",
        "  # Save community members in a flattened format\n",
        "  df_final.select('community_id',\n",
        "                  'Client1',\n",
        "                  'Client2',\n",
        "                  'duration_DDHHMM',\n",
        "                  'duration_minutes') \\\n",
        "      .distinct() \\\n",
        "      .orderBy(\"community_id\") \\\n",
        "      .write.mode(\"overwrite\").csv(\"community_members_results\")\n",
        "\n",
        "  # Optionally, if you want to save additional community statistics\n",
        "  community_stats = df_final.groupBy('community_id') \\\n",
        "      .agg(\n",
        "          countDistinct('Client1', 'Client2').alias('unique_members'),\n",
        "          count('*').alias('total_calls'),\n",
        "          sum('duration_minutes').alias('total_duration_minutes'),\n",
        "          avg('duration_minutes').alias('avg_call_duration'),\n",
        "          percentile_approx('duration_minutes', 0.25).alias('duration_25th_percentile'),\n",
        "          percentile_approx('duration_minutes', 0.5).alias('median_call_duration'),\n",
        "          percentile_approx('duration_minutes', 0.75).alias('duration_75th_percentile')\n",
        "      ) \\\n",
        "      .orderBy('community_id')\n",
        "\n",
        "  community_stats.write.mode(\"overwrite\").csv(\"community_statistics_results\")\n",
        "\n",
        "  print(\"This is the community stats:\")\n",
        "  community_stats.show(truncate=False)\n",
        "  return df_final, community_members, community_stats\n",
        "\n",
        "# Create CSR adjacency matrices for each community and serialize them\n",
        "@track_stage(\"Stage 3: Creating CSR matrices\")\n",
        "def format_members_to_csr_matrix(community_members):\n",
        "  \"\"\"\n",
        "  Create CSR adjacency matrices for each community and serialize them.\n",
        "\n",
        "  Parameters:\n",
        "    community_members: Dataframe\n",
        "    A dataframe of a specific community's members\n",
        "  \"\"\"\n",
        "  # Convert the collected list of Row objects to a list of dictionaries before passing to UDF\n",
        "  schema = StructType([\n",
        "      StructField(\"Client1\", StringType(), True),\n",
        "      StructField(\"Client2\", StringType(), True),\n",
        "      StructField(\"duration_DDHHMM\", StringType(), True),\n",
        "      StructField(\"duration_minutes\", DoubleType(), True)\n",
        "  ])\n",
        "  convert_members_udf = udf(lambda members: [member.asDict() for member in members], ArrayType(schema))\n",
        "  community_members = community_members.withColumn(\"members_dict\", convert_members_udf(col(\"members\")))\n",
        "  #Register UDF to create and serialize CSR matrices (both unweighted and weighted)\n",
        "  create_csr_unweighted_udf = udf(lambda members: create_csr_matrix(members, use_weights=False), StringType())\n",
        "  create_csr_weighted_udf = udf(lambda members: create_csr_matrix(members, use_weights=True), StringType())\n",
        "\n",
        "  # Add CSR matrix representations (unweighted and weighted) to each community\n",
        "  community_members = community_members.withColumn(\"csr_matrix_unweighted\", create_csr_unweighted_udf(col(\"members_dict\")))\n",
        "  community_members = community_members.withColumn(\"csr_matrix_weighted\", create_csr_weighted_udf(col(\"members_dict\")))\n",
        "\n",
        "  community_members.show(truncate=False)\n",
        "\n",
        "  # Print some information about the matrix\n",
        "  # print(f\"CSR Matrix shape: {csr_matrix_result.shape}\")\n",
        "  # print(f\"Number of non-zero elements: {csr_matrix_result.nnz}\")\n",
        "  # pretty_print_csr_matrix(csr_matrix_result)\n",
        "\n",
        "  return community_members\n",
        "\n",
        "@track_stage(\"Stage 4: Calculate similarities between communities\")\n",
        "def calculate_similarities(community_members):\n",
        "  \"\"\"\n",
        "  Comparing CSR matrices to detect similarity\n",
        "  \"\"\"\n",
        "\n",
        "  # Register UDF to compare structural similarity\n",
        "  compare_structural_similarity_udf = udf(lambda csr_1, csr_2: compare_weighted_structural_similarity(csr_1, csr_2), DoubleType())\n",
        "  compare_weighted_similarity_udf = udf(lambda csr_1, csr_2: compare_weighted_structural_similarity(csr_1, csr_2), DoubleType())\n",
        "\n",
        "  # Cross join to compare each pair of communities and calculate both similarities\n",
        "  cross_joined = community_members.alias(\"a\").crossJoin(community_members.alias(\"b\")) \\\n",
        "      .filter(col(\"a.community_id\") < col(\"b.community_id\")) \\\n",
        "      .withColumn(\"unweighted_similarity_score\", compare_structural_similarity_udf(col(\"a.csr_matrix_unweighted\"), col(\"b.csr_matrix_unweighted\"))) \\\n",
        "      .withColumn(\"weighted_similarity_score\", compare_weighted_similarity_udf(col(\"a.csr_matrix_weighted\"), col(\"b.csr_matrix_weighted\")))\n",
        "\n",
        "  # Add combined similarity score (50/50 importance)\n",
        "  cross_joined = cross_joined.withColumn(\"combined_similarity_score\",\n",
        "                                        0.5 * col(\"unweighted_similarity_score\") + 0.5 * col(\"weighted_similarity_score\"))\n",
        "\n",
        "  # Show the similarity scores between communities\n",
        "  cross_joined.select(col(\"a.community_id\").alias(\"community_id_1\"),\n",
        "                      col(\"b.community_id\").alias(\"community_id_2\"),\n",
        "                      round(col(\"unweighted_similarity_score\"), 2).alias(\"unweighted_similarity_score\"),  # Changed here\n",
        "                      round(col(\"weighted_similarity_score\"), 2).alias(\"weighted_similarity_score\"),  # Changed here\n",
        "                      round(col(\"combined_similarity_score\"), 2).alias(\"combined_similarity_score\")) \\\n",
        "        .orderBy([\"community_id_1\", \"community_id_2\"]) \\\n",
        "        .show(truncate=False)\n",
        "\n",
        "  # Select specific columns before writing to CSV to avoid duplicate column names\n",
        "  cross_joined.select(\"a.community_id\", \"b.community_id\", \"unweighted_similarity_score\", \"weighted_similarity_score\", \"combined_similarity_score\") \\\n",
        "      .withColumnRenamed(\"a.community_id\", \"community_id_1\") \\\n",
        "      .withColumnRenamed(\"b.community_id\", \"community_id_2\") \\\n",
        "      .write.mode(\"overwrite\").csv(\"groups_found\")\n",
        "\n",
        "  return cross_joined"
      ],
      "metadata": {
        "id": "Ike2lfPxo19c"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PhoneCallsCommunityDetection\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.jars.packages\", \"ch.cern.sparkmeasure:spark-measure_2.12:0.24,graphframes:graphframes:0.8.2-spark3.1-s_2.12\") \\\n",
        "    .config(\"spark.executor.memory\", \"20G\") \\\n",
        "    .config(\"spark.driver.memory\", \"50G\") \\\n",
        "    .config(\"spark.executor.memoryOverhead\", \"1G\") \\\n",
        "    .config(\"spark.default.parallelism\", \"100\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
        "    .config(\"spark.driver.maxResultSize\", \"2G\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Initialize StageMetrics\n",
        "stagemetrics = StageMetrics(spark)\n",
        "\n",
        "# Optional: Set logging level to reduce verbosity\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "# Set a checkpoint directory for Spark\n",
        "spark.sparkContext.setCheckpointDir(\"/tmp/spark-checkpoints\")"
      ],
      "metadata": {
        "id": "RAMTvsL2v2YH"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1 - read the dataset\n",
        "dataset_file_path = 'toy_dataset.csv' # set this variable to desired dataset\n",
        "clear_csv = False\n",
        "df_dataset = read_csv_to_dataframe(dataset_file_path)\n",
        "\n",
        "# step 2 - preprocess (convert to duartion in min, create grpah, and find commutnies)\n",
        "df_final, community_members, community_stats = create_graph_from_dataframe(df_dataset)\n",
        "\n",
        "# step 3 - create CSR matrix for each communite\n",
        "csr_matrix_result = format_members_to_csr_matrix(community_members)\n",
        "\n",
        "# step 4 - calculate similarities between communties for find groups\n",
        "cross_joined = calculate_similarities(csr_matrix_result)"
      ],
      "metadata": {
        "id": "FwBOu_C9tleA",
        "outputId": "46853917-53c3-4ed7-a88b-70591ab1c23a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Stage 1: Reading the calls dataset\n",
            "The following dataframe has been read from the CSV file:\n",
            "+-------+-------+----------+----------+----------------+---------------+\n",
            "|Client1|Client2|Start_Time|  End_Time|duration_minutes|duration_DDHHMM|\n",
            "+-------+-------+----------+----------+----------------+---------------+\n",
            "|      1|      2|2408060000|2408060200|           120.0|         000200|\n",
            "|      2|      3|2408040000|2408040500|           300.0|         000500|\n",
            "|      4|      5|2408020000|2408020600|           360.0|         000600|\n",
            "|      5|      6|2408090000|2408091500|           900.0|         001500|\n",
            "|      6|      7|2408070000|2408070800|           480.0|         000800|\n",
            "|      8|      9|2408090000|2408090300|           180.0|         000300|\n",
            "|      9|     10|2408070000|2408070500|           300.0|         000500|\n",
            "|     10|     11|2408010000|2408010400|           240.0|         000400|\n",
            "|     12|     13|2408010000|2408010200|           120.0|         000200|\n",
            "|     13|     14|2408030000|2408030500|           300.0|         000500|\n",
            "|     12|     14|2408020000|2408020800|           480.0|         000800|\n",
            "+-------+-------+----------+----------+----------------+---------------+\n",
            "\n",
            "+-----+--------+-------+-------------------------+--------------+--------------+-------------+--------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+--------------------+---------------------+-------------------------+-------------------------+--------------------------+---------------------+----------------------+----------------------------+------------------+----------------+-------------------+---------------------+\n",
            "|jobId|jobGroup|stageId|name                     |submissionTime|completionTime|stageDuration|numTasks|executorRunTime|executorCpuTime|executorDeserializeTime|executorDeserializeCpuTime|resultSerializationTime|jvmGCTime|resultSize|diskBytesSpilled|memoryBytesSpilled|peakExecutionMemory|recordsRead|bytesRead|recordsWritten|bytesWritten|shuffleFetchWaitTime|shuffleTotalBytesRead|shuffleTotalBlocksFetched|shuffleLocalBlocksFetched|shuffleRemoteBlocksFetched|shuffleLocalBytesRead|shuffleRemoteBytesRead|shuffleRemoteBytesReadToDisk|shuffleRecordsRead|shuffleWriteTime|shuffleBytesWritten|shuffleRecordsWritten|\n",
            "+-----+--------+-------+-------------------------+--------------+--------------+-------------+--------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+--------------------+---------------------+-------------------------+-------------------------+--------------------------+---------------------+----------------------+----------------------------+------------------+----------------+-------------------+---------------------+\n",
            "|480  |NULL    |1333   |csv at <unknown>:0       |1730632350126 |1730632350142 |16           |1       |3              |3              |5                      |2                         |0                      |0        |1605      |0               |0                 |0                  |1          |343      |0             |0           |0                   |0                    |0                        |0                        |0                         |0                    |0                     |0                           |0                 |0               |0                  |0                    |\n",
            "|481  |NULL    |1334   |csv at <unknown>:0       |1730632350188 |1730632350208 |20           |1       |11             |11             |2                      |2                         |0                      |0        |1576      |0               |0                 |0                  |12         |343      |0             |0           |0                   |0                    |0                        |0                        |0                         |0                    |0                     |0                           |0                 |0               |0                  |0                    |\n",
            "|482  |NULL    |1335   |showString at <unknown>:0|1730632350335 |1730632350566 |231          |1       |223            |27             |2                      |2                         |0                      |0        |2843      |0               |0                 |0                  |11         |343      |0             |0           |0                   |0                    |0                        |0                        |0                         |0                    |0                     |0                           |0                 |0               |0                  |0                    |\n",
            "+-----+--------+-------+-------------------------+--------------+--------------+-------------+--------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+--------------------+---------------------+-------------------------+-------------------------+--------------------------+---------------------+----------------------+----------------------------+------------------+----------------+-------------------+---------------------+\n",
            "\n",
            "Completed Stage 1: Reading the calls dataset\n",
            "\n",
            "+-------+-----+--------+--------------------+--------------+--------------+-------------+--------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+--------------------+---------------------+-------------------------+-------------------------+--------------------------+---------------------+----------------------+----------------------------+------------------+----------------+-------------------+---------------------+--------------------+------+-----------------+---------------+-----+\n",
            "|stageId|jobId|jobGroup|                name|submissionTime|completionTime|stageDuration|numTasks|executorRunTime|executorCpuTime|executorDeserializeTime|executorDeserializeCpuTime|resultSerializationTime|jvmGCTime|resultSize|diskBytesSpilled|memoryBytesSpilled|peakExecutionMemory|recordsRead|bytesRead|recordsWritten|bytesWritten|shuffleFetchWaitTime|shuffleTotalBytesRead|shuffleTotalBlocksFetched|shuffleLocalBlocksFetched|shuffleRemoteBlocksFetched|shuffleLocalBytesRead|shuffleRemoteBytesRead|shuffleRemoteBytesReadToDisk|shuffleRecordsRead|shuffleWriteTime|shuffleBytesWritten|shuffleRecordsWritten|          stage_name|Metric|Raw Value (Bytes)|Formatted Value|Units|\n",
            "+-------+-----+--------+--------------------+--------------+--------------+-------------+--------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+--------------------+---------------------+-------------------------+-------------------------+--------------------------+---------------------+----------------------+----------------------------+------------------+----------------+-------------------+---------------------+--------------------+------+-----------------+---------------+-----+\n",
            "|   1333|  480|    NULL|  csv at <unknown>:0| 1730632350126| 1730632350142|           16|       1|              3|              3|                      5|                         2|                      0|        0|      1605|               0|                 0|                  0|          1|      343|             0|           0|                   0|                    0|                        0|                        0|                         0|                    0|                     0|                           0|                 0|               0|                  0|                    0|Stage 1: Reading ...|  NULL|             NULL|           NULL| NULL|\n",
            "|   1334|  481|    NULL|  csv at <unknown>:0| 1730632350188| 1730632350208|           20|       1|             11|             11|                      2|                         2|                      0|        0|      1576|               0|                 0|                  0|         12|      343|             0|           0|                   0|                    0|                        0|                        0|                         0|                    0|                     0|                           0|                 0|               0|                  0|                    0|Stage 1: Reading ...|  NULL|             NULL|           NULL| NULL|\n",
            "|   1335|  482|    NULL|showString at <un...| 1730632350335| 1730632350566|          231|       1|            223|             27|                      2|                         2|                      0|        0|      2843|               0|                 0|                  0|         11|      343|             0|           0|                   0|                    0|                        0|                        0|                         0|                    0|                     0|                           0|                 0|               0|                  0|                    0|Stage 1: Reading ...|  NULL|             NULL|           NULL| NULL|\n",
            "+-------+-----+--------+--------------------+--------------+--------------+-------------+--------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+--------------------+---------------------+-------------------------+-------------------------+--------------------------+---------------------+----------------------+----------------------------+------------------+----------------+-------------------+---------------------+--------------------+------+-----------------+---------------+-----+\n",
            "\n",
            "Starting Stage 2: Preprocessing and creating the graph\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
            "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final DataFrame with Sequential Community IDs:\n",
            "+-------+-------+---------------+----------------+------------+--------------+\n",
            "|Client1|Client2|duration_DDHHMM|duration_minutes|community_id|community_size|\n",
            "+-------+-------+---------------+----------------+------------+--------------+\n",
            "|      2|      3|         000500|           300.0|           1|             3|\n",
            "|      1|      2|         000200|           120.0|           1|             3|\n",
            "|      6|      7|         000800|           480.0|           2|             4|\n",
            "|      5|      6|         001500|           900.0|           2|             4|\n",
            "|      4|      5|         000600|           360.0|           2|             4|\n",
            "|      9|     10|         000500|           300.0|           3|             4|\n",
            "|      8|      9|         000300|           180.0|           3|             4|\n",
            "|     10|     11|         000400|           240.0|           3|             4|\n",
            "|     13|     14|         000500|           300.0|           4|             3|\n",
            "|     12|     13|         000200|           120.0|           4|             3|\n",
            "|     12|     14|         000800|           480.0|           4|             3|\n",
            "+-------+-------+---------------+----------------+------------+--------------+\n",
            "\n",
            "\n",
            "Community Members with Sequential IDs:\n",
            "+------------+---------------------------------------------------------------------------+\n",
            "|community_id|members                                                                    |\n",
            "+------------+---------------------------------------------------------------------------+\n",
            "|1           |[{2, 3, 000500, 300.0}, {1, 2, 000200, 120.0}]                             |\n",
            "|2           |[{6, 7, 000800, 480.0}, {5, 6, 001500, 900.0}, {4, 5, 000600, 360.0}]      |\n",
            "|3           |[{9, 10, 000500, 300.0}, {8, 9, 000300, 180.0}, {10, 11, 000400, 240.0}]   |\n",
            "|4           |[{13, 14, 000500, 300.0}, {12, 13, 000200, 120.0}, {12, 14, 000800, 480.0}]|\n",
            "+------------+---------------------------------------------------------------------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "----------------------------------------\n",
            "Exception occurred during processing of request from ('127.0.0.1', 46776)\n",
            "ERROR:root:Exception while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 516, in send_command\n",
            "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
            "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
            "    raise Py4JNetworkError(\n",
            "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/socketserver.py\", line 316, in _handle_request_noblock\n",
            "    self.process_request(request, client_address)\n",
            "  File \"/usr/lib/python3.10/socketserver.py\", line 347, in process_request\n",
            "    self.finish_request(request, client_address)\n",
            "  File \"/usr/lib/python3.10/socketserver.py\", line 360, in finish_request\n",
            "    self.RequestHandlerClass(request, client_address, self)\n",
            "  File \"/usr/lib/python3.10/socketserver.py\", line 747, in __init__\n",
            "    self.handle()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/accumulators.py\", line 295, in handle\n",
            "    poll(accum_updates)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/accumulators.py\", line 267, in poll\n",
            "    if self.rfile in r and func():\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/accumulators.py\", line 271, in accum_updates\n",
            "    num_updates = read_int(self.rfile)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/serializers.py\", line 596, in read_int\n",
            "    raise EOFError\n",
            "EOFError\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Py4JError",
          "evalue": "An error occurred while calling o2061.csv",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-141-0ecbaf3278a4>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# step 2 - preprocess (convert to duartion in min, create grpah, and find commutnies)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommunity_members\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommunity_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph_from_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# step 3 - create CSR matrix for each communite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-137-6e5701cb6d4f>\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mstagemetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Begin collecting metrics for this stage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Run the actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mstagemetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Stop collecting metrics for this stage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-140-1658cad52720>\u001b[0m in \u001b[0;36mcreate_graph_from_dataframe\u001b[0;34m(df_dataset)\u001b[0m\n\u001b[1;32m    123\u001b[0m       \u001b[0;34m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m       \u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"community_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m       \u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"community_members_results\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m   \u001b[0;31m# Optionally, if you want to save additional community statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1862\u001b[0m             \u001b[0mlineSep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlineSep\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m         )\n\u001b[0;32m-> 1864\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m     def orc(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    332\u001b[0m                     format(target_id, \".\", name, value))\n\u001b[1;32m    333\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             raise Py4JError(\n\u001b[0m\u001b[1;32m    335\u001b[0m                 \u001b[0;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m                 format(target_id, \".\", name))\n",
            "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o2061.csv"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_monitor = spark.read.csv(\"stage_metrics\",  header=False)\n",
        "df_monitor.show(truncate=True)\n",
        "\n",
        "# df_m = spark.read.csv(\"/content/community_members_results\", header=True)\n",
        "# df_m.show()"
      ],
      "metadata": {
        "id": "Y7JAhTN0p9dB",
        "outputId": "0c62525e-e18b-4656-d271-613b3d9dbf7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+----+--------------------+-------------+-------------+---+---+---+---+----+----+----+----+------+----+----+---------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+--------------------+----+----+----+----+\n",
            "|_c0|_c1| _c2|                 _c3|          _c4|          _c5|_c6|_c7|_c8|_c9|_c10|_c11|_c12|_c13|  _c14|_c15|_c16|     _c17|_c18|_c19|_c20|_c21|_c22|_c23|_c24|_c25|_c26|_c27|_c28|_c29|_c30|_c31|_c32|_c33|                _c34|_c35|_c36|_c37|_c38|\n",
            "+---+---+----+--------------------+-------------+-------------+---+---+---+---+----+----+----+----+------+----+----+---------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+--------------------+----+----+----+----+\n",
            "|773|269|NULL|count at Connecte...|1730631742084|1730631742302|218|  1|205| 21|   2|   2|   0|   0|  2305|   0|   0|        0|  11| 343|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|Stage 2: Preproce...|NULL|NULL|NULL|NULL|\n",
            "|774|270|NULL|count at Connecte...|1730631742387|1730631742474| 87|  1| 75| 72|   4|   4|   0|   0|  3410|   0|   0|   262144|   1| 744|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1| 488|  11|Stage 2: Preproce...|NULL|NULL|NULL|NULL|\n",
            "|776|271|NULL|count at Connecte...|1730631742513|1730631742721|208| 10|264| 85|  78|  49|   0|   0| 58500|   0|   0|  2621440|   0|   0|   0|   0|   0| 488|   7|   7|   0| 488|   0|   0|  11|   0|   0|   0|Stage 2: Preproce...|NULL|NULL|NULL|NULL|\n",
            "|778|272|NULL|count at Connecte...|1730631742753|1730631742885|132| 10|143| 51|  70|  51|   0|   0| 57921|   0|   0|        0|   7|3296|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|  25| 581|  10|Stage 2: Preproce...|NULL|NULL|NULL|NULL|\n",
            "|781|273|NULL|count at Connecte...|1730631742920|1730631742940| 20|  1| 12| 11|   1|   1|   0|   0|  3995|   0|   0|        0|   0|   0|   0|   0|   0| 581|  10|  10|   0| 581|   0|   0|  10|   0|   0|   0|Stage 2: Preproce...|NULL|NULL|NULL|NULL|\n",
            "|783|274|NULL|collect at Connec...|1730631743065|1730631743222|157| 10|196|115|  67|  53|   0|   0| 60854|   0|   0|  2621440|   7|3296|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|  10|1290|  20|Stage 2: Preproce...|NULL|NULL|NULL|NULL|\n",
            "|786|275|NULL|collect at Connec...|1730631743257|1730631743399|142| 10|163| 58|  71|  58|   0|   0| 81070|   0|   0|673710080|   0|   0|   0|   0|   9|1290|  19|  19|   0|1290|   0|   0|  20|   0|   0|   0|Stage 2: Preproce...|NULL|NULL|NULL|NULL|\n",
            "|789|276|NULL|collect at Connec...|1730631743434|1730631743528| 94| 10| 50| 28|  84|  54|   0|   0| 78450|   0|   0|        0|  10|5664|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|Stage 2: Preproce...|NULL|NULL|NULL|NULL|\n",
            "|791|277|NULL|$anonfun$withThre...|1730631743707|1730631743764| 57| 10| 24| 23|  44|  44|   1|   0| 55988|   0|   0|        0|   7|3296|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|Stage 2: Preproce...|NULL|NULL|NULL|NULL|\n",
            "|794|278|NULL|collect at Connec...|1730631743816|1730631743961|145| 10|136| 88|  91|  67|   0|   0| 89292|   0|   0| 13108240|  10|5664|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   5| 693|  11|Stage 2: Preproce...|NULL|NULL|NULL|NULL|\n",
            "|798|279|NULL|collect at Connec...|1730631744002|1730631744218|216| 10|228| 73| 130|  70|   0|   0|109250|   0|   0|  2621440|   0|   0|   0|   0|   0| 693|  11|  11|   0| 693|   0|   0|  11|   0|   0|   0|Stage 2: Preproce...|NULL|NULL|NULL|NULL|\n",
            "|802|280|NULL|collect at Connec...|1730631744317|1730631744574|257| 10|271| 90| 111|  73|   5|   0|111087|   0|   0|  2621440|   7|3256|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|  33| 664|  10|Stage 2: Preproce...|NULL|NULL|NULL|NULL|\n",
            "|812|282|NULL|collect at Connec...|1730631745117|1730631745342|225| 10| 90| 17| 189|  75|   0|   0|128511|   0|   0|        0|   7|4080|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|Stage 2: Preproce...|NULL|NULL|NULL|NULL|\n",
            "|807|281|NULL|collect at Connec...|1730631744640|1730631744939|299| 10|352| 55| 137|  74|   0|   0|131260|   0|   0|472383488|   0|   0|   0|   0|   0| 664|  10|  10|   0| 664|   0|   0|  10|   0|   0|   0|Stage 2: Preproce...|NULL|NULL|NULL|NULL|\n",
            "|816|283|NULL|$anonfun$withThre...|1730631746042|1730631746299|257| 10|135| 36| 197|  72|   0|   0|106684|   0|   0|        0|   7|3256|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|Stage 2: Preproce...|NULL|NULL|NULL|NULL|\n",
            "|821|284|NULL|rdd at ConnectedC...|1730631746455|1730631747208|753| 20|743|164| 523| 176|   0|   0|279765|   0|   0| 15729680|  14|8160|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|  25| 630|  10|Stage 2: Preproce...|NULL|NULL|NULL|NULL|\n",
            "|827|285|NULL|rdd at ConnectedC...|1730631747319|1730631747714|395| 10|378| 69| 280|  89|   1|   0|160593|   0|   0|  2621440|   0|   0|   0|   0|   0| 630|  10|  10|   0| 630|   0|   0|  10|   0|   0|   0|Stage 2: Preproce...|NULL|NULL|NULL|NULL|\n",
            "|833|286|NULL|rdd at ConnectedC...|1730631747759|1730631748118|359| 10|297| 54| 294|  95|   0|   0|159928|   0|   0|        0|   6|2904|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|  13| 762|  10|Stage 2: Preproce...|NULL|NULL|NULL|NULL|\n",
            "|840|287|NULL|first at Connecte...|1730631748307|1730631748470|163|  1|104| 27|  21|  11|   0|   0| 18486|   0|   0|        0|   0|   0|   0|   0|   0| 762|  10|  10|   0| 762|   0|   0|  10|   0|   0|   0|Stage 2: Preproce...|NULL|NULL|NULL|NULL|\n",
            "|846|288|NULL|collect at Connec...|1730631749330|1730631749595|265| 10|238| 66| 149|  89|   2|   0|162775|   0|   0|  2621440|   6|2904|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|  46|1267|  19|Stage 2: Preproce...|NULL|NULL|NULL|NULL|\n",
            "+---+---+----+--------------------+-------------+-------------+---+---+---+---+----+----+----+----+------+----+----+---------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+--------------------+----+----+----+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}