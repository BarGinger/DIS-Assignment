{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BarGinger/DIS-Assignment/blob/main/Src/dis_notebook_02_11_24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "QFDENKMNs-52",
        "outputId": "04592a1a-ce75-4700-91ab-aaffb153f9d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "openjdk-8-jdk-headless is already the newest version (8u422-b05-1~22.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Requirement already satisfied: graphframes in /usr/local/lib/python3.10/dist-packages (0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from graphframes) (1.26.4)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.10/dist-packages (from graphframes) (1.3.7)\n",
            "Requirement already satisfied: sparkmeasure==0.24 in /usr/local/lib/python3.10/dist-packages (0.24.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "!pip install graphframes\n",
        "!pip install sparkmeasure==0.24\n",
        "!pip install matplotlib seaborn\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import (\n",
        "    col,\n",
        "    udf,\n",
        "    row_number,\n",
        "    countDistinct,\n",
        "    collect_list,\n",
        "    struct,\n",
        "    count,\n",
        "    sum,\n",
        "    avg,\n",
        "    expr,\n",
        "    percentile_approx,\n",
        "    max as spark_max,\n",
        "    explode,\n",
        "    round,\n",
        "    rand,\n",
        "    monotonically_increasing_id,\n",
        "    array,\n",
        "    lit,\n",
        "    broadcast,\n",
        "    lag\n",
        ")\n",
        "import pyspark.sql.functions as F\n",
        "from sparkmeasure import StageMetrics\n",
        "from pyspark.sql.types import (\n",
        "    StringType, IntegerType, BinaryType, DoubleType,\n",
        "    ArrayType, StructType, StructField, LongType, TimestampType\n",
        ")\n",
        "from pyspark.sql import Window\n",
        "from datetime import datetime, timedelta\n",
        "from graphframes import GraphFrame\n",
        "from scipy.sparse import csr_matrix, vstack, hstack\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import base64\n",
        "from sparkmeasure import StageMetrics # for resources monitoring\n",
        "from functools import wraps\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import random\n",
        "from operator import truediv\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "gjrP64v5QyEd"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Monitor CPU, Memory and running time\n",
        "def format_memory(report_memory_output):\n",
        "  # Initialize a list to store parsed data\n",
        "    parsed_data = []\n",
        "\n",
        "    # Iterate through each line in report_memory_output and parse data\n",
        "    for line in report_memory_output.split('\\n'):\n",
        "        if line.startswith(\"Stage\"):\n",
        "            # Split the line into parts\n",
        "            parts = line.split()\n",
        "\n",
        "            # Extract information\n",
        "            stage = parts[1]\n",
        "            metric = parts[2]\n",
        "            raw_value = int(parts[6])  # Raw value in bytes (integer)\n",
        "\n",
        "            # Extract formatted value and units\n",
        "            formatted_value_with_units = \" \".join(parts[7:]).replace(\"(\", \"\").replace(\")\", \"\")\n",
        "            formatted_value, units = formatted_value_with_units.split(\" \", 1)\n",
        "\n",
        "            # Append the extracted information to parsed_data\n",
        "            parsed_data.append({\n",
        "                \"stageId\": stage,\n",
        "                \"memory_metric\": metric,\n",
        "                \"memory_raw_value_bytes\": raw_value,\n",
        "                \"memory_formatted_value\": formatted_value,\n",
        "                \"memory_units\": units\n",
        "            })\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame(parsed_data)\n",
        "    return spark.createDataFrame(df)\n",
        "\n",
        "\n",
        "def track_stage(stage_name):\n",
        "    def decorator(func):\n",
        "        @wraps(func)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            print(f\"Starting {stage_name}\")\n",
        "            stagemetrics.begin()  # Begin collecting metrics for this stage\n",
        "\n",
        "            result = func(*args, **kwargs)  # Run the actual function\n",
        "\n",
        "            stagemetrics.end()  # Stop collecting metrics for this stage\n",
        "\n",
        "            time.sleep(15)\n",
        "\n",
        "            # Generate metrics DataFrame\n",
        "            print(f\"Completed {stage_name}\\n\")\n",
        "            df_metrics_all = stagemetrics.create_stagemetrics_DF()\n",
        "            df_metrics_agg = stagemetrics.aggregate_stagemetrics_DF()\n",
        "            # Add stage_name column and join metrics and memory DataFrames\n",
        "            df_metrics_agg = df_metrics_agg.withColumn(\"stage_name\", pyspark.sql.functions.lit(stage_name))\n",
        "            df_metrics_agg = df_metrics_agg.withColumn(\"dataset\", pyspark.sql.functions.lit(dataset_file_path))\n",
        "\n",
        "            df_metrics_agg.show(truncate=False)\n",
        "\n",
        "\n",
        "            # Generate memory information DataFrame\n",
        "            # memory_info = stagemetrics.report_memory()\n",
        "            # df_memory = format_memory(memory_info)\n",
        "\n",
        "\n",
        "            # df_metrics = df_metrics.join(df_memory, on=['stageId'], how='left')\n",
        "\n",
        "            # Set write mode based on the stage\n",
        "            if \"Stage 1\" in stage_name and clear_csv:\n",
        "                write_mode = \"overwrite\"\n",
        "                header = \"true\"\n",
        "            else:\n",
        "                write_mode = \"append\"\n",
        "                header = \"true\"\n",
        "\n",
        "            # Write metrics to CSV with appropriate mode and header settings\n",
        "            df_metrics_agg.coalesce(1).write \\\n",
        "                .mode(write_mode) \\\n",
        "                .option(\"header\", header) \\\n",
        "                .csv(f\"{dataset_name}_stage_metrics\")\n",
        "\n",
        "            return result\n",
        "        return wrapper\n",
        "    return decorator"
      ],
      "metadata": {
        "id": "R9jMJMWzY2LP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utils functions\n",
        "\n",
        "# Convert YYMMDDHHMM to a proper datetime object\n",
        "def calculate_duration_minutes(start_time, end_time):\n",
        "  \"\"\"\n",
        "  Calculate the duration between two times in minutes.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  start_time : str\n",
        "      The start time in HH:MM:SS format.\n",
        "  end_time : str\n",
        "      The end time in HH:MM:SS format.\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  duration_minutes : float\n",
        "      The duration between start_time and end_time in minutes.\n",
        "  \"\"\"\n",
        "  start_datetime = convert_to_datetime(start_time)\n",
        "  end_datetime = convert_to_datetime(end_time)\n",
        "  duration = end_datetime - start_datetime\n",
        "  duration_minutes = duration.total_seconds() / 60\n",
        "  return duration_minutes\n",
        "\n",
        "def convert_to_datetime(time_str):\n",
        "  \"\"\"\n",
        "  Convert a time string in '%y%m%d%H%M' format to a datetime object.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  time_str : str\n",
        "      The time string in '%y%m%d%H%M' format.\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  datetime_obj : datetime.datetime\n",
        "      The datetime object representing the given time string.\n",
        "  \"\"\"\n",
        "  # Use datetime.datetime.strptime to parse the time string\n",
        "  # This is the correct way to use strptime, avoiding the AttributeError\n",
        "  return datetime.strptime(str(time_str), '%y%m%d%H%M')\n",
        "\n",
        "# Define UDF for calculating duration in DDHHMM format\n",
        "def calculate_duration_string(start_time, end_time):\n",
        "    start_dt = convert_to_datetime(start_time)\n",
        "    end_dt = convert_to_datetime(end_time)\n",
        "    duration = end_dt - start_dt\n",
        "\n",
        "    days = duration.days\n",
        "    hours, remainder = divmod(duration.seconds, 3600)\n",
        "    minutes = remainder // 60\n",
        "    return f'{days:02d}{hours:02d}{minutes:02d}'\n",
        "\n",
        "# prompt: print csr_matrix_result pretty\n",
        "def pretty_print_csr_matrix(csr_matrix_result):\n",
        "  \"\"\"Prints a CSR matrix in a readable format.\"\"\"\n",
        "\n",
        "  rows, cols = csr_matrix_result.nonzero()\n",
        "  data = csr_matrix_result.data\n",
        "\n",
        "  df = pd.DataFrame({\n",
        "      'Row': rows,\n",
        "      'Col': cols,\n",
        "      'Value': data\n",
        "  })\n",
        "\n",
        "  print(df)\n",
        "\n",
        "def create_csr_matrix_from_edges_with_spark(members_df):\n",
        "    \"\"\"\n",
        "    Creates a CSR matrix from a Spark DataFrame based on unique vertices.\n",
        "\n",
        "    Args:\n",
        "        members_df: Spark DataFrame with 'community_id' and 'members' columns.\n",
        "\n",
        "    Returns:\n",
        "        A CSR matrix.\n",
        "    \"\"\"\n",
        "\n",
        "    # Explode the members array to get each connection in separate rows\n",
        "    exploded_df = members_df.select(\n",
        "        \"community_id\",\n",
        "        explode(\"members\").alias(\"member\")\n",
        "    ).select(\n",
        "        \"community_id\",\n",
        "        col(\"member.Client1\").alias(\"Client1\"),\n",
        "        col(\"member.Client2\").alias(\"Client2\"),\n",
        "        col(\"member.duration_minutes\").alias(\"duration_minutes\")\n",
        "    )\n",
        "\n",
        "    # Get unique clients and create a mapping to indices\n",
        "    unique_clients = exploded_df.select(\"Client1\").union(exploded_df.select(\"Client2\")).distinct().rdd.flatMap(lambda x: x).collect()\n",
        "    client_to_index = {client: i for i, client in enumerate(unique_clients)}\n",
        "    num_clients = len(unique_clients)\n",
        "\n",
        "    # Extract data for CSR matrix\n",
        "    rows = exploded_df.select(\"Client1\").rdd.map(lambda row: client_to_index[row[0]]).collect()\n",
        "    cols = exploded_df.select(\"Client2\").rdd.map(lambda row: client_to_index[row[0]]).collect()\n",
        "    data = exploded_df.select(\"duration_minutes\").rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "    # Create CSR matrix\n",
        "    csr = csr_matrix((data, (rows, cols)), shape=(num_clients, num_clients))\n",
        "\n",
        "    return csr\n",
        "\n",
        "# create csr matrix from given members list\n",
        "def create_csr_matrix(members, use_weights=False):\n",
        "    clients = list(set([member['Client1'] for member in members] + [member['Client2'] for member in members]))\n",
        "    client_index = {client: idx for idx, client in enumerate(clients)}\n",
        "\n",
        "    row_indices = []\n",
        "    col_indices = []\n",
        "    data = []\n",
        "\n",
        "    for member in members:\n",
        "        row_indices.append(client_index[member['Client1']])\n",
        "        col_indices.append(client_index[member['Client2']])\n",
        "        if use_weights:\n",
        "            data.append(float(member['duration_minutes']))  # Use duration in minutes as the weight of the edge\n",
        "        else:\n",
        "            data.append(1)  # Use 1 for unweighted similarity\n",
        "\n",
        "    num_clients = len(clients)\n",
        "    csr = csr_matrix((data, (row_indices, col_indices)), shape=(num_clients, num_clients))\n",
        "\n",
        "    # Serialize the CSR matrix\n",
        "    serialized_csr = base64.b64encode(pickle.dumps(csr)).decode('utf-8')\n",
        "    return serialized_csr\n",
        "\n",
        "# compare given two csr matrices (each relating to a community) to get similarity score\n",
        "def compare_weighted_structural_similarity(csr_matrix_1, csr_matrix_2):\n",
        "    # Deserialize CSR matrices\n",
        "    csr_1 = pickle.loads(base64.b64decode(csr_matrix_1))\n",
        "    csr_2 = pickle.loads(base64.b64decode(csr_matrix_2))\n",
        "\n",
        "\n",
        "    # Align matrix dimensions to the largest size\n",
        "    max_rows = max(csr_1.shape[0], csr_2.shape[0])\n",
        "    max_cols = max(csr_1.shape[1], csr_2.shape[1])\n",
        "\n",
        "    # Pad csr_1 to match max dimensions\n",
        "    if csr_1.shape[0] < max_rows or csr_1.shape[1] < max_cols:\n",
        "        csr_1 = vstack([csr_1, csr_matrix((max_rows - csr_1.shape[0], csr_1.shape[1]))]) if csr_1.shape[0] < max_rows else csr_1\n",
        "        csr_1 = hstack([csr_1, csr_matrix((csr_1.shape[0], max_cols - csr_1.shape[1]))]) if csr_1.shape[1] < max_cols else csr_1\n",
        "\n",
        "    # Pad csr_2 to match max dimensions\n",
        "    if csr_2.shape[0] < max_rows or csr_2.shape[1] < max_cols:\n",
        "        csr_2 = vstack([csr_2, csr_matrix((max_rows - csr_2.shape[0], csr_2.shape[1]))]) if csr_2.shape[0] < max_rows else csr_2\n",
        "        csr_2 = hstack([csr_2, csr_matrix((csr_2.shape[0], max_cols - csr_2.shape[1]))]) if csr_2.shape[1] < max_cols else csr_2\n",
        "\n",
        "    # Calculate structural similarity (e.g., using cosine similarity)\n",
        "    dot_product = csr_1.multiply(csr_2).sum()\n",
        "    norm_1 = np.sqrt(csr_1.multiply(csr_1).sum())\n",
        "    norm_2 = np.sqrt(csr_2.multiply(csr_2).sum())\n",
        "    similarity = dot_product / (norm_1 * norm_2) if norm_1 != 0 and norm_2 != 0 else 0\n",
        "    return float(similarity)"
      ],
      "metadata": {
        "id": "mhgOioE4ZU5t"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "import os\n",
        "import shutil\n",
        "from google.colab import files  # Import for downloading\n",
        "\n",
        "def create_spark_session(app_name=\"CallDatasetGenerator\"):\n",
        "    \"\"\"\n",
        "    Create or get a Spark session for distributed data processing with optimized settings.\n",
        "    \"\"\"\n",
        "    return SparkSession.builder \\\n",
        "        .appName(app_name) \\\n",
        "        .config(\"spark.sql.shuffle.partitions\", \"100\") \\\n",
        "        .config(\"spark.default.parallelism\", \"100\") \\\n",
        "        .config(\"spark.executor.memory\", \"2g\") \\\n",
        "        .config(\"spark.driver.memory\", \"2g\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "def generate_communities(spark, num_communities, community_size_range, density=0.3, extra_factor=1.5):\n",
        "    \"\"\"\n",
        "    Generate isolated communities with controlled sizes and connections.\n",
        "    Ensures enough connections for the sample count by using an extra factor.\n",
        "    \"\"\"\n",
        "    communities = []\n",
        "    for community_id in range(num_communities):\n",
        "        size = random.randint(community_size_range[0], community_size_range[1])\n",
        "        base_id = community_id * 1000\n",
        "        community_clients = [(community_id, base_id + i, base_id + j)\n",
        "                             for i in range(size) for j in range(i + 1, size) if random.random() < density * extra_factor]\n",
        "        communities.extend(community_clients)\n",
        "\n",
        "    return spark.createDataFrame(communities, [\"community_id\", \"client1\", \"client2\"])\n",
        "\n",
        "def generate_call_times(communities_df, calls_per_connection_range, duration_range, base_time, num_samples):\n",
        "    \"\"\"\n",
        "    Generate call start and end times for each client connection, ensuring total number of samples matches `num_samples`.\n",
        "    \"\"\"\n",
        "    calls_df = communities_df.withColumn(\n",
        "        \"num_calls\",\n",
        "        F.expr(f\"floor(rand() * ({calls_per_connection_range[1]} - {calls_per_connection_range[0]} + 1)) + {calls_per_connection_range[0]}\")\n",
        "    ).withColumn(\n",
        "        \"call_id\", F.monotonically_increasing_id()\n",
        "    ).withColumn(\n",
        "        \"calls\", F.expr(\"sequence(1, num_calls)\")\n",
        "    ).select(\"client1\", \"client2\", \"call_id\", F.explode(\"calls\").alias(\"call_num\"))\n",
        "\n",
        "    def generate_times():\n",
        "        start_time = base_time + timedelta(minutes=random.randint(0, 1440))\n",
        "        duration = random.randint(duration_range[0], duration_range[1])\n",
        "        end_time = start_time + timedelta(minutes=duration)\n",
        "        return start_time.strftime('%y%m%d%H%M'), end_time.strftime('%y%m%d%H%M')\n",
        "\n",
        "    time_udf = F.udf(lambda: generate_times(), \"struct<Start_Time:string, End_Time:string>\")\n",
        "    calls_df = calls_df.withColumn(\"call_times\", time_udf())\n",
        "\n",
        "    # Ensure consistent schema for the final DataFrame\n",
        "    calls_df = calls_df.select(\n",
        "        \"client1\", \"client2\", calls_df[\"call_times.Start_Time\"].alias(\"Start_Time\"), calls_df[\"call_times.End_Time\"].alias(\"End_Time\")\n",
        "    )\n",
        "\n",
        "    # Limit to the specified number of samples\n",
        "    final_calls_df = calls_df.limit(num_samples)\n",
        "\n",
        "    # Retry generation if the sample count isn't met\n",
        "    while final_calls_df.count() < num_samples:\n",
        "        additional_df = calls_df.limit(num_samples - final_calls_df.count()).select(\"client1\", \"client2\", \"Start_Time\", \"End_Time\")\n",
        "        final_calls_df = final_calls_df.union(additional_df).limit(num_samples)\n",
        "\n",
        "    return final_calls_df\n",
        "\n",
        "# Function to delete all generated datasets\n",
        "def delete_generated_datasets():\n",
        "    folder_path = \"/content/datasets/\"\n",
        "    deleted_files = []\n",
        "\n",
        "    if os.path.exists(folder_path):\n",
        "        # Loop through each item in the folder\n",
        "        for item in os.listdir(folder_path):\n",
        "            item_path = os.path.join(folder_path, item)\n",
        "            # Check if it's a directory and remove it with shutil.rmtree\n",
        "            if os.path.isdir(item_path):\n",
        "                shutil.rmtree(item_path)\n",
        "            else:\n",
        "                os.remove(item_path)\n",
        "            deleted_files.append(item)\n",
        "\n",
        "        # Print the results\n",
        "        if deleted_files:\n",
        "            print(\"Deleted the following items:\")\n",
        "            for item in deleted_files:\n",
        "                print(item)\n",
        "        else:\n",
        "            print(\"No files found in the folder to delete.\")\n",
        "    else:\n",
        "        print(\"The folder does not exist.\")\n",
        "\n",
        "def save_dataset(dataset, filename):\n",
        "    \"\"\"\n",
        "    Save the generated dataset to a temporary directory and then move to the final directory.\n",
        "\n",
        "    Parameters:\n",
        "        dataset (DataFrame): The DataFrame to save, containing generated call data.\n",
        "        filename (str): Base name for the dataset file.\n",
        "    \"\"\"\n",
        "    # Define the directories\n",
        "    final_dir = \"/content/datasets\"\n",
        "    temp_dir = f\"{final_dir}/{filename}_temp\"\n",
        "    final_path = os.path.join(final_dir, filename)\n",
        "\n",
        "    # Create directories if they don't exist\n",
        "    os.makedirs(final_dir, exist_ok=True)\n",
        "\n",
        "    # Write to the temporary directory\n",
        "    dataset.write.mode(\"overwrite\").option(\"header\", \"true\").csv(temp_dir)\n",
        "\n",
        "    # Move the content to the final directory\n",
        "    if os.path.exists(final_path):\n",
        "        shutil.rmtree(final_path)\n",
        "    shutil.move(temp_dir, final_path)\n",
        "\n",
        "    # Clean up by removing the temporary directory\n",
        "    if os.path.exists(temp_dir):\n",
        "        shutil.rmtree(temp_dir)\n",
        "\n",
        "    print(f\"Dataset saved as {final_path}\")\n",
        "    return final_path\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    delete_generated_datasets()\n",
        "    spark = create_spark_session()\n",
        "\n",
        "    # Define parameter configurations with num_samples\n",
        "    parameter_sets = [\n",
        "        {\"num_communities\": 5, \"community_size_range\": (3, 5), \"calls_per_connection_range\": (1, 2), \"duration_range\": (30, 120), \"density\": 0.5, \"num_samples\": 50},\n",
        "        {\"num_communities\": 10, \"community_size_range\": (5, 7), \"calls_per_connection_range\": (1, 3), \"duration_range\": (30, 180), \"density\": 0.4, \"num_samples\": 100},\n",
        "        # Add more configurations as needed\n",
        "    ]\n",
        "\n",
        "    base_time = datetime(2024, 1, 1)\n",
        "\n",
        "    for i, params in enumerate(parameter_sets):\n",
        "        print(f\"\\nGenerating dataset for configuration {i + 1}: {params}\")\n",
        "\n",
        "        # Generate communities and call times\n",
        "        communities_df = generate_communities(\n",
        "            spark,\n",
        "            num_communities=params[\"num_communities\"],\n",
        "            community_size_range=params[\"community_size_range\"],\n",
        "            density=params[\"density\"],\n",
        "            extra_factor=2  # Generate more potential connections initially\n",
        "        )\n",
        "        calls_df = generate_call_times(\n",
        "            communities_df,\n",
        "            calls_per_connection_range=params[\"calls_per_connection_range\"],\n",
        "            duration_range=params[\"duration_range\"],\n",
        "            base_time=base_time,\n",
        "            num_samples=params[\"num_samples\"]\n",
        "        )\n",
        "\n",
        "        # Save dataset and print information\n",
        "        filename = f\"dataset_config_{i + 1}\"\n",
        "        final_path = save_dataset(calls_df, filename)\n",
        "        parameter_sets[i]['dataset_name'] = filename\n",
        "        parameter_sets[i]['csv_filename'] = final_path\n",
        "\n",
        "    df_datasets = pd.DataFrame(parameter_sets)\n",
        "    df_datasets.to_csv(\"dataset_metadata.csv\", index=False)\n",
        "    # Stop Spark session after completing tasks\n",
        "    spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJPkATltavF1",
        "outputId": "5d833b7a-1a7a-4043-d28c-60ddf28b4cd9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted the following items:\n",
            "dataset_config_1\n",
            "\n",
            "Generating dataset for configuration 1: {'num_communities': 5, 'community_size_range': (3, 5), 'calls_per_connection_range': (1, 2), 'duration_range': (30, 120), 'density': 0.5, 'num_samples': 50}\n",
            "Dataset saved as /content/datasets/dataset_config_1\n",
            "\n",
            "Generating dataset for configuration 2: {'num_communities': 10, 'community_size_range': (5, 7), 'calls_per_connection_range': (1, 3), 'duration_range': (30, 180), 'density': 0.4, 'num_samples': 100}\n",
            "Dataset saved as /content/datasets/dataset_config_2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate datasets\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "  .appName(\"PhoneCallsCommunityDetection\") \\\n",
        "  .master(\"local[*]\") \\\n",
        "  .config(\"spark.jars.packages\", \"ch.cern.sparkmeasure:spark-measure_2.12:0.24,graphframes:graphframes:0.8.2-spark3.1-s_2.12\") \\\n",
        "  .config(\"spark.executor.memory\", \"20G\") \\\n",
        "  .config(\"spark.driver.memory\", \"50G\") \\\n",
        "  .config(\"spark.executor.memoryOverhead\", \"1G\") \\\n",
        "  .config(\"spark.default.parallelism\", \"100\") \\\n",
        "  .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
        "  .config(\"spark.driver.maxResultSize\", \"2G\") \\\n",
        "  .getOrCreate()\n",
        "\n",
        "\n",
        "# Initialize StageMetrics\n",
        "stagemetrics = StageMetrics(spark)\n",
        "\n",
        "# Optional: Set logging level to reduce verbosity\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "# Set a checkpoint directory for Spark\n",
        "spark.sparkContext.setCheckpointDir(\"/tmp/spark-checkpoints\")"
      ],
      "metadata": {
        "id": "xSdU6FWDinu3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7a443b8-38fc-4158-e4d6-0820a427ea6d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The class ch.cern.sparkmeasure.StageMetrics does not exist or could not be loaded. 'JavaPackage' object is not callable\n",
            "\n",
            "Error: the sparkMeasure jar is like not loaded\n",
            "Please check configuration: spark.jars.packages, spark.jars and/or driver classpath configuration and re-run the workload\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the\n",
        "@track_stage(\"Stage 1: Reading the calls dataset\")\n",
        "def read_csv_to_dataframe(file_path= 'toy_dataset.csv'):\n",
        "  \"\"\"\n",
        "  Read dataset from given path into a Spark DataFrame.\n",
        "  Parameters:\n",
        "    -----------\n",
        "    file_path : str\n",
        "        The name of the given dataset (unigrams or bigrams or both).\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    df_dataset : DataFrame\n",
        "        A DataFrame of calls with the given dataset info.\n",
        "  \"\"\"\n",
        "  df_dataset = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "  # convert start - end times to duration\n",
        "  # 1st Register the UDFs in Spark\n",
        "  calculate_duration_minutes_udf = udf(calculate_duration_minutes, DoubleType())\n",
        "  calculate_duration_string_udf = udf(calculate_duration_string, StringType())\n",
        "\n",
        "  # 2nd use udfs to add columns for duration in minutes and DDHHMM format\n",
        "  df_dataset = df_dataset.withColumn('duration_minutes', calculate_duration_minutes_udf(col('Start_Time'), col('End_Time')))\n",
        "  df_dataset = df_dataset.withColumn('duration_DDHHMM', calculate_duration_string_udf(col('Start_Time'), col('End_Time')))\n",
        "\n",
        "  print(\"The following dataframe has been read from the CSV file:\")\n",
        "  df_dataset.show()\n",
        "  return df_dataset\n",
        "\n",
        "@track_stage(\"Stage 2: Preprocessing and creating the graph\")\n",
        "def create_graph_from_dataframe(df_dataset):\n",
        "  \"\"\"\n",
        "  Create graph in GraphFrame from the calls in the current dataset.\n",
        "  Parameters:\n",
        "    -----------\n",
        "    df_dataset : DataFrame\n",
        "        A DataFrame of calls with the given dataset info.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    df_dataset : DataFrame\n",
        "        A DataFrame of calls with the given dataset info.\n",
        "  \"\"\"\n",
        "\n",
        "  # Create Graph using GraphFrames for community detection\n",
        "  vertices = df_dataset.selectExpr(\"Client1 as id\").union(df_dataset.selectExpr(\"Client2 as id\")).distinct()\n",
        "  edges = df_dataset.selectExpr(\"Client1 as src\", \"Client2 as dst\", \"duration_minutes as weight\")\n",
        "\n",
        "  # Cache vertices and edges\n",
        "  vertices.cache()\n",
        "  edges.cache()\n",
        "\n",
        "  # Create a GraphFrame\n",
        "  g = GraphFrame(vertices, edges)\n",
        "\n",
        "  # Find connected components (communities) using GraphFrames\n",
        "  connected_components_result = g.connectedComponents()\n",
        "\n",
        "  # Create a mapping from original community IDs to sequential ones\n",
        "  community_mapping = connected_components_result.select(\"component\").distinct() \\\n",
        "      .orderBy(\"component\") \\\n",
        "      .withColumn(\"new_id\", row_number().over(Window.orderBy(\"component\"))) \\\n",
        "      .cache()\n",
        "\n",
        "  # Join the result (community IDs) with the original dataframe and map to new sequential IDs\n",
        "  df_with_communities = df_dataset.join(connected_components_result, df_dataset['Client1'] == connected_components_result['id'], 'inner') \\\n",
        "      .join(community_mapping, connected_components_result['component'] == community_mapping['component'], 'inner') \\\n",
        "      .drop(connected_components_result['id']) \\\n",
        "      .drop(community_mapping['component']) \\\n",
        "      .withColumnRenamed('new_id', 'community_id')\n",
        "\n",
        "  # Calculate the number of unique clients (community size) per community\n",
        "  community_sizes = df_with_communities.select(\"community_id\", \"Client1\").union(df_with_communities.select(\"community_id\", \"Client2\")) \\\n",
        "      .distinct() \\\n",
        "      .groupBy(\"community_id\").agg(countDistinct(\"Client1\").alias(\"community_size\"))\n",
        "\n",
        "  # Merge the community sizes into the main DataFrame\n",
        "  df_final = df_with_communities.join(community_sizes, 'community_id')\n",
        "\n",
        "  # Get list of tuples for each community member by considering both Client1 and Client2\n",
        "  community_members = df_final.select(\"community_id\", \"Client1\", \"Client2\", \"duration_DDHHMM\", \"duration_minutes\") \\\n",
        "      .distinct() \\\n",
        "      .groupBy(\"community_id\") \\\n",
        "      .agg(collect_list(struct(col(\"Client1\"),\n",
        "                            col(\"Client2\"),\n",
        "                            col(\"duration_DDHHMM\"),\n",
        "                            col(\"duration_minutes\"))).alias(\"members\")) \\\n",
        "      .orderBy(\"community_id\")\n",
        "\n",
        "  # Show the final DataFrame with community IDs, duration, and community sizes\n",
        "  print(\"\\nFinal DataFrame with Sequential Community IDs:\")\n",
        "  df_final.select('Client1',\n",
        "                  'Client2',\n",
        "                  'duration_DDHHMM',\n",
        "                  'duration_minutes',\n",
        "                  'community_id',\n",
        "                  'community_size') \\\n",
        "      .orderBy(\"community_id\") \\\n",
        "      .show()\n",
        "\n",
        "  # Show the list of community members as tuples\n",
        "  print(\"\\nCommunity Members with Sequential IDs:\")\n",
        "  community_members.show(truncate=False)\n",
        "\n",
        "  # Save results to CSV files\n",
        "  # Save the main analysis results\n",
        "  df_final.select('Client1',\n",
        "                  'Client2',\n",
        "                  'duration_DDHHMM',\n",
        "                  'duration_minutes',\n",
        "                  'community_id',\n",
        "                  'community_size') \\\n",
        "      .orderBy(\"community_id\") \\\n",
        "      .write.mode(\"overwrite\").option(\"header\", \"true\")\n",
        "      .csv(f\"{dataset_name}_community_analysis_results\")\n",
        "\n",
        "  # Save community members in a flattened format\n",
        "  df_final.select('community_id',\n",
        "                  'Client1',\n",
        "                  'Client2',\n",
        "                  'duration_DDHHMM',\n",
        "                  'duration_minutes') \\\n",
        "      .distinct() \\\n",
        "      .orderBy(\"community_id\") \\\n",
        "      .write.mode(\"overwrite\").option(\"header\", \"true\") \\\n",
        "      .csv(f\"{dataset_name}_community_members_results\")\n",
        "\n",
        "  # Optionally, if you want to save additional community statistics\n",
        "  community_stats = df_final.groupBy('community_id') \\\n",
        "      .agg(\n",
        "          countDistinct('Client1', 'Client2').alias('unique_members'),\n",
        "          count('*').alias('total_calls'),\n",
        "          sum('duration_minutes').alias('total_duration_minutes'),\n",
        "          avg('duration_minutes').alias('avg_call_duration'),\n",
        "          percentile_approx('duration_minutes', 0.25).alias('duration_25th_percentile'),\n",
        "          percentile_approx('duration_minutes', 0.5).alias('median_call_duration'),\n",
        "          percentile_approx('duration_minutes', 0.75).alias('duration_75th_percentile')\n",
        "      ) \\\n",
        "      .orderBy('community_id')\n",
        "\n",
        "  community_stats.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"community_statistics_results\")\n",
        "\n",
        "  print(\"This is the community stats:\")\n",
        "  community_stats.show(truncate=False)\n",
        "  return df_final, community_members, community_stats\n",
        "\n",
        "# Create CSR adjacency matrices for each community and serialize them\n",
        "@track_stage(\"Stage 3: Creating CSR matrices\")\n",
        "def format_members_to_csr_matrix(community_members):\n",
        "  \"\"\"\n",
        "  Create CSR adjacency matrices for each community and serialize them.\n",
        "\n",
        "  Parameters:\n",
        "    community_members: Dataframe\n",
        "    A dataframe of a specific community's members\n",
        "  \"\"\"\n",
        "  # Convert the collected list of Row objects to a list of dictionaries before passing to UDF\n",
        "  schema = StructType([\n",
        "      StructField(\"Client1\", StringType(), True),\n",
        "      StructField(\"Client2\", StringType(), True),\n",
        "      StructField(\"duration_DDHHMM\", StringType(), True),\n",
        "      StructField(\"duration_minutes\", DoubleType(), True)\n",
        "  ])\n",
        "  convert_members_udf = udf(lambda members: [member.asDict() for member in members], ArrayType(schema))\n",
        "  community_members = community_members.withColumn(\"members_dict\", convert_members_udf(col(\"members\")))\n",
        "  #Register UDF to create and serialize CSR matrices (both unweighted and weighted)\n",
        "  create_csr_unweighted_udf = udf(lambda members: create_csr_matrix(members, use_weights=False), StringType())\n",
        "  create_csr_weighted_udf = udf(lambda members: create_csr_matrix(members, use_weights=True), StringType())\n",
        "\n",
        "  # Add CSR matrix representations (unweighted and weighted) to each community\n",
        "  community_members = community_members.withColumn(\"csr_matrix_unweighted\", create_csr_unweighted_udf(col(\"members_dict\")))\n",
        "  community_members = community_members.withColumn(\"csr_matrix_weighted\", create_csr_weighted_udf(col(\"members_dict\")))\n",
        "\n",
        "  community_members.show(truncate=False)\n",
        "\n",
        "  # Print some information about the matrix\n",
        "  # print(f\"CSR Matrix shape: {csr_matrix_result.shape}\")\n",
        "  # print(f\"Number of non-zero elements: {csr_matrix_result.nnz}\")\n",
        "  # pretty_print_csr_matrix(csr_matrix_result)\n",
        "\n",
        "  return community_members\n",
        "\n",
        "from pyspark.sql import functions as F\n",
        "from itertools import combinations\n",
        "\n",
        "def calculate_similarities(subgroup_community_members):\n",
        "  \"\"\"\n",
        "  Comparing CSR matrices to detect similarity\n",
        "  \"\"\"\n",
        "\n",
        "  # Register UDF to compare structural similarity\n",
        "  compare_structural_similarity_udf = udf(lambda csr_1, csr_2: compare_weighted_structural_similarity(csr_1, csr_2), DoubleType())\n",
        "  compare_weighted_similarity_udf = udf(lambda csr_1, csr_2: compare_weighted_structural_similarity(csr_1, csr_2), DoubleType())\n",
        "\n",
        "  # Cross join to compare each pair of communities and calculate both similarities\n",
        "  cross_joined = subgroup_community_members.alias(\"a\").crossJoin(subgroup_community_members.alias(\"b\")) \\\n",
        "      .filter(col(\"a.community_id\") < col(\"b.community_id\")) \\\n",
        "      .withColumn(\"unweighted_similarity_score\", compare_structural_similarity_udf(col(\"a.csr_matrix_unweighted\"), col(\"b.csr_matrix_unweighted\"))) \\\n",
        "      .withColumn(\"weighted_similarity_score\", compare_weighted_similarity_udf(col(\"a.csr_matrix_weighted\"), col(\"b.csr_matrix_weighted\")))\n",
        "\n",
        "  # Add combined similarity score (50/50 importance)\n",
        "  cross_joined = cross_joined.withColumn(\"combined_similarity_score\",\n",
        "                                        0.5 * col(\"unweighted_similarity_score\") + 0.5 * col(\"weighted_similarity_score\"))\n",
        "\n",
        "  # Show the similarity scores between communities\n",
        "  # Rename and select columns to create a new DataFrame for writing\n",
        "  df_to_export = cross_joined.select(\n",
        "      col(\"a.community_id\").alias(\"community_id_1\"),\n",
        "      col(\"b.community_id\").alias(\"community_id_2\"),\n",
        "      round(col(\"unweighted_similarity_score\"), 2).alias(\"unweighted_similarity_score\"),\n",
        "      round(col(\"weighted_similarity_score\"), 2).alias(\"weighted_similarity_score\"),\n",
        "      round(col(\"combined_similarity_score\"), 2).alias(\"combined_similarity_score\")\n",
        "  )\n",
        "\n",
        "  # Display the DataFrame\n",
        "  print(\"these are the groups:\")\n",
        "  df_to_export.orderBy([\"community_id_1\", \"community_id_2\"]).show(truncate=False)\n",
        "\n",
        "  # Write the new DataFrame to CSV\n",
        "  print(\"exporting to csv file\")\n",
        "  df_to_export.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"groups_found\")\n",
        "\n",
        "  return cross_joined\n",
        "\n",
        "# Function to create similarity-based subgroups by comparing multiple columns\n",
        "@track_stage(\"Stage 4: Calculate similarities between communities\")\n",
        "def create_similarity_subgroups(df, columns, tolerances):\n",
        "    \"\"\"\n",
        "    Create similarity-based subgroups based on specified columns and tolerances, then apply a custom function.\n",
        "\n",
        "    Parameters:\n",
        "        df (DataFrame): The Spark DataFrame with community data.\n",
        "        columns (list): List of column names to consider for similarity.\n",
        "        tolerances (dict): Dictionary specifying the tolerance (± range) for each column.\n",
        "    \"\"\"\n",
        "    # Collect the DataFrame into a list of rows\n",
        "    communities = df.collect()\n",
        "\n",
        "    # Initialize a list to store similarity groups\n",
        "    similarity_groups = []\n",
        "    df_groups = None\n",
        "\n",
        "    # Compare each pair of communities\n",
        "    for i, j in combinations(range(len(communities)), 2):\n",
        "        community_i = communities[i]\n",
        "        community_j = communities[j]\n",
        "\n",
        "        # Check if the communities are similar based on all specified columns and tolerances\n",
        "        is_similar = all(\n",
        "            abs(community_i[column] - community_j[column]) <= tolerances[column]\n",
        "            for column in columns\n",
        "        )\n",
        "\n",
        "        # If they are similar, add them to the same group\n",
        "        if is_similar:\n",
        "            found_group = False\n",
        "            for group in similarity_groups:\n",
        "                if community_i.community_id in group or community_j.community_id in group:\n",
        "                    group.add(community_i.community_id)\n",
        "                    group.add(community_j.community_id)\n",
        "                    found_group = True\n",
        "                    break\n",
        "            if not found_group:\n",
        "                similarity_groups.append({community_i.community_id, community_j.community_id})\n",
        "\n",
        "    # Create a DataFrame for each subgroup and apply the custom function\n",
        "    for group in similarity_groups:\n",
        "        subgroup_df = df.filter(F.col(\"community_id\").isin(group))\n",
        "        subgroup_cross_joined = calculate_similarities(subgroup_df)\n",
        "        # Initialize or append to df_groups\n",
        "        if df_groups is None:\n",
        "            df_groups = subgroup_cross_joined\n",
        "        else:\n",
        "            df_groups = df_groups.union(subgroup_cross_joined)\n",
        "\n",
        "\n",
        "    # export all found groups\n",
        "    df_groups.coalesce(1).write \\\n",
        "                .mode(write_mode) \\\n",
        "                .option(\"header\", header) \\\n",
        "                .csv(f\"df_groups.csv\")\n",
        "    return df_groups"
      ],
      "metadata": {
        "id": "Ike2lfPxo19c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read dataset dataframe, and iterate over each one to create communities and form groups\n",
        "df_datasets = pd.read_csv(\"dataset_metadata.csv\")\n",
        "print(\"These are the found datasets\")\n",
        "df_datasets.head(10)\n",
        "\n",
        "# set global params\n",
        "clear_csv = False\n",
        "dataset_file_path = \"toy_dataset.csv\"\n",
        "dataset_name = \"toy_dataset\"\n",
        "\n",
        "for i, dataset in df_datasets.iterrows():\n",
        "  print(f\"Starting to process {i+1} dataset with the following params: \\n{dataset}\")\n",
        "  # step 1 - read the dataset\n",
        "  dataset_file_path = dataset[\"csv_filename\"]\n",
        "  # Get the base name from the path (e.g., 'file.txt' from '/path/to/file.txt')\n",
        "  basename =  os.path.basename(filepath)\n",
        "  # Split the filename and extension\n",
        "  dataset_name = os.path.splitext(basename)[0]\n",
        "  clear_csv = i == 0 # only clear the if this is the 1st dataset\n",
        "  df_dataset = spark.read.csv(dataset_file_path, header=True, inferSchema=True)\n",
        "  df_dataset.show(20)\n",
        "  df_dataset = read_csv_to_dataframe(dataset_file_path)\n",
        "\n",
        "  # step 2 - preprocess (convert to duartion in min, create grpah, and find commutnies)\n",
        "  df_final, community_members, community_stats = create_graph_from_dataframe(df_dataset)\n",
        "\n",
        "  # step 3 - create CSR matrix for each communite\n",
        "  csr_matrix_result = format_members_to_csr_matrix(community_members)\n",
        "\n",
        "  # step 4 - calculate similarities between communties for find groups\n",
        "  # Define the columns and tolerances for similarity-based grouping\n",
        "  columns = ['total_duration_minutes', 'avg_call_duration']\n",
        "  tolerances = {'total_duration_minutes': 50, 'avg_call_duration': 100}\n",
        "\n",
        "  # Apply the similarity-based grouping and custom function\n",
        "  cross_joined = create_similarity_subgroups(community_stats, columns, tolerances, custom_function)\n",
        "  # cross_joined = calculate_similarities(csr_matrix_result)\n",
        "\n",
        "# # end the current spark session\n",
        "# spark.stop()"
      ],
      "metadata": {
        "id": "RAMTvsL2v2YH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the resources usgae of current dataset\n",
        "df_monitor = spark.read.csv(\"stage_metrics\",  header=True)\n",
        "df_monitor = df_monitor.orderBy('stage_name')\n",
        "print(f\"count: {df_monitor.count()}\")\n",
        "df_monitor.show(truncate=False)\n",
        "\n",
        "# Convert the Spark DataFrame to a Pandas DataFrame for plotting\n",
        "pdf_monitor = df_monitor.toPandas()\n",
        "\n",
        "# Select the columns of interest for plotting\n",
        "columns_of_interest = ['stage_name', 'numTasks', 'stageDuration', 'peakExecutionMemory', 'executorCpuTime']\n",
        "pdf_plot = pdf_monitor[columns_of_interest].copy()  # Create a copy to avoid the warning\n",
        "\n",
        "# Format memory and time measurements using .loc\n",
        "pdf_plot.loc[:, 'peakExecutionMemory'] = pdf_plot['peakExecutionMemory'].astype(float) / (1024**3)  # Bytes to GB\n",
        "pdf_plot.loc[:, 'stageDuration'] = pdf_plot['stageDuration'].astype(float) / (1000 * 60)  # ms to minutes\n",
        "pdf_plot.loc[:, 'executorCpuTime'] = pdf_plot['executorCpuTime'].astype(float) / (1000 * 60)  # ms to minutes\n",
        "pdf_plot.loc[:, 'numTasks'] = pdf_plot['numTasks'].astype(int)\n",
        "\n",
        "# Extract stage numbers from stage_name\n",
        "pdf_plot['stage_number'] = pdf_plot['stage_name'].apply(lambda x: int(re.search(r'\\d+', x).group()))\n",
        "\n",
        "# Melt the DataFrame for easier plotting with Seaborn\n",
        "pdf_plot_melted = pd.melt(pdf_plot, id_vars=['stage_name', 'stage_number'], var_name='Metric', value_name='Value')\n",
        "\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "# Create the plot with facets, stage numbers as x-axis, and legend with full stage names\n",
        "g = sns.FacetGrid(pdf_plot_melted, col='Metric',\n",
        "                   height=6, aspect=1.5,\n",
        "                  col_wrap=2, sharey=False, sharex=False)\n",
        "g.map(sns.barplot, 'stage_number', 'Value', palette='hls', hue='stage_name',\n",
        "      data=pdf_plot_melted, dodge=False)  # Pass data argument\n",
        "g.set_xticklabels(pdf_plot['stage_number'].unique(), size=16)\n",
        "g.set_titles(\"{col_name}\", size=18)\n",
        "g.fig.suptitle('Spark Stage Metrics', y=1.02, size=18)\n",
        "g.add_legend(loc='upper right', bbox_to_anchor=(1.2, 0.92))\n",
        "g.legend.set_title('Stage Names', prop={'weight': 'bold', 'size': 22})  # Add title and format it\n",
        "for text in g.legend.get_texts():\n",
        "    text.set_fontsize(18)  # Legend label font size# Set y-axis labels with units and add x-axis label\n",
        "for ax in g.axes.flat:\n",
        "    metric = ax.get_title()\n",
        "    if metric == 'peakExecutionMemory':\n",
        "        ax.set_ylabel('Peak Execution Memory (GB)', size=18)\n",
        "    elif metric in ('stageDuration', 'executorCpuTime'):\n",
        "        ax.set_ylabel('Time (minutes)', size=18)\n",
        "    else:\n",
        "        ax.set_ylabel(metric, size=18)\n",
        "\n",
        "    ax.set_xlabel('Stage Number', size=18)  # Change x-axis label to \"Stage Number\"\n",
        "\n",
        "g.set_yticklabels(fontsize=18)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8_ZLxdC5nDWG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}