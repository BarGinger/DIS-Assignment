{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BarGinger/DIS-Assignment/blob/main/Src/dis_notebook_02_11_24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFDENKMNs-52"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "!pip install graphframes\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import (\n",
        "    col,\n",
        "    udf,\n",
        "    row_number,\n",
        "    countDistinct,\n",
        "    collect_list,\n",
        "    struct,\n",
        "    count,\n",
        "    sum,\n",
        "    avg,\n",
        "    expr,\n",
        "    percentile_approx,\n",
        "    max as spark_max,\n",
        "    explode,\n",
        "    round\n",
        ")\n",
        "from pyspark.sql.types import StringType, IntegerType, BinaryType, DoubleType, ArrayType, StructType, StructField\n",
        "from pyspark.sql import Window\n",
        "from datetime import datetime\n",
        "from graphframes import GraphFrame\n",
        "from scipy.sparse import csr_matrix, vstack, hstack\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import base64\n",
        "from sparkmeasure import StageMetrics # for resources monitoring\n",
        "from functools import wraps"
      ],
      "metadata": {
        "id": "gjrP64v5QyEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Monitor CPU, Memory and running time\n",
        "def track_stage(stage_name):\n",
        "    def decorator(func):\n",
        "        @wraps(func)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            print(f\"Starting {stage_name}\")\n",
        "            stagemetrics.begin()  # Begin collecting metrics for this stage\n",
        "\n",
        "            result = func(*args, **kwargs)  # Run the actual function\n",
        "\n",
        "            stagemetrics.end()  # Stop collecting metrics for this stage\n",
        "\n",
        "            # Print or retrieve the metrics summary for the stage\n",
        "            metrics = stagemetrics.createStageMetricsDF()\n",
        "            metrics.show(truncate=False)\n",
        "            print(f\"Completed {stage_name}\\n\")\n",
        "            return result\n",
        "        return wrapper\n",
        "    return decorator"
      ],
      "metadata": {
        "id": "R9jMJMWzY2LP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Utils functions\n",
        "\n",
        "# Convert YYMMDDHHMM to a proper datetime object\n",
        "def convert_to_datetime(yyMMddHHMM):\n",
        "    return datetime.strptime(str(yyMMddHHMM), '%y%m%d%H%M')\n",
        "\n",
        "# Define UDF for calculating duration in minutes\n",
        "def calculate_duration_minutes(start_time, end_time):\n",
        "    start_dt = convert_to_datetime(start_time)\n",
        "    end_dt = convert_to_datetime(end_time)\n",
        "    duration = end_dt - start_dt\n",
        "    return duration.total_seconds() / 60\n",
        "\n",
        "# Define UDF for calculating duration in DDHHMM format\n",
        "def calculate_duration_string(start_time, end_time):\n",
        "    start_dt = convert_to_datetime(start_time)\n",
        "    end_dt = convert_to_datetime(end_time)\n",
        "    duration = end_dt - start_dt\n",
        "\n",
        "    days = duration.days\n",
        "    hours, remainder = divmod(duration.seconds, 3600)\n",
        "    minutes = remainder // 60\n",
        "    return f'{days:02d}{hours:02d}{minutes:02d}'\n",
        "\n",
        "# prompt: print csr_matrix_result pretty\n",
        "def pretty_print_csr_matrix(csr_matrix_result):\n",
        "  \"\"\"Prints a CSR matrix in a readable format.\"\"\"\n",
        "\n",
        "  rows, cols = csr_matrix_result.nonzero()\n",
        "  data = csr_matrix_result.data\n",
        "\n",
        "  df = pd.DataFrame({\n",
        "      'Row': rows,\n",
        "      'Col': cols,\n",
        "      'Value': data\n",
        "  })\n",
        "\n",
        "  print(df)\n",
        "\n",
        "def create_csr_matrix_from_edges_with_spark(members_df):\n",
        "    \"\"\"\n",
        "    Creates a CSR matrix from a Spark DataFrame based on unique vertices.\n",
        "\n",
        "    Args:\n",
        "        members_df: Spark DataFrame with 'community_id' and 'members' columns.\n",
        "\n",
        "    Returns:\n",
        "        A CSR matrix.\n",
        "    \"\"\"\n",
        "\n",
        "    # Explode the members array to get each connection in separate rows\n",
        "    exploded_df = members_df.select(\n",
        "        \"community_id\",\n",
        "        explode(\"members\").alias(\"member\")\n",
        "    ).select(\n",
        "        \"community_id\",\n",
        "        col(\"member.Client1\").alias(\"Client1\"),\n",
        "        col(\"member.Client2\").alias(\"Client2\"),\n",
        "        col(\"member.duration_minutes\").alias(\"duration_minutes\")\n",
        "    )\n",
        "\n",
        "    # Get unique clients and create a mapping to indices\n",
        "    unique_clients = exploded_df.select(\"Client1\").union(exploded_df.select(\"Client2\")).distinct().rdd.flatMap(lambda x: x).collect()\n",
        "    client_to_index = {client: i for i, client in enumerate(unique_clients)}\n",
        "    num_clients = len(unique_clients)\n",
        "\n",
        "    # Extract data for CSR matrix\n",
        "    rows = exploded_df.select(\"Client1\").rdd.map(lambda row: client_to_index[row[0]]).collect()\n",
        "    cols = exploded_df.select(\"Client2\").rdd.map(lambda row: client_to_index[row[0]]).collect()\n",
        "    data = exploded_df.select(\"duration_minutes\").rdd.flatMap(lambda x: x).collect()\n",
        "\n",
        "    # Create CSR matrix\n",
        "    csr = csr_matrix((data, (rows, cols)), shape=(num_clients, num_clients))\n",
        "\n",
        "    return csr\n",
        "\n",
        "# create csr matrix from given members list\n",
        "def create_csr_matrix(members, use_weights=False):\n",
        "    clients = list(set([member['Client1'] for member in members] + [member['Client2'] for member in members]))\n",
        "    client_index = {client: idx for idx, client in enumerate(clients)}\n",
        "\n",
        "    row_indices = []\n",
        "    col_indices = []\n",
        "    data = []\n",
        "\n",
        "    for member in members:\n",
        "        row_indices.append(client_index[member['Client1']])\n",
        "        col_indices.append(client_index[member['Client2']])\n",
        "        if use_weights:\n",
        "            data.append(float(member['duration_minutes']))  # Use duration in minutes as the weight of the edge\n",
        "        else:\n",
        "            data.append(1)  # Use 1 for unweighted similarity\n",
        "\n",
        "    num_clients = len(clients)\n",
        "    csr = csr_matrix((data, (row_indices, col_indices)), shape=(num_clients, num_clients))\n",
        "\n",
        "    # Serialize the CSR matrix\n",
        "    serialized_csr = base64.b64encode(pickle.dumps(csr)).decode('utf-8')\n",
        "    return serialized_csr\n",
        "\n",
        "# compare given two csr matrices (each relating to a community) to get similarity score\n",
        "def compare_weighted_structural_similarity(csr_matrix_1, csr_matrix_2):\n",
        "    # Deserialize CSR matrices\n",
        "    csr_1 = pickle.loads(base64.b64decode(csr_matrix_1))\n",
        "    csr_2 = pickle.loads(base64.b64decode(csr_matrix_2))\n",
        "\n",
        "\n",
        "    # Align matrix dimensions to the largest size\n",
        "    max_rows = max(csr_1.shape[0], csr_2.shape[0])\n",
        "    max_cols = max(csr_1.shape[1], csr_2.shape[1])\n",
        "\n",
        "    # Pad csr_1 to match max dimensions\n",
        "    if csr_1.shape[0] < max_rows or csr_1.shape[1] < max_cols:\n",
        "        csr_1 = vstack([csr_1, csr_matrix((max_rows - csr_1.shape[0], csr_1.shape[1]))]) if csr_1.shape[0] < max_rows else csr_1\n",
        "        csr_1 = hstack([csr_1, csr_matrix((csr_1.shape[0], max_cols - csr_1.shape[1]))]) if csr_1.shape[1] < max_cols else csr_1\n",
        "\n",
        "    # Pad csr_2 to match max dimensions\n",
        "    if csr_2.shape[0] < max_rows or csr_2.shape[1] < max_cols:\n",
        "        csr_2 = vstack([csr_2, csr_matrix((max_rows - csr_2.shape[0], csr_2.shape[1]))]) if csr_2.shape[0] < max_rows else csr_2\n",
        "        csr_2 = hstack([csr_2, csr_matrix((csr_2.shape[0], max_cols - csr_2.shape[1]))]) if csr_2.shape[1] < max_cols else csr_2\n",
        "\n",
        "    # Calculate structural similarity (e.g., using cosine similarity)\n",
        "    dot_product = csr_1.multiply(csr_2).sum()\n",
        "    norm_1 = np.sqrt(csr_1.multiply(csr_1).sum())\n",
        "    norm_2 = np.sqrt(csr_2.multiply(csr_2).sum())\n",
        "    similarity = dot_product / (norm_1 * norm_2) if norm_1 != 0 and norm_2 != 0 else 0\n",
        "    return float(similarity)"
      ],
      "metadata": {
        "id": "mhgOioE4ZU5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the\n",
        "@track_stage(\"Stage 1: Reading the calls dataset\")\n",
        "def read_csv_to_dataframe(file_path= 'toy_dataset.csv'):\n",
        "  \"\"\"\n",
        "  Read dataset from given path into a Spark DataFrame.\n",
        "  Parameters:\n",
        "    -----------\n",
        "    file_path : str\n",
        "        The name of the given dataset (unigrams or bigrams or both).\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    df_dataset : DataFrame\n",
        "        A DataFrame of calls with the given dataset info.\n",
        "  \"\"\"\n",
        "  df_dataset = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "  # convert start - end times to duration\n",
        "  # 1st Register the UDFs in Spark\n",
        "  calculate_duration_minutes_udf = udf(calculate_duration_minutes, DoubleType())\n",
        "  calculate_duration_string_udf = udf(calculate_duration_string, StringType())\n",
        "\n",
        "  # 2nd use udfs to add columns for duration in minutes and DDHHMM format\n",
        "  df_dataset = df_dataset.withColumn('duration_minutes', calculate_duration_minutes_udf(col('Start_Time'), col('End_Time')))\n",
        "  df_dataset = df_dataset.withColumn('duration_DDHHMM', calculate_duration_string_udf(col('Start_Time'), col('End_Time')))\n",
        "\n",
        "  print(\"The following dataframe has been read from the CSV file:\")\n",
        "  df_dataset.show()\n",
        "  return df_dataset\n",
        "\n",
        "@track_stage(\"Stage 2: Preprocessing and creating the graph\")\n",
        "def create_graph_from_dataframe(df_dataset):\n",
        "  \"\"\"\n",
        "  Create graph in GraphFrame from the calls in the current dataset.\n",
        "  Parameters:\n",
        "    -----------\n",
        "    df_dataset : DataFrame\n",
        "        A DataFrame of calls with the given dataset info.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    df_dataset : DataFrame\n",
        "        A DataFrame of calls with the given dataset info.\n",
        "  \"\"\"\n",
        "\n",
        "  # Create Graph using GraphFrames for community detection\n",
        "  vertices = df_dataset.selectExpr(\"Client1 as id\").union(df_dataset.selectExpr(\"Client2 as id\")).distinct()\n",
        "  edges = df_dataset.selectExpr(\"Client1 as src\", \"Client2 as dst\", \"duration_minutes as weight\")\n",
        "\n",
        "  # Cache vertices and edges\n",
        "  vertices.cache()\n",
        "  edges.cache()\n",
        "\n",
        "  # Create a GraphFrame\n",
        "  g = GraphFrame(vertices, edges)\n",
        "\n",
        "  # Find connected components (communities) using GraphFrames\n",
        "  connected_components_result = g.connectedComponents()\n",
        "\n",
        "  # Create a mapping from original community IDs to sequential ones\n",
        "  community_mapping = connected_components_result.select(\"component\").distinct() \\\n",
        "      .orderBy(\"component\") \\\n",
        "      .withColumn(\"new_id\", row_number().over(Window.orderBy(\"component\"))) \\\n",
        "      .cache()\n",
        "\n",
        "  # Join the result (community IDs) with the original dataframe and map to new sequential IDs\n",
        "  df_with_communities = df_dataset.join(result, df_dataset['Client1'] == connected_components_result['id'], 'inner') \\\n",
        "      .join(community_mapping, connected_components_result['component'] == community_mapping['component'], 'inner') \\\n",
        "      .drop(connected_components_result['id']) \\\n",
        "      .drop(community_mapping['component']) \\\n",
        "      .withColumnRenamed('new_id', 'community_id')\n",
        "\n",
        "  # Calculate the number of unique clients (community size) per community\n",
        "  community_sizes = df_with_communities.select(\"community_id\", \"Client1\").union(df_with_communities.select(\"community_id\", \"Client2\")) \\\n",
        "      .distinct() \\\n",
        "      .groupBy(\"community_id\").agg(countDistinct(\"Client1\").alias(\"community_size\"))\n",
        "\n",
        "  # Merge the community sizes into the main DataFrame\n",
        "  df_final = df_with_communities.join(community_sizes, 'community_id')\n",
        "\n",
        "  # Get list of tuples for each community member by considering both Client1 and Client2\n",
        "  community_members = df_final.select(\"community_id\", \"Client1\", \"Client2\", \"duration_DDHHMM\", \"duration_minutes\") \\\n",
        "      .distinct() \\\n",
        "      .groupBy(\"community_id\") \\\n",
        "      .agg(collect_list(struct(col(\"Client1\"),\n",
        "                            col(\"Client2\"),\n",
        "                            col(\"duration_DDHHMM\"),\n",
        "                            col(\"duration_minutes\"))).alias(\"members\")) \\\n",
        "      .orderBy(\"community_id\")\n",
        "\n",
        "  # Show the final DataFrame with community IDs, duration, and community sizes\n",
        "  print(\"\\nFinal DataFrame with Sequential Community IDs:\")\n",
        "  df_final.select('Client1',\n",
        "                  'Client2',\n",
        "                  'duration_DDHHMM',\n",
        "                  'duration_minutes',\n",
        "                  'community_id',\n",
        "                  'community_size') \\\n",
        "      .orderBy(\"community_id\") \\\n",
        "      .show()\n",
        "\n",
        "  # Show the list of community members as tuples\n",
        "  print(\"\\nCommunity Members with Sequential IDs:\")\n",
        "  community_members.show(truncate=False)\n",
        "\n",
        "  # Save results to CSV files\n",
        "  # Save the main analysis results\n",
        "  df_final.select('Client1',\n",
        "                  'Client2',\n",
        "                  'duration_DDHHMM',\n",
        "                  'duration_minutes',\n",
        "                  'community_id',\n",
        "                  'community_size') \\\n",
        "      .orderBy(\"community_id\") \\\n",
        "      .write.mode(\"overwrite\").csv(\"community_analysis_results\")\n",
        "\n",
        "  # Save community members in a flattened format\n",
        "  df_final.select('community_id',\n",
        "                  'Client1',\n",
        "                  'Client2',\n",
        "                  'duration_DDHHMM',\n",
        "                  'duration_minutes') \\\n",
        "      .distinct() \\\n",
        "      .orderBy(\"community_id\") \\\n",
        "      .write.mode(\"overwrite\").csv(\"community_members_results\")\n",
        "\n",
        "  # Optionally, if you want to save additional community statistics\n",
        "  community_stats = df_final.groupBy('community_id') \\\n",
        "      .agg(\n",
        "          countDistinct('Client1', 'Client2').alias('unique_members'),\n",
        "          count('*').alias('total_calls'),\n",
        "          sum('duration_minutes').alias('total_duration_minutes'),\n",
        "          avg('duration_minutes').alias('avg_call_duration'),\n",
        "          percentile_approx('duration_minutes', 0.25).alias('duration_25th_percentile'),\n",
        "          percentile_approx('duration_minutes', 0.5).alias('median_call_duration'),\n",
        "          percentile_approx('duration_minutes', 0.75).alias('duration_75th_percentile')\n",
        "      ) \\\n",
        "      .orderBy('community_id')\n",
        "\n",
        "  community_stats.write.mode(\"overwrite\").csv(\"community_statistics_results\")\n",
        "\n",
        "  print(\"This is the community stats:\")\n",
        "  community_stats.show(truncate=False)\n",
        "  return df_final, community_members, community_stats\n",
        "\n",
        "# Create CSR adjacency matrices for each community and serialize them\n",
        "@track_stage(\"Stage 3: Creating CSR matrices\")\n",
        "def format_members_to_csr_matrix(community_members):\n",
        "  \"\"\"\n",
        "  Create CSR adjacency matrices for each community and serialize them.\n",
        "\n",
        "  Parameters:\n",
        "    community_members: Dataframe\n",
        "    A dataframe of a specific community's members\n",
        "  \"\"\"\n",
        "  # Convert the collected list of Row objects to a list of dictionaries before passing to UDF\n",
        "  schema = StructType([\n",
        "      StructField(\"Client1\", StringType(), True),\n",
        "      StructField(\"Client2\", StringType(), True),\n",
        "      StructField(\"duration_DDHHMM\", StringType(), True),\n",
        "      StructField(\"duration_minutes\", DoubleType(), True)\n",
        "  ])\n",
        "  convert_members_udf = udf(lambda members: [member.asDict() for member in members], ArrayType(schema))\n",
        "  community_members = community_members.withColumn(\"members_dict\", convert_members_udf(col(\"members\")))\n",
        "  #Register UDF to create and serialize CSR matrices (both unweighted and weighted)\n",
        "  create_csr_unweighted_udf = udf(lambda members: create_csr_matrix(members, use_weights=False), StringType())\n",
        "  create_csr_weighted_udf = udf(lambda members: create_csr_matrix(members, use_weights=True), StringType())\n",
        "\n",
        "  # Add CSR matrix representations (unweighted and weighted) to each community\n",
        "  community_members = community_members.withColumn(\"csr_matrix_unweighted\", create_csr_unweighted_udf(col(\"members_dict\")))\n",
        "  community_members = community_members.withColumn(\"csr_matrix_weighted\", create_csr_weighted_udf(col(\"members_dict\")))\n",
        "\n",
        "  community_members.show(truncate=False)\n",
        "\n",
        "  # Print some information about the matrix\n",
        "  print(f\"CSR Matrix shape: {csr_matrix_result.shape}\")\n",
        "  print(f\"Number of non-zero elements: {csr_matrix_result.nnz}\")\n",
        "  pretty_print_csr_matrix(csr_matrix_result)\n",
        "\n",
        "  return community_members\n",
        "\n",
        "@track_stage(\"Stage 4: Calculate similarities between communities\")\n",
        "def calculate_similarities(community_members):\n",
        "  \"\"\"\n",
        "  Comparing CSR matrices to detect similarity\n",
        "  \"\"\"\n",
        "\n",
        "  # Register UDF to compare structural similarity\n",
        "  compare_structural_similarity_udf = udf(lambda csr_1, csr_2: compare_weighted_structural_similarity(csr_1, csr_2), DoubleType())\n",
        "  compare_weighted_similarity_udf = udf(lambda csr_1, csr_2: compare_weighted_structural_similarity(csr_1, csr_2), DoubleType())\n",
        "\n",
        "  # Cross join to compare each pair of communities and calculate both similarities\n",
        "  cross_joined = community_members.alias(\"a\").crossJoin(community_members.alias(\"b\")) \\\n",
        "      .filter(col(\"a.community_id\") < col(\"b.community_id\")) \\\n",
        "      .withColumn(\"unweighted_similarity_score\", compare_structural_similarity_udf(col(\"a.csr_matrix_unweighted\"), col(\"b.csr_matrix_unweighted\"))) \\\n",
        "      .withColumn(\"weighted_similarity_score\", compare_weighted_similarity_udf(col(\"a.csr_matrix_weighted\"), col(\"b.csr_matrix_weighted\")))\n",
        "\n",
        "  # Add combined similarity score (50/50 importance)\n",
        "  cross_joined = cross_joined.withColumn(\"combined_similarity_score\",\n",
        "                                        0.5 * col(\"unweighted_similarity_score\") + 0.5 * col(\"weighted_similarity_score\"))\n",
        "\n",
        "  # Show the similarity scores between communities\n",
        "  cross_joined.select(col(\"a.community_id\").alias(\"community_id_1\"),\n",
        "                      col(\"b.community_id\").alias(\"community_id_2\"),\n",
        "                      round(col(\"unweighted_similarity_score\"), 2).alias(\"unweighted_similarity_score\"),  # Changed here\n",
        "                      round(col(\"weighted_similarity_score\"), 2).alias(\"weighted_similarity_score\"),  # Changed here\n",
        "                      round(col(\"combined_similarity_score\"), 2).alias(\"combined_similarity_score\")) \\\n",
        "        .orderBy([\"community_id_1\", \"community_id_2\"]) \\\n",
        "        .show(truncate=False)\n",
        "\n",
        "  cross_joined.write.mode(\"overwrite\").csv(\"groups_found\")\n",
        "\n",
        "  return cross_joined"
      ],
      "metadata": {
        "id": "Ike2lfPxo19c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PhoneCallsCommunityDetection\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.1-s_2.12\") \\\n",
        "    .config(\"spark.executor.memory\", \"20G\") \\\n",
        "    .config(\"spark.driver.memory\", \"50G\") \\\n",
        "    .config(\"spark.executor.memoryOverhead\", \"1G\") \\\n",
        "    .config(\"spark.default.parallelism\", \"100\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
        "    .config(\"spark.driver.maxResultSize\", \"2G\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Initialize StageMetrics\n",
        "stagemetrics = StageMetrics(spark)\n",
        "\n",
        "# Optional: Set logging level to reduce verbosity\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "# Set a checkpoint directory for Spark\n",
        "spark.sparkContext.setCheckpointDir(\"/tmp/spark-checkpoints\")"
      ],
      "metadata": {
        "id": "RAMTvsL2v2YH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1 - read the dataset\n",
        "dataset_file_path = 'toy_dataset.csv' # set this variable to desired dataset\n",
        "df_dataset = read_csv_to_dataframe(dataset_file_path)\n",
        "\n",
        "# step 2 - preprocess (convert to duartion in min, create grpah, and find commutnies)\n",
        "df_final, community_members, community_stats = create_graph_from_dataframe(df_dataset)\n",
        "\n",
        "# step 3 - create CSR matrix for each communite\n",
        "csr_matrix_result = format_members_to_csr_matrix(community_members)\n",
        "\n",
        "# step 4 - calc simmulries\n",
        "cross_joined = calculate_similarities(csr_matrix_result)"
      ],
      "metadata": {
        "id": "FwBOu_C9tleA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}