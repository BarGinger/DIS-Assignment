{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tazO6EBzefAH",
        "outputId": "7a555eb3-719c-4f21-e125-6f445629ab45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "openjdk-8-jdk-headless is already the newest version (8u422-b05-1~22.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Requirement already satisfied: graphframes in /usr/local/lib/python3.10/dist-packages (0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from graphframes) (1.26.4)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.10/dist-packages (from graphframes) (1.3.7)\n",
            "\u001b[31mERROR: You must give at least one requirement to install (see \"pip help install\")\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "!pip install graphframes\n",
        "!pip install\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.functions import (\n",
        "    col,\n",
        "    udf,\n",
        "    row_number,\n",
        "    countDistinct,\n",
        "    collect_list,\n",
        "    struct,\n",
        "    count,\n",
        "    sum,\n",
        "    avg,\n",
        "    expr,\n",
        "    lit,\n",
        "    percentile_approx,\n",
        "    max as spark_max,\n",
        "    explode,\n",
        "    least,\n",
        "    greatest\n",
        ")\n",
        "from pyspark.sql.types import StringType, IntegerType, BinaryType, DoubleType, ArrayType, StructType, StructField\n",
        "from pyspark.sql import Window\n",
        "from datetime import datetime\n",
        "from graphframes import GraphFrame\n",
        "from scipy.sparse import csr_matrix, vstack, hstack\n",
        "import numpy as np\n",
        "import pickle\n",
        "import base64\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "-hPbdVyielrT"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PhoneCallsCommunityDetection\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.1-s_2.12\") \\\n",
        "    .config(\"spark.executor.memory\", \"20G\") \\\n",
        "    .config(\"spark.driver.memory\", \"50G\") \\\n",
        "    .config(\"spark.executor.memoryOverhead\", \"1G\") \\\n",
        "    .config(\"spark.default.parallelism\", \"100\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
        "    .config(\"spark.driver.maxResultSize\", \"2G\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HcTCCvX4eold"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Set logging level to reduce verbosity\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "# Set a checkpoint directory for Spark\n",
        "spark.sparkContext.setCheckpointDir(\"/tmp/spark-checkpoints\")\n",
        "\n",
        "file_path = '/content/toy_dataset_Copy.csv'  # Adjust this to your file path\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "# Convert YYMMDDHHMM to a proper datetime object\n",
        "def convert_to_datetime(yyMMddHHMM):\n",
        "    return datetime.strptime(str(yyMMddHHMM), '%y%m%d%H%M')\n",
        "\n",
        "# Define UDF for calculating duration in minutes\n",
        "def calculate_duration_minutes(start_time, end_time):\n",
        "    start_dt = convert_to_datetime(start_time)\n",
        "    end_dt = convert_to_datetime(end_time)\n",
        "    duration = end_dt - start_dt\n",
        "    return duration.total_seconds() / 60\n",
        "\n",
        "# Register the UDF for duration in minutes\n",
        "calculate_duration_minutes_udf = udf(calculate_duration_minutes, DoubleType())\n",
        "\n",
        "# Add column for duration in minutes\n",
        "df = df.withColumn('duration_minutes', calculate_duration_minutes_udf(col('Start_Time'), col('End_Time')))\n",
        "\n",
        "# Adjust Client1 and Client2 to ensure Client1 is the smaller value and Client2 the larger\n",
        "df = df.withColumn(\"Client1_min\", least(col(\"Client1\"), col(\"Client2\"))) \\\n",
        "       .withColumn(\"Client2_max\", greatest(col(\"Client1\"), col(\"Client2\"))) \\\n",
        "       .drop(\"Client1\", \"Client2\") \\\n",
        "       .withColumnRenamed(\"Client1_min\", \"Client1\") \\\n",
        "       .withColumnRenamed(\"Client2_max\", \"Client2\")\n",
        "\n",
        "# Aggregate total duration for each unique pair (Client1, Client2)\n",
        "df_aggregated = df.groupBy(\"Client1\", \"Client2\") \\\n",
        "    .agg(F.sum(\"duration_minutes\").alias(\"total_duration_minutes\"))\n",
        "\n",
        "# Join the aggregated total duration back to the original DataFrame\n",
        "df = df.drop(\"duration_minutes\") \\\n",
        "       .join(df_aggregated, on=[\"Client1\", \"Client2\"], how=\"left\")\n",
        "\n",
        "# Create Graph using GraphFrames for community detection\n",
        "vertices = df.selectExpr(\"Client1 as id\").union(df.selectExpr(\"Client2 as id\")).distinct()\n",
        "edges = df.selectExpr(\"Client1 as src\", \"Client2 as dst\", \"total_duration_minutes as weight\")\n",
        "\n",
        "# Cache vertices and edges\n",
        "vertices.cache()\n",
        "edges.cache()\n",
        "\n",
        "# Create a GraphFrame\n",
        "g = GraphFrame(vertices, edges)\n",
        "\n",
        "# Find connected components (communities) using GraphFrames\n",
        "result = g.connectedComponents()\n",
        "\n",
        "# Create a mapping from original community IDs to sequential ones\n",
        "community_mapping = result.select(\"component\").distinct() \\\n",
        "    .orderBy(\"component\") \\\n",
        "    .withColumn(\"new_id\", row_number().over(Window.orderBy(\"component\"))) \\\n",
        "    .cache()\n",
        "\n",
        "# Join the result (community IDs) with the original DataFrame and map to new sequential IDs\n",
        "df_with_communities = df.join(result, df['Client1'] == result['id'], 'inner') \\\n",
        "    .join(community_mapping, result['component'] == community_mapping['component'], 'inner') \\\n",
        "    .drop(result['id']) \\\n",
        "    .drop(community_mapping['component']) \\\n",
        "    .withColumnRenamed('new_id', 'community_id')\n",
        "\n",
        "# Calculate the number of unique clients (community size) per community\n",
        "community_sizes = df_with_communities.select(\"community_id\", \"Client1\").union(df_with_communities.select(\"community_id\", \"Client2\")) \\\n",
        "    .distinct() \\\n",
        "    .groupBy(\"community_id\").agg(countDistinct(\"Client1\").alias(\"community_size\"))\n",
        "\n",
        "# Merge the community sizes into the main DataFrame\n",
        "df_final = df_with_communities.join(community_sizes, 'community_id')\n",
        "\n",
        "# Create community_members with unique tuples for each community\n",
        "community_members = df_final.select(\"community_id\", \"Client1\", \"Client2\", \"total_duration_minutes\") \\\n",
        "    .distinct() \\\n",
        "    .groupBy(\"community_id\") \\\n",
        "    .agg(F.collect_list(F.struct(\n",
        "        F.col(\"Client1\"),\n",
        "        F.col(\"Client2\"),\n",
        "        F.col(\"total_duration_minutes\")\n",
        "    )).alias(\"members\")) \\\n",
        "    .orderBy(\"community_id\")\n",
        "\n",
        "# Show the final DataFrame with community IDs, duration, and community sizes\n",
        "print(\"\\nFinal DataFrame with Sequential Community IDs:\")\n",
        "df_final.select(\n",
        "    'Client1',\n",
        "    'Client2',\n",
        "    'Start_Time',\n",
        "    'End_Time',\n",
        "    'total_duration_minutes',\n",
        "    'community_id',\n",
        "    'community_size'\n",
        ").orderBy(\"community_id\").show()"
      ],
      "metadata": {
        "id": "eal40YRg4TvV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a745bedc-9af0-4ca8-e538-4c02edadc17a"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
            "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final DataFrame with Sequential Community IDs:\n",
            "+-------+-------+----------+----------+----------------------+------------+--------------+\n",
            "|Client1|Client2|Start_Time|  End_Time|total_duration_minutes|community_id|community_size|\n",
            "+-------+-------+----------+----------+----------------------+------------+--------------+\n",
            "|      1|      2|2408060000|2408060200|                 420.0|           1|             3|\n",
            "|      2|      3|2408040000|2408040500|                 300.0|           1|             3|\n",
            "|      1|      2|2408040000|2408040500|                 420.0|           1|             3|\n",
            "|      4|      5|2408020000|2408020600|                 360.0|           2|             4|\n",
            "|      5|      6|2408090000|2408091500|                 900.0|           2|             4|\n",
            "|      6|      7|2408070000|2408070800|                 480.0|           2|             4|\n",
            "|      8|      9|2408020000|2408020600|                 360.0|           3|             4|\n",
            "|      9|     10|2408090000|2408091500|                 900.0|           3|             4|\n",
            "|     10|     11|2408070000|2408070800|                 480.0|           3|             4|\n",
            "|     12|     13|2408010000|2408010200|                 120.0|           4|             3|\n",
            "|     13|     14|2408030000|2408030500|                 300.0|           4|             3|\n",
            "|     12|     14|2408020000|2408020800|                 480.0|           4|             3|\n",
            "+-------+-------+----------+----------+----------------------+------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define community_members to get a list of unique (Client1, Client2, total_duration_minutes) tuples for each community\n",
        "# community_members = df_final.select(\"community_id\", \"Client1\", \"Client2\", \"total_duration_minutes\") \\\n",
        "#     .distinct() \\\n",
        "#     .orderBy(\"community_id\", \"total_duration_minutes\") \\\n",
        "#     .groupBy(\"community_id\") \\\n",
        "#     .agg(F.collect_list(F.struct(\n",
        "#         F.col(\"Client1\"),\n",
        "#         F.col(\"Client2\"),\n",
        "#         F.col(\"total_duration_minutes\")\n",
        "#     )).alias(\"members\")) \\\n",
        "#     .orderBy(\"community_id\")\n",
        "# Define community_members to get a list of unique (Client1, Client2, total_duration_minutes) tuples for each community\n",
        "community_members = df_final.select(\"community_id\", \"Client1\", \"Client2\", \"total_duration_minutes\") \\\n",
        "    .distinct() \\\n",
        "    .orderBy(\"Client1\") \\\n",
        "    .groupBy(\"community_id\") \\\n",
        "    .agg(F.collect_list(F.struct(\n",
        "        F.col(\"Client1\"),\n",
        "        F.col(\"Client2\"),\n",
        "        F.col(\"total_duration_minutes\")\n",
        "    )).alias(\"members\")) \\\n",
        "    .orderBy(\"community_id\")\n",
        "# Show the list of community members as tuples\n",
        "print(\"\\nCommunity Members with Sequential IDs:\")\n",
        "community_members.show(truncate=False)"
      ],
      "metadata": {
        "id": "KX3yr4DDe494",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26b28551-43b0-44e0-9e5e-2c3534019c0f"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Community Members with Sequential IDs:\n",
            "+------------+---------------------------------------------------+\n",
            "|community_id|members                                            |\n",
            "+------------+---------------------------------------------------+\n",
            "|1           |[{1, 2, 420.0}, {2, 3, 300.0}]                     |\n",
            "|2           |[{4, 5, 360.0}, {5, 6, 900.0}, {6, 7, 480.0}]      |\n",
            "|3           |[{8, 9, 360.0}, {9, 10, 900.0}, {10, 11, 480.0}]   |\n",
            "|4           |[{12, 13, 120.0}, {12, 14, 480.0}, {13, 14, 300.0}]|\n",
            "+------------+---------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarity computation"
      ],
      "metadata": {
        "id": "Hne9HBYnfLoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import pandas_udf, PandasUDFType, col, explode, struct\n",
        "from pyspark.sql.types import BinaryType, StructType, StructField, IntegerType\n",
        "from scipy.sparse import csr_matrix\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import numpy as np\n",
        "'''Decorator and Function Definition:\n",
        "The @pandas_udf decorator marks this function as a Pandas UDF (User Defined Function) that will be applied on grouped data.\n",
        "GROUPED_MAP tells Spark that the function will receive a DataFrame for each group (grouped by community_id).\n",
        "The schema defines the expected output structure of the function, which is a DataFrame with community_id\n",
        "and a binary field containing the serialized matrix.\n",
        "The function converts the connections (edges) between clients into a CSR matrix and serializes it for storage.'''\n",
        "\n",
        "# Define the schema for the Pandas UDF output\n",
        "schema = StructType([\n",
        "    StructField(\"community_id\", IntegerType(), True),\n",
        "    StructField(\"csr_matrix\", BinaryType(), True)\n",
        "])\n",
        "\n",
        "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
        "def create_csr_matrix_from_edges(members_df):\n",
        "    \"\"\"\n",
        "    Creates a serialized CSR matrix from a Spark DataFrame for each community.\n",
        "\n",
        "    Args:\n",
        "        members_df: Spark DataFrame with 'community_id' and 'members' columns.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with 'community_id' and a serialized CSR matrix as binary data.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract the community ID (assuming it's consistent within the group)\n",
        "    community_id = members_df['community_id'].iloc[0]\n",
        "    '''Since each members_df contains data for a single community (due to groupBy operation),\n",
        "    the function retrieves the community_id from the first row.\n",
        "    This ID will be included in the output so that each serialized CSR matrix can be linked back\n",
        "    to its respective community.'''\n",
        "    # Explode the members array to get each connection in separate rows\n",
        "    exploded_df = members_df.explode(\"members\").dropna().reset_index(drop=True)\n",
        "    exploded_df = pd.DataFrame({\n",
        "        'Client1': exploded_df['members'].apply(lambda x: x['Client1']),\n",
        "        'Client2': exploded_df['members'].apply(lambda x: x['Client2']),\n",
        "        'total_duration_minutes': exploded_df['members'].apply(lambda x: x['total_duration_minutes'])\n",
        "    })\n",
        "    '''Flattening and Extracting Connection Data:\n",
        "    The members_df contains a column with a list of connections (pairs of clients and call durations).\n",
        "    The function uses explode to convert this list into individual rows, making it easier to work with each connection.\n",
        "    It then creates a new DataFrame, exploded_df, with separate columns for Client1, Client2, and duration_minutes\n",
        "    extracted from the connection data.\n",
        "    This simplifies further processing by ensuring each row represents a single call between two clients.'''\n",
        "    # Get unique clients and create a mapping to indices\n",
        "    unique_clients = sorted(pd.concat([exploded_df['Client1'], exploded_df['Client2']]).unique())\n",
        "    client_to_index = {client: i for i, client in enumerate(unique_clients)}\n",
        "    num_clients = len(unique_clients)\n",
        "\n",
        "    # Extract data for CSR matrix\n",
        "    rows = exploded_df['Client1'].map(client_to_index).values\n",
        "    cols = exploded_df['Client2'].map(client_to_index).values\n",
        "    if weight:\n",
        "      data = exploded_df['total_duration_minutes'].values #if weight else [1] * len(rows)\n",
        "    else:\n",
        "      data = [1] * len(rows)\n",
        "\n",
        "    # Create CSR matrix\n",
        "    csr = csr_matrix((data, (rows, cols)), shape=(num_clients, num_clients))\n",
        "    '''Serializing the CSR Matrix: The function uses Python’s pickle module to serialize the CSR matrix.\n",
        "    This converts the matrix into a binary format, allowing it to be stored or transferred efficiently.\n",
        "    Serialization is necessary because Spark DataFrames cannot directly store complex Python objects like CSR matrices.'''\n",
        "    # Serialize CSR matrix to binary format\n",
        "    serialized_csr = pickle.dumps(csr)\n",
        "\n",
        "    # Return as DataFrame\n",
        "    return pd.DataFrame({\"community_id\": [community_id], \"csr_matrix\": [serialized_csr]})\n",
        "weight=True\n",
        "# Use the function to generate a serialized CSR matrix for each community and show the results\n",
        "result_true = community_members.groupBy(\"community_id\").apply(create_csr_matrix_from_edges)\n",
        "weight=False\n",
        "result_false = community_members.groupBy(\"community_id\").apply(create_csr_matrix_from_edges)"
      ],
      "metadata": {
        "id": "yr2FkFKVfrix"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pretty_print_csr_matrix(csr_matrix_result, weight=True):\n",
        "    \"\"\"Prints a CSR matrix in a readable format.\"\"\"\n",
        "    rows, cols = csr_matrix_result.nonzero()\n",
        "    data = csr_matrix_result.data\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        'Row': rows,\n",
        "        'Col': cols,\n",
        "        'Value': data\n",
        "    })\n",
        "\n",
        "    print(df)\n",
        "\n",
        "# Deserialize and print CSR matrices for each community\n",
        "for row in result_true.collect():\n",
        "    community_id = row['community_id']\n",
        "    serialized_csr = row['csr_matrix']\n",
        "\n",
        "    # Deserialize the binary data back to a CSR matrix\n",
        "    csr_matrix_result = pickle.loads(serialized_csr)\n",
        "\n",
        "    print(f\"Community ID: {community_id}\")\n",
        "    pretty_print_csr_matrix(csr_matrix_result)\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "id": "n7s0QAXNnXtb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18466afa-b76f-4f9f-b5c2-2272adb42696"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Community ID: 1\n",
            "   Row  Col  Value\n",
            "0    0    1  420.0\n",
            "1    1    2  300.0\n",
            "----------------------------------------\n",
            "Community ID: 2\n",
            "   Row  Col  Value\n",
            "0    0    1  360.0\n",
            "1    1    2  900.0\n",
            "2    2    3  480.0\n",
            "----------------------------------------\n",
            "Community ID: 3\n",
            "   Row  Col  Value\n",
            "0    0    1  360.0\n",
            "1    1    2  900.0\n",
            "2    2    3  480.0\n",
            "----------------------------------------\n",
            "Community ID: 4\n",
            "   Row  Col  Value\n",
            "0    0    1  120.0\n",
            "1    0    2  480.0\n",
            "2    1    2  300.0\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Padding and calculating DeltaCon similarity\n",
        "def pad_csr_matrix(csr, max_shape):\n",
        "    current_rows, current_cols = csr.shape\n",
        "    max_rows, max_cols = max_shape\n",
        "    if current_rows < max_rows:\n",
        "        additional_rows = csr_matrix((max_rows - current_rows, current_cols))\n",
        "        csr = vstack([csr, additional_rows])\n",
        "    if current_cols < max_cols:\n",
        "        additional_cols = csr_matrix((csr.shape[0], max_cols - current_cols))\n",
        "        csr = hstack([csr, additional_cols])\n",
        "    return csr"
      ],
      "metadata": {
        "id": "MDq7rT3PneYg"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_csr_matrices(df, max_size):\n",
        "    def pad_and_calculate(row):\n",
        "        csr_matrix_padded = pad_csr_matrix(pickle.loads(row['csr_matrix']), max_size)\n",
        "        serialized_csr = pickle.dumps(csr_matrix_padded)\n",
        "        return (row['community_id'], serialized_csr)\n",
        "\n",
        "    return df.rdd.map(pad_and_calculate).toDF([\"community_id\", \"csr_matrix\"])\n",
        "padded_result_true = process_csr_matrices(result_true, max_size)\n",
        "padded_result_false = process_csr_matrices(result_false, max_size)\n"
      ],
      "metadata": {
        "id": "JWP4c8zUxNFG"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\n",
        "import pickle\n",
        "from scipy.sparse import csr_matrix\n",
        "import numpy as np\n",
        "\n",
        "def normalize_matrix(matrix):\n",
        "    \"\"\"\n",
        "    Normalize the matrix values to the range [0, 1].\n",
        "\n",
        "    Parameters:\n",
        "    matrix : csr_matrix\n",
        "        Sparse matrix to normalize.\n",
        "\n",
        "    Returns:\n",
        "    csr_matrix\n",
        "        Normalized sparse matrix.\n",
        "    \"\"\"\n",
        "    data = matrix.data\n",
        "    if len(data) == 0:  # Handle empty matrices\n",
        "        return matrix\n",
        "    min_val = np.min(data)\n",
        "    max_val = np.max(data)\n",
        "    normalized_data = (data - min_val) / (max_val - min_val) if max_val > min_val else data\n",
        "    return matrix.__class__((normalized_data, matrix.indices, matrix.indptr), shape=matrix.shape)\n",
        "\n",
        "def frobenius_norm(csr_1, csr_2):\n",
        "    \"\"\"\n",
        "    Compute Frobenius norm between two sparse matrices.\n",
        "\n",
        "    Parameters:\n",
        "    csr_1, csr_2 : csr_matrix\n",
        "        Sparse adjacency matrices of the graphs.\n",
        "\n",
        "    Returns:\n",
        "    float\n",
        "        Frobenius norm distance between the graphs.\n",
        "    \"\"\"\n",
        "    # csr_1 = log_transform_matrix(csr_1)\n",
        "    # csr_2 = log_transform_matrix(csr_2)\n",
        "    csr_1 = normalize_matrix(csr_1)\n",
        "    csr_2 = normalize_matrix(csr_2)\n",
        "    assert csr_1.shape == csr_2.shape, \"Adjacency matrices must have the same dimensions.\"\n",
        "    diff = csr_1 - csr_2\n",
        "    return np.sqrt((diff.power(2)).sum())\n",
        "\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "def frobenius_sim(csr_1, csr_2):\n",
        "    \"\"\"\n",
        "    Adds a similarity column to the DataFrame based on Frobenius distance.\n",
        "\n",
        "    Parameters:\n",
        "    df (DataFrame): Input DataFrame containing 'frobenius_distance' column.\n",
        "\n",
        "    Returns:\n",
        "    DataFrame: A DataFrame with an additional 'similarity' column.\n",
        "    \"\"\"\n",
        "    dist=frobenius_norm(csr_1, csr_2)\n",
        "    return 1 / (1 + dist)"
      ],
      "metadata": {
        "id": "O17aieXsz4tu"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse.linalg import inv\n",
        "from scipy.sparse import identity\n",
        "def deltacon_similarity(csr_1, csr_2, epsilon=0.5):\n",
        "    # Ensure both matrices are of the same size\n",
        "    assert csr_1.shape == csr_2.shape, \"Adjacency matrices must be of the same size for comparison.\"\n",
        "    I = identity(csr_1.shape[0])\n",
        "    D1 = csr_1.sum(axis=1).A.flatten()\n",
        "    D1 = csr_matrix((D1, (range(csr_1.shape[0]), range(csr_1.shape[0]))))\n",
        "    D2 = csr_2.sum(axis=1).A.flatten()\n",
        "    D2 = csr_matrix((D2, (range(csr_2.shape[0]), range(csr_2.shape[0]))))\n",
        "\n",
        "    S1 = inv(I + epsilon**2 * D1 - epsilon * csr_1)\n",
        "    S2 = inv(I + epsilon**2 * D2 - epsilon * csr_2)\n",
        "    frobenius_norm = np.sqrt(((S1 - S2).power(2)).sum())\n",
        "    return 1 / (1 + frobenius_norm)\n",
        "max_size = result.rdd.map(lambda row: pickle.loads(row['csr_matrix']).shape).reduce(lambda x, y: (max(x[0], y[0]), max(x[1], y[1])))"
      ],
      "metadata": {
        "id": "62KP1mHFmWZD"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# Comparison function for structural and weight-based similarities\n",
        "def cosine_sim(csr_1, csr_2):\n",
        "    # Compute cosine similarity\n",
        "    cosine_sim = cosine_similarity(csr_1, csr_2)\n",
        "    return cosine_sim"
      ],
      "metadata": {
        "id": "r12hzP-Y9-VS"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename columns from df2 to remove ambiguity\n",
        "padded_result_true_renamed = padded_result_true.select(\n",
        "    col(\"community_id\").alias(\"community_id_2\"),\n",
        "    col(\"csr_matrix\").alias(\"csr_matrix_2\")\n",
        ")\n",
        "\n",
        "# Perform a cross join to compare every community to every other community\n",
        "cross_joined_df = padded_result_true.alias(\"df1\").crossJoin(padded_result_true_renamed.alias(\"df2\")) \\\n",
        "    .filter(col(\"df1.community_id\") < col(\"df2.community_id_2\"))\n",
        "\n",
        "# Define the schema for the Pandas UDF output\n",
        "similarity_schema = StructType([\n",
        "    StructField(\"community_id_1\", IntegerType(), True),\n",
        "    StructField(\"community_id_2\", IntegerType(), True),\n",
        "    StructField(\"frobenius_similarity\", DoubleType(), True)\n",
        "])\n",
        "\n",
        "# Define the function to calculate Frobenius similarity\n",
        "def calculate_similarity(grouped_df):\n",
        "    \"\"\"\n",
        "    Computes the Frobenius similarity between the csr_matrices of two communities.\n",
        "    \"\"\"\n",
        "    community_id_1 = grouped_df.iloc[0]['community_id']\n",
        "    community_id_2 = grouped_df.iloc[0]['community_id_2']\n",
        "\n",
        "    csr_1 = pickle.loads(grouped_df.iloc[0]['csr_matrix'])\n",
        "    csr_2 = pickle.loads(grouped_df.iloc[0]['csr_matrix_2'])\n",
        "\n",
        "    similarity_score_f = frobenius_sim(csr_1, csr_2)\n",
        "\n",
        "    # Return a DataFrame with the results\n",
        "    return pd.DataFrame([{\n",
        "        \"community_id_1\": community_id_1,\n",
        "        \"community_id_2\": community_id_2,\n",
        "        \"frobenius_similarity\": similarity_score_f\n",
        "    }])\n",
        "\n",
        "# Apply the similarity calculation using applyInPandas\n",
        "similarity_df = cross_joined_df.select(\"df1.community_id\", \"df2.community_id_2\", \"df1.csr_matrix\", \"df2.csr_matrix_2\") \\\n",
        "    .groupBy(\"community_id\", \"community_id_2\") \\\n",
        "    .applyInPandas(calculate_similarity, schema=similarity_schema)\n",
        "\n",
        "# Show the results\n",
        "similarity_df.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "xZ9X1yhgmyak",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "251a7b10-9ec5-4f03-d7d8-facf277f54a6"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+--------------+--------------------+\n",
            "|community_id_1|community_id_2|frobenius_similarity|\n",
            "+--------------+--------------+--------------------+\n",
            "|1             |2             |0.4112575122414839  |\n",
            "|1             |3             |0.4112575122414839  |\n",
            "|1             |4             |0.4                 |\n",
            "|2             |3             |1.0                 |\n",
            "|2             |4             |0.4673105310958499  |\n",
            "|3             |4             |0.4673105310958499  |\n",
            "+--------------+--------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rename columns from df2 to remove ambiguity\n",
        "padded_result_false_renamed = padded_result_false.select(\n",
        "    col(\"community_id\").alias(\"community_id_2\"),\n",
        "    col(\"csr_matrix\").alias(\"csr_matrix_2\")\n",
        ")\n",
        "\n",
        "# Perform a cross join to compare every community to every other community\n",
        "cross_joined_df = padded_result_false.alias(\"df1\").crossJoin(padded_result_false_renamed.alias(\"df2\")) \\\n",
        "    .filter(col(\"df1.community_id\") < col(\"df2.community_id_2\"))\n",
        "\n",
        "# Define the schema for the Pandas UDF output\n",
        "similarity_schema = StructType([\n",
        "    StructField(\"community_id_1\", IntegerType(), True),\n",
        "    StructField(\"community_id_2\", IntegerType(), True),\n",
        "    StructField(\"deltacon\", DoubleType(), True)\n",
        "])\n",
        "\n",
        "# Define the function to calculate Frobenius similarity\n",
        "def calculate_similarity(grouped_df):\n",
        "    \"\"\"\n",
        "    Computes the Frobenius similarity between the csr_matrices of two communities.\n",
        "    \"\"\"\n",
        "    community_id_1 = grouped_df.iloc[0]['community_id']\n",
        "    community_id_2 = grouped_df.iloc[0]['community_id_2']\n",
        "\n",
        "    csr_1 = pickle.loads(grouped_df.iloc[0]['csr_matrix'])\n",
        "    csr_2 = pickle.loads(grouped_df.iloc[0]['csr_matrix_2'])\n",
        "\n",
        "    similarity_score_d = deltacon_similarity(csr_1, csr_2)\n",
        "\n",
        "    # Return a DataFrame with the results\n",
        "    return pd.DataFrame([{\n",
        "        \"community_id_1\": community_id_1,\n",
        "        \"community_id_2\": community_id_2,\n",
        "        \"deltacon\": similarity_score_d\n",
        "    }])\n",
        "\n",
        "# Apply the similarity calculation using applyInPandas\n",
        "similarity_df = cross_joined_df.select(\"df1.community_id\", \"df2.community_id_2\", \"df1.csr_matrix\", \"df2.csr_matrix_2\") \\\n",
        "    .groupBy(\"community_id\", \"community_id_2\") \\\n",
        "    .applyInPandas(calculate_similarity, schema=similarity_schema)\n",
        "\n",
        "# Show the results\n",
        "similarity_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmz-UOJCf3-N",
        "outputId": "f3303daf-80d7-45e2-cde8-521420637393"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+--------------+------------------+\n",
            "|community_id_1|community_id_2|deltacon          |\n",
            "+--------------+--------------+------------------+\n",
            "|1             |2             |0.6725177100028019|\n",
            "|1             |3             |0.6725177100028019|\n",
            "|1             |4             |0.7470353885783044|\n",
            "|2             |3             |1.0               |\n",
            "|2             |4             |0.6213353259709683|\n",
            "|3             |4             |0.6213353259709683|\n",
            "+--------------+--------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, expr\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "# Step 1: Compute Frobenius Similarity (using padded_result_true)\n",
        "\n",
        "# Rename columns from df2 to remove ambiguity for Frobenius similarity calculation\n",
        "padded_result_true_renamed = padded_result_true.select(\n",
        "    col(\"community_id\").alias(\"community_id_2\"),\n",
        "    col(\"csr_matrix\").alias(\"csr_matrix_2\")\n",
        ")\n",
        "\n",
        "# Cross join to compare every community for Frobenius similarity\n",
        "cross_joined_df_frobenius = padded_result_true.alias(\"df1\").crossJoin(padded_result_true_renamed.alias(\"df2\")) \\\n",
        "    .filter(col(\"df1.community_id\") < col(\"df2.community_id_2\"))\n",
        "\n",
        "# Define schema for Frobenius similarity output\n",
        "frobenius_similarity_schema = StructType([\n",
        "    StructField(\"community_id_1\", IntegerType(), True),\n",
        "    StructField(\"community_id_2\", IntegerType(), True),\n",
        "    StructField(\"frobenius_similarity\", DoubleType(), True)\n",
        "])\n",
        "\n",
        "# Define function to calculate Frobenius similarity\n",
        "def calculate_frobenius_similarity(grouped_df):\n",
        "    community_id_1 = grouped_df.iloc[0]['community_id']\n",
        "    community_id_2 = grouped_df.iloc[0]['community_id_2']\n",
        "    csr_1 = pickle.loads(grouped_df.iloc[0]['csr_matrix'])\n",
        "    csr_2 = pickle.loads(grouped_df.iloc[0]['csr_matrix_2'])\n",
        "    similarity_score_f = frobenius_sim(csr_1, csr_2)\n",
        "    return pd.DataFrame([{\n",
        "        \"community_id_1\": community_id_1,\n",
        "        \"community_id_2\": community_id_2,\n",
        "        \"frobenius_similarity\": similarity_score_f\n",
        "    }])\n",
        "\n",
        "# Apply Frobenius similarity calculation\n",
        "frobenius_similarity_df = cross_joined_df_frobenius.select(\n",
        "    \"df1.community_id\", \"df2.community_id_2\", \"df1.csr_matrix\", \"df2.csr_matrix_2\"\n",
        ").groupBy(\"community_id\", \"community_id_2\") \\\n",
        "    .applyInPandas(calculate_frobenius_similarity, schema=frobenius_similarity_schema)\n",
        "\n",
        "\n",
        "# Step 2: Compute DeltaCon Similarity (using padded_result_false)\n",
        "\n",
        "# Rename columns from df2 to remove ambiguity for DeltaCon similarity calculation\n",
        "padded_result_false_renamed = padded_result_false.select(\n",
        "    col(\"community_id\").alias(\"community_id_2\"),\n",
        "    col(\"csr_matrix\").alias(\"csr_matrix_2\")\n",
        ")\n",
        "\n",
        "# Cross join to compare every community for DeltaCon similarity\n",
        "cross_joined_df_deltacon = padded_result_false.alias(\"df1\").crossJoin(padded_result_false_renamed.alias(\"df2\")) \\\n",
        "    .filter(col(\"df1.community_id\") < col(\"df2.community_id_2\"))\n",
        "\n",
        "# Define schema for DeltaCon similarity output\n",
        "deltacon_similarity_schema = StructType([\n",
        "    StructField(\"community_id_1\", IntegerType(), True),\n",
        "    StructField(\"community_id_2\", IntegerType(), True),\n",
        "    StructField(\"deltacon\", DoubleType(), True)\n",
        "])\n",
        "\n",
        "# Define function to calculate DeltaCon similarity\n",
        "def calculate_deltacon_similarity(grouped_df):\n",
        "    community_id_1 = grouped_df.iloc[0]['community_id']\n",
        "    community_id_2 = grouped_df.iloc[0]['community_id_2']\n",
        "    csr_1 = pickle.loads(grouped_df.iloc[0]['csr_matrix'])\n",
        "    csr_2 = pickle.loads(grouped_df.iloc[0]['csr_matrix_2'])\n",
        "    similarity_score_d = deltacon_similarity(csr_1, csr_2)\n",
        "    return pd.DataFrame([{\n",
        "        \"community_id_1\": community_id_1,\n",
        "        \"community_id_2\": community_id_2,\n",
        "        \"deltacon\": similarity_score_d\n",
        "    }])\n",
        "\n",
        "# Apply DeltaCon similarity calculation\n",
        "deltacon_similarity_df = cross_joined_df_deltacon.select(\n",
        "    \"df1.community_id\", \"df2.community_id_2\", \"df1.csr_matrix\", \"df2.csr_matrix_2\"\n",
        ").groupBy(\"community_id\", \"community_id_2\") \\\n",
        "    .applyInPandas(calculate_deltacon_similarity, schema=deltacon_similarity_schema)\n",
        "\n",
        "\n",
        "# Step 3: Join Results and Calculate Final Similarity Score\n",
        "\n",
        "# Join the Frobenius and DeltaCon similarity DataFrames\n",
        "combined_similarity_df = frobenius_similarity_df.join(\n",
        "    deltacon_similarity_df,\n",
        "    on=[\"community_id_1\", \"community_id_2\"],\n",
        "    how=\"inner\"\n",
        ")\n",
        "\n",
        "# Calculate the final similarity score as an average of Frobenius and DeltaCon similarities\n",
        "final_similarity_df = combined_similarity_df.withColumn(\n",
        "    \"final_similarity\",\n",
        "    expr(\"0.5 * frobenius_similarity + 0.5 * deltacon\")\n",
        ")\n",
        "\n",
        "# Show the final results\n",
        "final_similarity_df.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfazJyMpepEv",
        "outputId": "97072e3d-9038-420f-825d-efab5331db57"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+--------------+--------------------+------------------+------------------+\n",
            "|community_id_1|community_id_2|frobenius_similarity|deltacon          |final_similarity  |\n",
            "+--------------+--------------+--------------------+------------------+------------------+\n",
            "|1             |2             |0.4112575122414839  |0.6725177100028019|0.5418876111221429|\n",
            "|1             |3             |0.4112575122414839  |0.6725177100028019|0.5418876111221429|\n",
            "|1             |4             |0.4                 |0.7470353885783044|0.5735176942891522|\n",
            "|2             |3             |1.0                 |1.0               |1.0               |\n",
            "|2             |4             |0.4673105310958499  |0.6213353259709683|0.5443229285334091|\n",
            "|3             |4             |0.4673105310958499  |0.6213353259709683|0.5443229285334091|\n",
            "+--------------+--------------+--------------------+------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the overall similarity threshold\n",
        "similarity_threshold = 0.55\n",
        "\n",
        "# Filter pairs with an overall similarity above the threshold\n",
        "similar_pairs = final_similarity_df.filter(F.col(\"final_similarity\") >= similarity_threshold)\n",
        "\n",
        "# Create vertices (unique community IDs) and edges (pairs with similarity above threshold)\n",
        "vertices = similar_pairs.select(\"community_id_1\").union(similar_pairs.select(\"community_id_2\")).distinct() \\\n",
        "    .withColumnRenamed(\"community_id_1\", \"id\")\n",
        "\n",
        "edges = similar_pairs.select(\n",
        "    F.col(\"community_id_1\").alias(\"src\"),\n",
        "    F.col(\"community_id_2\").alias(\"dst\")\n",
        ")\n",
        "\n",
        "# Build the GraphFrame for community grouping\n",
        "g = GraphFrame(vertices, edges)\n",
        "\n",
        "# Find connected components (clusters of communities)\n",
        "connected_components = g.connectedComponents()\n",
        "\n",
        "# Group communities by connected component (cluster)\n",
        "grouped_communities = connected_components.groupBy(\"component\").agg(F.collect_list(\"id\").alias(\"community_group\"))\n",
        "\n",
        "# Show the clustered communities based on the similarity threshold\n",
        "print(\"\\nGrouped Communities Based on Similarity Threshold:\")\n",
        "grouped_communities.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUqpSLXSiSfT",
        "outputId": "cfe34822-9372-4462-9247-be23cedbf714"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
            "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Grouped Communities Based on Similarity Threshold:\n",
            "+---------+---------------+\n",
            "|component|community_group|\n",
            "+---------+---------------+\n",
            "|2        |[2, 3]         |\n",
            "|1        |[1, 4]         |\n",
            "+---------+---------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}