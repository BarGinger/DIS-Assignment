{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BarGinger/DIS-Assignment/blob/main/Src/dis_notebook_25_10_24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFDENKMNs-52",
        "outputId": "2a797797-f97b-49d9-e5dc-6202ed3f991c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra fonts-nanum fonts-ipafont-gothic\n",
            "  fonts-ipafont-mincho fonts-wqy-microhei fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 39.6 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 123622 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u422-b05-1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u422-b05-1~22.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u422-b05-1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u422-b05-1~22.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u422-b05-1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u422-b05-1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "!pip install graphframes\n",
        "!pip install\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjrP64v5QyEd"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import (\n",
        "    col,\n",
        "    udf,\n",
        "    row_number,\n",
        "    countDistinct,\n",
        "    collect_list,\n",
        "    struct,\n",
        "    count,\n",
        "    sum,\n",
        "    avg,\n",
        "    expr,\n",
        "    percentile_approx,\n",
        "    max as spark_max,\n",
        "    explode\n",
        ")\n",
        "from pyspark.sql.types import StringType, IntegerType, BinaryType, DoubleType, ArrayType, StructType, StructField\n",
        "from pyspark.sql import Window\n",
        "from datetime import datetime\n",
        "from graphframes import GraphFrame\n",
        "from scipy.sparse import csr_matrix, vstack, hstack\n",
        "import numpy as np\n",
        "import pickle\n",
        "import base64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAMTvsL2v2YH",
        "outputId": "9b379887-6060-4c3c-c828-b8cca9118b59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------+----------+----------+----------------+---------------+\n",
            "|Client1|Client2|Start_Time|  End_Time|duration_minutes|duration_DDHHMM|\n",
            "+-------+-------+----------+----------+----------------+---------------+\n",
            "|      1|      2|2408060000|2408060200|           120.0|         000200|\n",
            "|      2|      3|2408040000|2408040500|           300.0|         000500|\n",
            "|      4|      5|2408020000|2408020600|           360.0|         000600|\n",
            "|      5|      6|2408090000|2408091500|           900.0|         001500|\n",
            "|      6|      7|2408070000|2408070800|           480.0|         000800|\n",
            "|      8|      9|2408090000|2408090300|           180.0|         000300|\n",
            "|      9|     10|2408070000|2408070500|           300.0|         000500|\n",
            "|     10|     11|2408010000|2408010400|           240.0|         000400|\n",
            "|     12|     13|2408010000|2408010200|           120.0|         000200|\n",
            "|     13|     14|2408030000|2408030500|           300.0|         000500|\n",
            "|     12|     14|2408020000|2408020800|           480.0|         000800|\n",
            "+-------+-------+----------+----------+----------------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Re-initialize Spark session if needed\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"PhoneCallsCommunityDetection\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.jars.packages\", \"graphframes:graphframes:0.8.2-spark3.1-s_2.12\") \\\n",
        "    .config(\"spark.executor.memory\", \"20G\") \\\n",
        "    .config(\"spark.driver.memory\", \"50G\") \\\n",
        "    .config(\"spark.executor.memoryOverhead\", \"1G\") \\\n",
        "    .config(\"spark.default.parallelism\", \"100\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
        "    .config(\"spark.driver.maxResultSize\", \"2G\") \\\n",
        "    .getOrCreate()\n",
        "# Optional: Set logging level to reduce verbosity\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "# Set a checkpoint directory for Spark\n",
        "spark.sparkContext.setCheckpointDir(\"/tmp/spark-checkpoints\")\n",
        "\n",
        "file_path = 'toy_dataset.csv' #'adjusted_phone_calls.csv'\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "# Convert YYMMDDHHMM to a proper datetime object\n",
        "def convert_to_datetime(yyMMddHHMM):\n",
        "    return datetime.strptime(str(yyMMddHHMM), '%y%m%d%H%M')\n",
        "\n",
        "# Define UDF for calculating duration in minutes\n",
        "def calculate_duration_minutes(start_time, end_time):\n",
        "    start_dt = convert_to_datetime(start_time)\n",
        "    end_dt = convert_to_datetime(end_time)\n",
        "    duration = end_dt - start_dt\n",
        "    return duration.total_seconds() / 60\n",
        "\n",
        "# Define UDF for calculating duration in DDHHMM format\n",
        "def calculate_duration_string(start_time, end_time):\n",
        "    start_dt = convert_to_datetime(start_time)\n",
        "    end_dt = convert_to_datetime(end_time)\n",
        "    duration = end_dt - start_dt\n",
        "\n",
        "    days = duration.days\n",
        "    hours, remainder = divmod(duration.seconds, 3600)\n",
        "    minutes = remainder // 60\n",
        "    return f'{days:02d}{hours:02d}{minutes:02d}'\n",
        "\n",
        "# Register the UDFs in Spark\n",
        "calculate_duration_minutes_udf = udf(calculate_duration_minutes, DoubleType())\n",
        "calculate_duration_string_udf = udf(calculate_duration_string, StringType())\n",
        "\n",
        "# Add columns for duration in minutes and DDHHMM format\n",
        "df = df.withColumn('duration_minutes', calculate_duration_minutes_udf(col('Start_Time'), col('End_Time')))\n",
        "df = df.withColumn('duration_DDHHMM', calculate_duration_string_udf(col('Start_Time'), col('End_Time')))\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "k10Ux9bh-zMO"
      },
      "outputs": [],
      "source": [
        "# Create Graph using GraphFrames for community detection\n",
        "vertices = df.selectExpr(\"Client1 as id\").union(df.selectExpr(\"Client2 as id\")).distinct()\n",
        "edges = df.selectExpr(\"Client1 as src\", \"Client2 as dst\", \"duration_minutes as weight\")\n",
        "\n",
        "# Cache vertices and edges\n",
        "vertices.cache()\n",
        "edges.cache()\n",
        "\n",
        "# Create a GraphFrame\n",
        "g = GraphFrame(vertices, edges)\n",
        "\n",
        "# Find connected components (communities) using GraphFrames\n",
        "result = g.connectedComponents()\n",
        "\n",
        "# Create a mapping from original community IDs to sequential ones\n",
        "community_mapping = result.select(\"component\").distinct() \\\n",
        "    .orderBy(\"component\") \\\n",
        "    .withColumn(\"new_id\", row_number().over(Window.orderBy(\"component\"))) \\\n",
        "    .cache()\n",
        "\n",
        "# Join the result (community IDs) with the original dataframe and map to new sequential IDs\n",
        "df_with_communities = df.join(result, df['Client1'] == result['id'], 'inner') \\\n",
        "    .join(community_mapping, result['component'] == community_mapping['component'], 'inner') \\\n",
        "    .drop(result['id']) \\\n",
        "    .drop(community_mapping['component']) \\\n",
        "    .withColumnRenamed('new_id', 'community_id')\n",
        "\n",
        "# Calculate the number of unique clients (community size) per community\n",
        "community_sizes = df_with_communities.select(\"community_id\", \"Client1\").union(df_with_communities.select(\"community_id\", \"Client2\")) \\\n",
        "    .distinct() \\\n",
        "    .groupBy(\"community_id\").agg(countDistinct(\"Client1\").alias(\"community_size\"))\n",
        "\n",
        "# Merge the community sizes into the main DataFrame\n",
        "df_final = df_with_communities.join(community_sizes, 'community_id')\n",
        "\n",
        "# Get list of tuples for each community member by considering both Client1 and Client2\n",
        "community_members = df_final.select(\"community_id\", \"Client1\", \"Client2\", \"duration_DDHHMM\", \"duration_minutes\") \\\n",
        "    .distinct() \\\n",
        "    .groupBy(\"community_id\") \\\n",
        "    .agg(collect_list(struct(col(\"Client1\"),\n",
        "                           col(\"Client2\"),\n",
        "                           col(\"duration_DDHHMM\"),\n",
        "                           col(\"duration_minutes\"))).alias(\"members\")) \\\n",
        "    .orderBy(\"community_id\")\n",
        "\n",
        "# Show the final DataFrame with community IDs, duration, and community sizes\n",
        "print(\"\\nFinal DataFrame with Sequential Community IDs:\")\n",
        "df_final.select('Client1',\n",
        "                'Client2',\n",
        "                'duration_DDHHMM',\n",
        "                'duration_minutes',\n",
        "                'community_id',\n",
        "                'community_size') \\\n",
        "    .orderBy(\"community_id\") \\\n",
        "    .show()\n",
        "\n",
        "# Show the list of community members as tuples\n",
        "print(\"\\nCommunity Members with Sequential IDs:\")\n",
        "community_members.show(truncate=False)\n",
        "\n",
        "# Save results to CSV files\n",
        "# Save the main analysis results\n",
        "df_final.select('Client1',\n",
        "                'Client2',\n",
        "                'duration_DDHHMM',\n",
        "                'duration_minutes',\n",
        "                'community_id',\n",
        "                'community_size') \\\n",
        "    .orderBy(\"community_id\") \\\n",
        "    .write.mode(\"overwrite\").csv(\"community_analysis_results\")\n",
        "\n",
        "# Save community members in a flattened format\n",
        "df_final.select('community_id',\n",
        "                'Client1',\n",
        "                'Client2',\n",
        "                'duration_DDHHMM',\n",
        "                'duration_minutes') \\\n",
        "    .distinct() \\\n",
        "    .orderBy(\"community_id\") \\\n",
        "    .write.mode(\"overwrite\").csv(\"community_members_results\")\n",
        "\n",
        "# Optionally, if you want to save additional community statistics\n",
        "community_stats = df_final.groupBy('community_id') \\\n",
        "    .agg(\n",
        "        countDistinct('Client1', 'Client2').alias('unique_members'),\n",
        "        count('*').alias('total_calls'),\n",
        "        sum('duration_minutes').alias('total_duration_minutes'),\n",
        "        avg('duration_minutes').alias('avg_call_duration'),\n",
        "        percentile_approx('duration_minutes', 0.25).alias('duration_25th_percentile'),\n",
        "        percentile_approx('duration_minutes', 0.5).alias('median_call_duration'),\n",
        "        percentile_approx('duration_minutes', 0.75).alias('duration_75th_percentile')\n",
        "    ) \\\n",
        "    .orderBy('community_id')\n",
        "\n",
        "community_stats.write.mode(\"overwrite\").csv(\"community_statistics_results\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWB3dqK0qOgX",
        "outputId": "ac96a7a6-e694-4c18-af2f-34a4d9c2ee6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|community_id|csr_matrix                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
            "+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|1           |[80 04 95 9F 01 00 00 00 00 00 00 8C 11 73 63 69 70 79 2E 73 70 61 72 73 65 2E 5F 63 73 72 94 8C 0A 63 73 72 5F 6D 61 74 72 69 78 94 93 94 29 81 94 7D 94 28 8C 06 5F 73 68 61 70 65 94 4B 03 4B 03 86 94 8C 08 6D 61 78 70 72 69 6E 74 94 4B 32 8C 06 69 6E 64 70 74 72 94 8C 15 6E 75 6D 70 79 2E 63 6F 72 65 2E 6D 75 6C 74 69 61 72 72 61 79 94 8C 0C 5F 72 65 63 6F 6E 73 74 72 75 63 74 94 93 94 8C 05 6E 75 6D 70 79 94 8C 07 6E 64 61 72 72 61 79 94 93 94 4B 00 85 94 43 01 62 94 87 94 52 94 28 4B 01 4B 04 85 94 68 0C 8C 05 64 74 79 70 65 94 93 94 8C 02 69 34 94 89 88 87 94 52 94 28 4B 03 8C 01 3C 94 4E 4E 4E 4A FF FF FF FF 4A FF FF FF FF 4B 00 74 94 62 89 43 10 00 00 00 00 01 00 00 00 02 00 00 00 02 00 00 00 94 74 94 62 8C 07 69 6E 64 69 63 65 73 94 68 0B 68 0E 4B 00 85 94 68 10 87 94 52 94 28 4B 01 4B 02 85 94 68 18 89 43 08 02 00 00 00 00 00 00 00 94 74 94 62 8C 04 64 61 74 61 94 68 0B 68 0E 4B 00 85 94 68 10 87 94 52 94 28 4B 01 4B 02 85 94 68 15 8C 02 66 38 94 89 88 87 94 52 94 28 4B 03 68 19 4E 4E 4E 4A FF FF FF FF 4A FF FF FF FF 4B 00 74 94 62 89 43 10 00 00 00 00 00 C0 72 40 00 00 00 00 00 00 5E 40 94 74 94 62 8C 15 5F 68 61 73 5F 63 61 6E 6F 6E 69 63 61 6C 5F 66 6F 72 6D 61 74 94 88 8C 13 5F 68 61 73 5F 73 6F 72 74 65 64 5F 69 6E 64 69 63 65 73 94 88 75 62 2E]                                                |\n",
            "|2           |[80 04 95 AF 01 00 00 00 00 00 00 8C 11 73 63 69 70 79 2E 73 70 61 72 73 65 2E 5F 63 73 72 94 8C 0A 63 73 72 5F 6D 61 74 72 69 78 94 93 94 29 81 94 7D 94 28 8C 06 5F 73 68 61 70 65 94 4B 04 4B 04 86 94 8C 08 6D 61 78 70 72 69 6E 74 94 4B 32 8C 06 69 6E 64 70 74 72 94 8C 15 6E 75 6D 70 79 2E 63 6F 72 65 2E 6D 75 6C 74 69 61 72 72 61 79 94 8C 0C 5F 72 65 63 6F 6E 73 74 72 75 63 74 94 93 94 8C 05 6E 75 6D 70 79 94 8C 07 6E 64 61 72 72 61 79 94 93 94 4B 00 85 94 43 01 62 94 87 94 52 94 28 4B 01 4B 05 85 94 68 0C 8C 05 64 74 79 70 65 94 93 94 8C 02 69 34 94 89 88 87 94 52 94 28 4B 03 8C 01 3C 94 4E 4E 4E 4A FF FF FF FF 4A FF FF FF FF 4B 00 74 94 62 89 43 14 00 00 00 00 01 00 00 00 02 00 00 00 03 00 00 00 03 00 00 00 94 74 94 62 8C 07 69 6E 64 69 63 65 73 94 68 0B 68 0E 4B 00 85 94 68 10 87 94 52 94 28 4B 01 4B 03 85 94 68 18 89 43 0C 03 00 00 00 00 00 00 00 01 00 00 00 94 74 94 62 8C 04 64 61 74 61 94 68 0B 68 0E 4B 00 85 94 68 10 87 94 52 94 28 4B 01 4B 03 85 94 68 15 8C 02 66 38 94 89 88 87 94 52 94 28 4B 03 68 19 4E 4E 4E 4A FF FF FF FF 4A FF FF FF FF 4B 00 74 94 62 89 43 18 00 00 00 00 00 00 7E 40 00 00 00 00 00 20 8C 40 00 00 00 00 00 80 76 40 94 74 94 62 8C 15 5F 68 61 73 5F 63 61 6E 6F 6E 69 63 61 6C 5F 66 6F 72 6D 61 74 94 88 8C 13 5F 68 61 73 5F 73 6F 72 74 65 64 5F 69 6E 64 69 63 65 73 94 88 75 62 2E]|\n",
            "|3           |[80 04 95 AF 01 00 00 00 00 00 00 8C 11 73 63 69 70 79 2E 73 70 61 72 73 65 2E 5F 63 73 72 94 8C 0A 63 73 72 5F 6D 61 74 72 69 78 94 93 94 29 81 94 7D 94 28 8C 06 5F 73 68 61 70 65 94 4B 04 4B 04 86 94 8C 08 6D 61 78 70 72 69 6E 74 94 4B 32 8C 06 69 6E 64 70 74 72 94 8C 15 6E 75 6D 70 79 2E 63 6F 72 65 2E 6D 75 6C 74 69 61 72 72 61 79 94 8C 0C 5F 72 65 63 6F 6E 73 74 72 75 63 74 94 93 94 8C 05 6E 75 6D 70 79 94 8C 07 6E 64 61 72 72 61 79 94 93 94 4B 00 85 94 43 01 62 94 87 94 52 94 28 4B 01 4B 05 85 94 68 0C 8C 05 64 74 79 70 65 94 93 94 8C 02 69 34 94 89 88 87 94 52 94 28 4B 03 8C 01 3C 94 4E 4E 4E 4A FF FF FF FF 4A FF FF FF FF 4B 00 74 94 62 89 43 14 00 00 00 00 01 00 00 00 02 00 00 00 03 00 00 00 03 00 00 00 94 74 94 62 8C 07 69 6E 64 69 63 65 73 94 68 0B 68 0E 4B 00 85 94 68 10 87 94 52 94 28 4B 01 4B 03 85 94 68 18 89 43 0C 02 00 00 00 00 00 00 00 03 00 00 00 94 74 94 62 8C 04 64 61 74 61 94 68 0B 68 0E 4B 00 85 94 68 10 87 94 52 94 28 4B 01 4B 03 85 94 68 15 8C 02 66 38 94 89 88 87 94 52 94 28 4B 03 68 19 4E 4E 4E 4A FF FF FF FF 4A FF FF FF FF 4B 00 74 94 62 89 43 18 00 00 00 00 00 C0 72 40 00 00 00 00 00 80 66 40 00 00 00 00 00 00 6E 40 94 74 94 62 8C 15 5F 68 61 73 5F 63 61 6E 6F 6E 69 63 61 6C 5F 66 6F 72 6D 61 74 94 88 8C 13 5F 68 61 73 5F 73 6F 72 74 65 64 5F 69 6E 64 69 63 65 73 94 88 75 62 2E]|\n",
            "|4           |[80 04 95 AB 01 00 00 00 00 00 00 8C 11 73 63 69 70 79 2E 73 70 61 72 73 65 2E 5F 63 73 72 94 8C 0A 63 73 72 5F 6D 61 74 72 69 78 94 93 94 29 81 94 7D 94 28 8C 06 5F 73 68 61 70 65 94 4B 03 4B 03 86 94 8C 08 6D 61 78 70 72 69 6E 74 94 4B 32 8C 06 69 6E 64 70 74 72 94 8C 15 6E 75 6D 70 79 2E 63 6F 72 65 2E 6D 75 6C 74 69 61 72 72 61 79 94 8C 0C 5F 72 65 63 6F 6E 73 74 72 75 63 74 94 93 94 8C 05 6E 75 6D 70 79 94 8C 07 6E 64 61 72 72 61 79 94 93 94 4B 00 85 94 43 01 62 94 87 94 52 94 28 4B 01 4B 04 85 94 68 0C 8C 05 64 74 79 70 65 94 93 94 8C 02 69 34 94 89 88 87 94 52 94 28 4B 03 8C 01 3C 94 4E 4E 4E 4A FF FF FF FF 4A FF FF FF FF 4B 00 74 94 62 89 43 10 00 00 00 00 01 00 00 00 03 00 00 00 03 00 00 00 94 74 94 62 8C 07 69 6E 64 69 63 65 73 94 68 0B 68 0E 4B 00 85 94 68 10 87 94 52 94 28 4B 01 4B 03 85 94 68 18 89 43 0C 02 00 00 00 00 00 00 00 02 00 00 00 94 74 94 62 8C 04 64 61 74 61 94 68 0B 68 0E 4B 00 85 94 68 10 87 94 52 94 28 4B 01 4B 03 85 94 68 15 8C 02 66 38 94 89 88 87 94 52 94 28 4B 03 68 19 4E 4E 4E 4A FF FF FF FF 4A FF FF FF FF 4B 00 74 94 62 89 43 18 00 00 00 00 00 C0 72 40 00 00 00 00 00 00 5E 40 00 00 00 00 00 00 7E 40 94 74 94 62 8C 15 5F 68 61 73 5F 63 61 6E 6F 6E 69 63 61 6C 5F 66 6F 72 6D 61 74 94 88 8C 13 5F 68 61 73 5F 73 6F 72 74 65 64 5F 69 6E 64 69 63 65 73 94 88 75 62 2E]            |\n",
            "+------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import pandas_udf, PandasUDFType, col, explode, struct\n",
        "from pyspark.sql.types import BinaryType, StructType, StructField, IntegerType\n",
        "from scipy.sparse import csr_matrix\n",
        "import pandas as pd\n",
        "import pickle\n",
        "'''Decorator and Function Definition:\n",
        "The @pandas_udf decorator marks this function as a Pandas UDF (User Defined Function) that will be applied on grouped data.\n",
        "GROUPED_MAP tells Spark that the function will receive a DataFrame for each group (grouped by community_id).\n",
        "The schema defines the expected output structure of the function, which is a DataFrame with community_id\n",
        "and a binary field containing the serialized matrix.\n",
        "The function converts the connections (edges) between clients into a CSR matrix and serializes it for storage.'''\n",
        "\n",
        "# Define the schema for the Pandas UDF output\n",
        "schema = StructType([\n",
        "    StructField(\"community_id\", IntegerType(), True),\n",
        "    StructField(\"csr_matrix\", BinaryType(), True)\n",
        "])\n",
        "\n",
        "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
        "def create_csr_matrix_from_edges(members_df):\n",
        "    \"\"\"\n",
        "    Creates a serialized CSR matrix from a Spark DataFrame for each community.\n",
        "\n",
        "    Args:\n",
        "        members_df: Spark DataFrame with 'community_id' and 'members' columns.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with 'community_id' and a serialized CSR matrix as binary data.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract the community ID (assuming it's consistent within the group)\n",
        "    community_id = members_df['community_id'].iloc[0]\n",
        "    '''Since each members_df contains data for a single community (due to groupBy operation),\n",
        "    the function retrieves the community_id from the first row.\n",
        "    This ID will be included in the output so that each serialized CSR matrix can be linked back\n",
        "    to its respective community.'''\n",
        "    # Explode the members array to get each connection in separate rows\n",
        "    exploded_df = members_df.explode(\"members\").dropna().reset_index(drop=True)\n",
        "    exploded_df = pd.DataFrame({\n",
        "        'Client1': exploded_df['members'].apply(lambda x: x['Client1']),\n",
        "        'Client2': exploded_df['members'].apply(lambda x: x['Client2']),\n",
        "        'duration_minutes': exploded_df['members'].apply(lambda x: x['duration_minutes'])\n",
        "    })\n",
        "    '''Flattening and Extracting Connection Data:\n",
        "    The members_df contains a column with a list of connections (pairs of clients and call durations).\n",
        "    The function uses explode to convert this list into individual rows, making it easier to work with each connection.\n",
        "    It then creates a new DataFrame, exploded_df, with separate columns for Client1, Client2, and duration_minutes\n",
        "    extracted from the connection data.\n",
        "    This simplifies further processing by ensuring each row represents a single call between two clients.'''\n",
        "    # Get unique clients and create a mapping to indices\n",
        "    unique_clients = pd.concat([exploded_df['Client1'], exploded_df['Client2']]).unique()\n",
        "    client_to_index = {client: i for i, client in enumerate(unique_clients)}\n",
        "    num_clients = len(unique_clients)\n",
        "\n",
        "    # Extract data for CSR matrix\n",
        "    rows = exploded_df['Client1'].map(client_to_index).values\n",
        "    cols = exploded_df['Client2'].map(client_to_index).values\n",
        "    data = exploded_df['duration_minutes'].values #if weight else [1] * len(rows)\n",
        "\n",
        "\n",
        "    # Create CSR matrix\n",
        "    csr = csr_matrix((data, (rows, cols)), shape=(num_clients, num_clients))\n",
        "    '''Serializing the CSR Matrix: The function uses Pythonâ€™s pickle module to serialize the CSR matrix.\n",
        "    This converts the matrix into a binary format, allowing it to be stored or transferred efficiently.\n",
        "    Serialization is necessary because Spark DataFrames cannot directly store complex Python objects like CSR matrices.'''\n",
        "    # Serialize CSR matrix to binary format\n",
        "    serialized_csr = pickle.dumps(csr)\n",
        "\n",
        "    # Return as DataFrame\n",
        "    return pd.DataFrame({\"community_id\": [community_id], \"csr_matrix\": [serialized_csr]})\n",
        "\n",
        "# Use the function to generate a serialized CSR matrix for each community and show the results\n",
        "result = community_members.groupBy(\"community_id\").apply(create_csr_matrix_from_edges)\n",
        "result.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwBOu_C9tleA",
        "outputId": "6c02ae8c-6552-479c-eb96-6ccb42be70d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Community ID: 1\n",
            "   Row  Col  Value\n",
            "0    0    2  300.0\n",
            "1    1    0  120.0\n",
            "----------------------------------------\n",
            "Community ID: 2\n",
            "   Row  Col  Value\n",
            "0    0    3  480.0\n",
            "1    1    0  900.0\n",
            "2    2    1  360.0\n",
            "----------------------------------------\n",
            "Community ID: 3\n",
            "   Row  Col  Value\n",
            "0    0    2  300.0\n",
            "1    1    0  180.0\n",
            "2    2    3  240.0\n",
            "----------------------------------------\n",
            "Community ID: 4\n",
            "   Row  Col  Value\n",
            "0    0    2  300.0\n",
            "1    1    0  120.0\n",
            "2    1    2  480.0\n",
            "----------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "def pretty_print_csr_matrix(csr_matrix_result, weight=True):\n",
        "    \"\"\"Prints a CSR matrix in a readable format.\"\"\"\n",
        "    rows, cols = csr_matrix_result.nonzero()\n",
        "    data = csr_matrix_result.data\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        'Row': rows,\n",
        "        'Col': cols,\n",
        "        'Value': data\n",
        "    })\n",
        "\n",
        "    print(df)\n",
        "\n",
        "# Deserialize and print CSR matrices for each community\n",
        "for row in result.collect():\n",
        "    community_id = row['community_id']\n",
        "    serialized_csr = row['csr_matrix']\n",
        "\n",
        "    # Deserialize the binary data back to a CSR matrix\n",
        "    csr_matrix_result = pickle.loads(serialized_csr)\n",
        "\n",
        "    print(f\"Community ID: {community_id}\")\n",
        "    pretty_print_csr_matrix(csr_matrix_result)\n",
        "    print(\"-\" * 40)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-64vmO6eNT0",
        "outputId": "6db51a12-11c7-4e7a-c8fe-1a6456b7adbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/group_ops.py:104: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Use the function to generate a serialized CSR matrix for each community\n",
        "result = community_members.groupBy(\"community_id\").apply(create_csr_matrix_from_edges)\n",
        "\n",
        "# Padding and calculating DeltaCon similarity\n",
        "def pad_csr_matrix(csr, max_shape):\n",
        "    current_rows, current_cols = csr.shape\n",
        "    max_rows, max_cols = max_shape\n",
        "    if current_rows < max_rows:\n",
        "        additional_rows = csr_matrix((max_rows - current_rows, current_cols))\n",
        "        csr = vstack([csr, additional_rows])\n",
        "    if current_cols < max_cols:\n",
        "        additional_cols = csr_matrix((csr.shape[0], max_cols - current_cols))\n",
        "        csr = hstack([csr, additional_cols])\n",
        "    return csr\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method #1: Deltacon Similarity"
      ],
      "metadata": {
        "id": "xAKW47HXQHKy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "OVAKvqW9jQEg"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse.linalg import inv\n",
        "from scipy.sparse import identity\n",
        "def deltacon_similarity(csr_1, csr_2, epsilon=0.5):\n",
        "    # Ensure both matrices are of the same size\n",
        "    assert csr_1.shape == csr_2.shape, \"Adjacency matrices must be of the same size for comparison.\"\n",
        "    I = identity(csr_1.shape[0])\n",
        "    D1 = csr_1.sum(axis=1).A.flatten()\n",
        "    D1 = csr_matrix((D1, (range(csr_1.shape[0]), range(csr_1.shape[0]))))\n",
        "    D2 = csr_2.sum(axis=1).A.flatten()\n",
        "    D2 = csr_matrix((D2, (range(csr_2.shape[0]), range(csr_2.shape[0]))))\n",
        "\n",
        "    S1 = inv(I + epsilon**2 * D1 - epsilon * csr_1)\n",
        "    S2 = inv(I + epsilon**2 * D2 - epsilon * csr_2)\n",
        "    frobenius_norm = np.sqrt(((S1 - S2).power(2)).sum())\n",
        "    return 1 / (1 + frobenius_norm)\n",
        "max_size = result.rdd.map(lambda row: pickle.loads(row['csr_matrix']).shape).reduce(lambda x, y: (max(x[0], y[0]), max(x[1], y[1])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LXw_Pfk5Q8zf"
      },
      "outputs": [],
      "source": [
        "# Pad CSR matrices and calculate DeltaCon similarity using Spark DataFrame operations\n",
        "def process_csr_matrices(df, max_size):\n",
        "    def pad_and_calculate(row):\n",
        "        csr_matrix_padded = pad_csr_matrix(pickle.loads(row['csr_matrix']), max_size)\n",
        "        serialized_csr = pickle.dumps(csr_matrix_padded)\n",
        "        return (row['community_id'], serialized_csr)\n",
        "\n",
        "    return df.rdd.map(pad_and_calculate).toDF([\"community_id\", \"csr_matrix\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dOG_uZIH_HR",
        "outputId": "04836fb9-32d8-461e-8f3d-db10935d5e6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/group_ops.py:104: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+--------------+-------------------+\n",
            "|community_id_1|community_id_2|similarity         |\n",
            "+--------------+--------------+-------------------+\n",
            "|1             |2             |0.09089789700695985|\n",
            "|1             |3             |0.09282921066953156|\n",
            "|1             |4             |0.40870486948985857|\n",
            "|2             |3             |0.12274808949972305|\n",
            "|2             |4             |0.09485010059678281|\n",
            "|3             |4             |0.09696780258452274|\n",
            "+--------------+--------------+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "padded_result = process_csr_matrices(result, max_size)\n",
        "\n",
        "# Cross join the DataFrame with itself to calculate DeltaCon similarity for all pairs of communities\n",
        "cross_joined = padded_result.alias(\"df1\").crossJoin(padded_result.alias(\"df2\")) \\\n",
        "    .filter(col(\"df1.community_id\") < col(\"df2.community_id\"))\n",
        "\n",
        "# Define a Pandas UDF to calculate similarity for each pair\n",
        "schema_similarity = StructType([\n",
        "    StructField(\"community_id_1\", IntegerType(), True),\n",
        "    StructField(\"community_id_2\", IntegerType(), True),\n",
        "    StructField(\"similarity\", DoubleType(), True)\n",
        "])\n",
        "\n",
        "@pandas_udf(schema_similarity, PandasUDFType.GROUPED_MAP)\n",
        "def calculate_similarity(df):\n",
        "    csr_1 = pickle.loads(df['csr_matrix_1'].iloc[0])\n",
        "    csr_2 = pickle.loads(df['csr_matrix_2'].iloc[0])\n",
        "    similarity = deltacon_similarity(csr_1, csr_2)\n",
        "    return pd.DataFrame({\"community_id_1\": [df['community_id_1'].iloc[0]], \"community_id_2\": [df['community_id_2'].iloc[0]], \"similarity\": [similarity]})\n",
        "\n",
        "cross_joined = cross_joined.select(\n",
        "    col(\"df1.community_id\").alias(\"community_id_1\"),\n",
        "    col(\"df2.community_id\").alias(\"community_id_2\"),\n",
        "    col(\"df1.csr_matrix\").alias(\"csr_matrix_1\"),\n",
        "    col(\"df2.csr_matrix\").alias(\"csr_matrix_2\")\n",
        ")\n",
        "\n",
        "similarities = cross_joined.groupBy(\"community_id_1\", \"community_id_2\").apply(calculate_similarity)\n",
        "\n",
        "similarities.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "fJolyAUxQnim",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aabebeca-dd8b-4764-d3ab-288a0fa4289f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+--------------+-------------------+\n",
            "|community_id_1|community_id_2|similarity         |\n",
            "+--------------+--------------+-------------------+\n",
            "|1             |4             |0.40870486948985857|\n",
            "|2             |3             |0.12274808949972305|\n",
            "+--------------+--------------+-------------------+\n",
            "\n",
            "+---+\n",
            "|id |\n",
            "+---+\n",
            "|1  |\n",
            "|2  |\n",
            "|3  |\n",
            "|4  |\n",
            "+---+\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
            "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+---------------+\n",
            "|component|community_group|\n",
            "+---------+---------------+\n",
            "|2        |[2, 3]         |\n",
            "|1        |[1, 4]         |\n",
            "+---------+---------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import functions as F\n",
        "# Define the similarity threshold\n",
        "similarity_threshold = 0.1  # Adjust this threshold as needed\n",
        "\n",
        "# Filter pairs with similarity above the threshold\n",
        "similar_pairs = similarities.filter(F.col(\"similarity\") >= similarity_threshold)\n",
        "similar_pairs.show(truncate=False)\n",
        "# Create vertices (unique community IDs) and edges (pairs above threshold)\n",
        " # Each community is treated as a node, and high similarity as an edge between nodes\n",
        "vertices = similar_pairs.select(\"community_id_1\").union(similar_pairs.select(\"community_id_2\")).distinct() \\\n",
        "     .withColumnRenamed(\"community_id_1\", \"id\")\n",
        "# vertices.show(truncate=False)\n",
        "edges = similar_pairs.select(F.col(\"community_id_1\").alias(\"src\"), F.col(\"community_id_2\").alias(\"dst\"))\n",
        "\n",
        "# # Build the GraphFrame for community grouping\n",
        "g = GraphFrame(vertices, edges)\n",
        "\n",
        "# # Find connected components (clusters of communities)\n",
        "connected_components = g.connectedComponents()\n",
        "\n",
        "# # Group communities by connected component (cluster)\n",
        "grouped_communities = connected_components.groupBy(\"component\").agg(F.collect_list(\"id\").alias(\"community_group\"))\n",
        "\n",
        "# # Show the clustered communities\n",
        "# print(\"\\nGrouped Communities Based on Similarity Threshold:\")\n",
        "grouped_communities.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Method #2: 50% structural (with cosine similarity) + 50% weighted (with correlation distance)\n",
        "\n",
        "Correlation similarity measures the similarity in the patterns of values (weights) between two vectors. When applied to adjacency matrices (or flattened versions representing edges), it captures how the weights on edges change in proportion across two networks, rather than focusing on their exact values. This can be especially useful if you want to see if the networks have similar patterns in edge weights, regardless of their absolute differences.\n",
        "\n"
      ],
      "metadata": {
        "id": "_5XaOPXDQONY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Define schema for similarity results\n",
        "schema_similarity = StructType([\n",
        "    StructField(\"community_id_1\", IntegerType(), True),\n",
        "    StructField(\"community_id_2\", IntegerType(), True),\n",
        "    StructField(\"cosine_similarity\", DoubleType(), True),\n",
        "    StructField(\"correlation_distance\", DoubleType(), True),\n",
        "    StructField(\"overall_similarity\", DoubleType(), True)\n",
        "])\n",
        "\n",
        "# Function to calculate correlation distance\n",
        "def correlation_distance(vec1, vec2):\n",
        "    if np.std(vec1) == 0 or np.std(vec2) == 0:\n",
        "        return 1.0  # Max distance if there's no variance\n",
        "    return 1 - np.corrcoef(vec1, vec2)[0, 1]\n",
        "\n",
        "# Comparison function for structural and weight-based similarities\n",
        "def compare_structural_and_weight_only(csr_1, csr_2):\n",
        "    # Convert CSR matrices to dense vectors\n",
        "    vec1 = csr_1.toarray().flatten()\n",
        "    vec2 = csr_2.toarray().flatten()\n",
        "    binary_1 = (csr_1.toarray() > 0).astype(int).flatten()  # Convert all non-zero entries to 1\n",
        "    binary_2 = (csr_2.toarray() > 0).astype(int).flatten()\n",
        "\n",
        "    # Cosine similarity for structural similarity\n",
        "    cosine_sim = cosine_similarity([binary_1], [binary_2])[0, 0]\n",
        "\n",
        "    # Weight-only distances\n",
        "    correlation = correlation_distance(vec1, vec2)\n",
        "\n",
        "    return cosine_sim, correlation\n",
        "\n",
        "# Define the Pandas UDF for similarity calculations\n",
        "@pandas_udf(schema_similarity, PandasUDFType.GROUPED_MAP)\n",
        "def compute_similarity(df):\n",
        "    csr_1 = pickle.loads(df['csr_matrix_1'].iloc[0])\n",
        "    csr_2 = pickle.loads(df['csr_matrix_2'].iloc[0])\n",
        "\n",
        "    # Calculate cosine similarity and correlation distance\n",
        "    cosine_sim, correlation = compare_structural_and_weight_only(csr_1, csr_2)\n",
        "\n",
        "    # Calculate overall similarity directly without additional adjustment\n",
        "    overall_similarity = 0.5 * cosine_sim + 0.5 * correlation  # Combine for overall similarity\n",
        "\n",
        "    return pd.DataFrame({\n",
        "        \"community_id_1\": [df['community_id_1'].iloc[0]],\n",
        "        \"community_id_2\": [df['community_id_2'].iloc[0]],\n",
        "        \"cosine_similarity\": [cosine_sim],\n",
        "        \"correlation_distance\": [correlation],\n",
        "        \"overall_similarity\": [overall_similarity]\n",
        "    })\n",
        "\n",
        "# Cross join to get all pairs of communities for comparison\n",
        "cross_joined = padded_result.alias(\"df1\").crossJoin(padded_result.alias(\"df2\")) \\\n",
        "    .filter(F.col(\"df1.community_id\") < F.col(\"df2.community_id\"))\n",
        "\n",
        "# Select relevant columns for the UDF\n",
        "cross_joined = cross_joined.select(\n",
        "    F.col(\"df1.community_id\").alias(\"community_id_1\"),\n",
        "    F.col(\"df2.community_id\").alias(\"community_id_2\"),\n",
        "    F.col(\"df1.csr_matrix\").alias(\"csr_matrix_1\"),\n",
        "    F.col(\"df2.csr_matrix\").alias(\"csr_matrix_2\")\n",
        ")\n",
        "\n",
        "# Apply the similarity calculations\n",
        "similarities = cross_joined.groupBy(\"community_id_1\", \"community_id_2\").apply(compute_similarity)\n",
        "\n",
        "# Show the similarity results including overall similarity\n",
        "similarities.select(\"community_id_1\", \"community_id_2\", \"cosine_similarity\", \"correlation_distance\", \"overall_similarity\").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1W-J8b0QNdW",
        "outputId": "735957c9-57d5-43a2-b49a-8ee54fa469f2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+--------------+------------------+--------------------+------------------+\n",
            "|community_id_1|community_id_2|cosine_similarity |correlation_distance|overall_similarity|\n",
            "+--------------+--------------+------------------+--------------------+------------------+\n",
            "|1             |2             |0.408248290463863 |0.7940497676773866  |0.6011490290706247|\n",
            "|1             |3             |0.816496580927726 |0.2103743489338583  |0.5134354649307922|\n",
            "|1             |4             |0.816496580927726 |0.5041289831766382  |0.6603127820521821|\n",
            "|2             |3             |0.3333333333333334|0.7800127120441522  |0.5566730226887429|\n",
            "|2             |4             |0.3333333333333334|0.9808213778032979  |0.6570773555683157|\n",
            "|3             |4             |0.6666666666666669|0.6528369560371887  |0.6597518113519278|\n",
            "+--------------+--------------+------------------+--------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from graphframes import GraphFrame\n",
        "\n",
        "# Set the overall similarity threshold\n",
        "similarity_threshold = 0.66\n",
        "\n",
        "# Filter pairs with an overall similarity above the threshold\n",
        "similar_pairs = similarities.filter(F.col(\"overall_similarity\") >= similarity_threshold)\n",
        "\n",
        "# Create vertices (unique community IDs) and edges (pairs with similarity above threshold)\n",
        "vertices = similar_pairs.select(\"community_id_1\").union(similar_pairs.select(\"community_id_2\")).distinct() \\\n",
        "    .withColumnRenamed(\"community_id_1\", \"id\")\n",
        "\n",
        "edges = similar_pairs.select(\n",
        "    F.col(\"community_id_1\").alias(\"src\"),\n",
        "    F.col(\"community_id_2\").alias(\"dst\")\n",
        ")\n",
        "\n",
        "# Build the GraphFrame for community grouping\n",
        "g = GraphFrame(vertices, edges)\n",
        "\n",
        "# Find connected components (clusters of communities)\n",
        "connected_components = g.connectedComponents()\n",
        "\n",
        "# Group communities by connected component (cluster)\n",
        "grouped_communities = connected_components.groupBy(\"component\").agg(F.collect_list(\"id\").alias(\"community_group\"))\n",
        "\n",
        "# Show the clustered communities based on the similarity threshold\n",
        "print(\"\\nGrouped Communities Based on Similarity Threshold:\")\n",
        "grouped_communities.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvQP-hGYa8Gv",
        "outputId": "1bca9d5f-406b-4392-fc9b-17f9c7f81fd3"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Grouped Communities Based on Similarity Threshold:\n",
            "+---------+---------------+\n",
            "|component|community_group|\n",
            "+---------+---------------+\n",
            "|1        |[1, 4]         |\n",
            "+---------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJ8vALJnJpAc",
        "outputId": "6047670a-bdc1-46d6-aedf-dd082b82b18b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------+---------------------------------------------------------------------------+\n",
            "|community_id|members                                                                    |\n",
            "+------------+---------------------------------------------------------------------------+\n",
            "|1           |[{2, 3, 000500, 300.0}, {1, 2, 000200, 120.0}]                             |\n",
            "|2           |[{6, 7, 000800, 480.0}, {5, 6, 001500, 900.0}, {4, 5, 000600, 360.0}]      |\n",
            "|3           |[{9, 10, 000500, 300.0}, {8, 9, 000300, 180.0}, {10, 11, 000400, 240.0}]   |\n",
            "|4           |[{13, 14, 000500, 300.0}, {12, 13, 000200, 120.0}, {12, 14, 000800, 480.0}]|\n",
            "+------------+---------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "community_members.show(truncate=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}