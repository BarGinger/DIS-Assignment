{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BarGinger/DIS-Assignment/blob/main/Src/dis_notebook_08_11_24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QFDENKMNs-52",
        "outputId": "89ee5b0d-3f96-4217-e3b5-8abb25b33b18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "openjdk-8-jdk-headless is already the newest version (8u422-b05-1~22.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n",
            "Requirement already satisfied: graphframes in /usr/local/lib/python3.10/dist-packages (0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from graphframes) (1.26.4)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.10/dist-packages (from graphframes) (1.3.7)\n",
            "Requirement already satisfied: sparkmeasure==0.24 in /usr/local/lib/python3.10/dist-packages (0.24.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# 1st cell - Install requirements\n",
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "!pip install graphframes\n",
        "!pip install sparkmeasure==0.24\n",
        "!pip install matplotlib seaborn\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2nd cell - Import libraries\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession, Row, DataFrame\n",
        "from pyspark.sql.functions import (\n",
        "    col,\n",
        "    udf,\n",
        "    row_number,\n",
        "    countDistinct,\n",
        "    collect_list,\n",
        "    struct,\n",
        "    count,\n",
        "    sum,\n",
        "    avg,\n",
        "    expr,\n",
        "    percentile_approx,\n",
        "    max as spark_max,\n",
        "    explode,\n",
        "    round,\n",
        "    rand,\n",
        "    monotonically_increasing_id,\n",
        "    array,\n",
        "    lit,\n",
        "    broadcast,\n",
        "    lag,\n",
        "    pandas_udf,\n",
        "    PandasUDFType,\n",
        "    least,\n",
        "    greatest\n",
        ")\n",
        "import pyspark.sql.functions as F\n",
        "from sparkmeasure import StageMetrics\n",
        "from pyspark.sql.types import (\n",
        "    StringType, IntegerType, BinaryType, DoubleType,\n",
        "    ArrayType, StructType, StructField, LongType, TimestampType\n",
        ")\n",
        "from pyspark.sql import Window\n",
        "from datetime import datetime, timedelta\n",
        "from graphframes import GraphFrame\n",
        "from scipy.sparse import csr_matrix, vstack, hstack\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import base64\n",
        "from sparkmeasure import StageMetrics # for resources monitoring\n",
        "from functools import wraps\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import random\n",
        "from operator import truediv\n",
        "from google.colab import files\n",
        "from itertools import combinations\n",
        "from scipy.sparse.linalg import inv\n",
        "from scipy.sparse import identity\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import shutil"
      ],
      "metadata": {
        "id": "gjrP64v5QyEd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3rd cell - Initialize Spark session\n",
        "spark = SparkSession.builder \\\n",
        "  .appName(\"PhoneCallsCommunityDetection\") \\\n",
        "  .master(\"local[*]\") \\\n",
        "  .config(\"spark.jars.packages\", \"ch.cern.sparkmeasure:spark-measure_2.12:0.24,graphframes:graphframes:0.8.2-spark3.1-s_2.12\") \\\n",
        "  .config(\"spark.executor.memory\", \"20G\") \\\n",
        "  .config(\"spark.driver.memory\", \"50G\") \\\n",
        "  .config(\"spark.executor.memoryOverhead\", \"1G\") \\\n",
        "  .config(\"spark.default.parallelism\", \"100\") \\\n",
        "  .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
        "  .config(\"spark.driver.maxResultSize\", \"2G\") \\\n",
        "  .getOrCreate()\n",
        "\n",
        "# Initialize StageMetrics\n",
        "stagemetrics = StageMetrics(spark)\n",
        "\n",
        "# Optional: Set logging level to reduce verbosity\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "# Set a checkpoint directory for Spark\n",
        "spark.sparkContext.setCheckpointDir(\"/tmp/spark-checkpoints\")"
      ],
      "metadata": {
        "id": "xSdU6FWDinu3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4th cell - Generate datasets - PLEASE only run this if datasets folder is empty / does not exists\n",
        "def generate_communities(spark, num_communities, community_size_range, density=0.3, extra_factor=1.5):\n",
        "    \"\"\"\n",
        "    Generate isolated communities with controlled sizes and connections.\n",
        "    Ensures enough connections for the sample count by using an extra factor.\n",
        "    \"\"\"\n",
        "    communities = []\n",
        "    for community_id in range(num_communities):\n",
        "        size = random.randint(community_size_range[0], community_size_range[1])\n",
        "        base_id = community_id * 1000\n",
        "        community_clients = [(community_id, base_id + i, base_id + j)\n",
        "                             for i in range(size) for j in range(i + 1, size) if random.random() < density * extra_factor]\n",
        "        communities.extend(community_clients)\n",
        "\n",
        "    return spark.createDataFrame(communities, [\"community_id\", \"client1\", \"client2\"])\n",
        "\n",
        "def generate_call_times(communities_df, calls_per_connection_range, duration_range, base_time, num_samples):\n",
        "    \"\"\"\n",
        "    Generate call start and end times for each client connection, ensuring total number of samples matches `num_samples`.\n",
        "    \"\"\"\n",
        "    calls_df = communities_df.withColumn(\n",
        "        \"num_calls\",\n",
        "        F.expr(f\"floor(rand() * ({calls_per_connection_range[1]} - {calls_per_connection_range[0]} + 1)) + {calls_per_connection_range[0]}\")\n",
        "    ).withColumn(\n",
        "        \"call_id\", F.monotonically_increasing_id()\n",
        "    ).withColumn(\n",
        "        \"calls\", F.expr(\"sequence(1, num_calls)\")\n",
        "    ).select(\"client1\", \"client2\", \"call_id\", F.explode(\"calls\").alias(\"call_num\"))\n",
        "\n",
        "    def generate_times():\n",
        "        start_time = base_time + timedelta(minutes=random.randint(0, 1440))\n",
        "        duration = random.randint(duration_range[0], duration_range[1])\n",
        "        end_time = start_time + timedelta(minutes=duration)\n",
        "        return start_time.strftime('%y%m%d%H%M'), end_time.strftime('%y%m%d%H%M')\n",
        "\n",
        "    time_udf = F.udf(lambda: generate_times(), \"struct<Start_Time:string, End_Time:string>\")\n",
        "    calls_df = calls_df.withColumn(\"call_times\", time_udf())\n",
        "\n",
        "    # Ensure consistent schema for the final DataFrame\n",
        "    calls_df = calls_df.select(\n",
        "        \"client1\", \"client2\", calls_df[\"call_times.Start_Time\"].alias(\"Start_Time\"), calls_df[\"call_times.End_Time\"].alias(\"End_Time\")\n",
        "    )\n",
        "\n",
        "    # Limit to the specified number of samples\n",
        "    final_calls_df = calls_df.limit(num_samples)\n",
        "\n",
        "    # Retry generation if the sample count isn't met\n",
        "    while final_calls_df.count() < num_samples:\n",
        "        additional_df = calls_df.limit(num_samples - final_calls_df.count()).select(\"client1\", \"client2\", \"Start_Time\", \"End_Time\")\n",
        "        final_calls_df = final_calls_df.union(additional_df).limit(num_samples)\n",
        "\n",
        "    return final_calls_df\n",
        "\n",
        "# Function to delete all generated datasets\n",
        "def delete_generated_datasets():\n",
        "    folder_path = \"/content/datasets/\"\n",
        "    deleted_files = []\n",
        "\n",
        "    if os.path.exists(folder_path):\n",
        "        # Loop through each item in the folder\n",
        "        for item in os.listdir(folder_path):\n",
        "            item_path = os.path.join(folder_path, item)\n",
        "            # Check if it's a directory and remove it with shutil.rmtree\n",
        "            if os.path.isdir(item_path):\n",
        "                shutil.rmtree(item_path)\n",
        "            else:\n",
        "                os.remove(item_path)\n",
        "            deleted_files.append(item)\n",
        "\n",
        "        # Print the results\n",
        "        if deleted_files:\n",
        "            print(\"Deleted the following items:\")\n",
        "            for item in deleted_files:\n",
        "                print(item)\n",
        "        else:\n",
        "            print(\"No files found in the folder to delete.\")\n",
        "    else:\n",
        "        print(\"The folder does not exist.\")\n",
        "\n",
        "def save_dataset(dataset, filename):\n",
        "    \"\"\"\n",
        "    Save the generated dataset to a temporary directory and then move to the final directory.\n",
        "\n",
        "    Parameters:\n",
        "        dataset (DataFrame): The DataFrame to save, containing generated call data.\n",
        "        filename (str): Base name for the dataset file.\n",
        "    \"\"\"\n",
        "    # Define the directories\n",
        "    final_dir = \"/content/datasets\"\n",
        "    temp_dir = f\"{final_dir}/{filename}_temp\"\n",
        "    final_path = os.path.join(final_dir, filename)\n",
        "\n",
        "    # Create directories if they don't exist\n",
        "    os.makedirs(final_dir, exist_ok=True)\n",
        "\n",
        "    # Write to the temporary directory\n",
        "    dataset.write.mode(\"overwrite\").option(\"header\", \"true\").csv(temp_dir)\n",
        "\n",
        "    # Move the content to the final directory\n",
        "    if os.path.exists(final_path):\n",
        "        shutil.rmtree(final_path)\n",
        "    shutil.move(temp_dir, final_path)\n",
        "\n",
        "    # Clean up by removing the temporary directory\n",
        "    if os.path.exists(temp_dir):\n",
        "        shutil.rmtree(temp_dir)\n",
        "\n",
        "    print(f\"Dataset saved as {final_path}\")\n",
        "    # files.download(final_path)\n",
        "    return final_path\n",
        "\n",
        "\n",
        "def export_datasets_to_computer(folder_path=\"/content/datasets\"):\n",
        "    \"\"\"\n",
        "    Compresses and exports each dataset in the specified folder to download to the local computer.\n",
        "\n",
        "    Parameters:\n",
        "        folder_path (str): The path to the folder containing datasets.\n",
        "    \"\"\"\n",
        "    # Check if the folder exists\n",
        "    if not os.path.exists(folder_path):\n",
        "        print(f\"The folder '{folder_path}' does not exist.\")\n",
        "        return\n",
        "\n",
        "    # Iterate over each file/directory in the folder\n",
        "    for item in os.listdir(folder_path):\n",
        "        item_path = os.path.join(folder_path, item)\n",
        "\n",
        "        # If it's a directory (dataset in CSV format is usually saved as a directory)\n",
        "        if os.path.isdir(item_path):\n",
        "            # Create a zip file of the dataset directory\n",
        "            zip_filename = f\"{item}.zip\"\n",
        "            shutil.make_archive(item_path, 'zip', item_path)\n",
        "            print(f\"Compressed '{item}' as '{zip_filename}'.\")\n",
        "\n",
        "            # Download the zip file\n",
        "            files.download(f\"{item_path}.zip\")\n",
        "        else:\n",
        "            # Download individual files if they are directly in the folder\n",
        "            files.download(item_path)\n",
        "\n",
        "    print(\"All datasets have been exported to your computer.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    delete_generated_datasets()\n",
        "    # spark = create_spark_session()\n",
        "\n",
        "    # Define parameter configurations with num_samples\n",
        "    parameter_sets = [\n",
        "        {\"num_communities\": 5, \"community_size_range\": (3, 5), \"calls_per_connection_range\": (1, 2), \"duration_range\": (30, 120), \"density\": 0.5, \"num_samples\": 50},\n",
        "        {\"num_communities\": 10, \"community_size_range\": (5, 7), \"calls_per_connection_range\": (1, 3), \"duration_range\": (30, 180), \"density\": 0.4, \"num_samples\": 100},\n",
        "        {\"num_communities\": 100, \"community_size_range\": (5, 100), \"calls_per_connection_range\": (1, 5), \"duration_range\": (30, 380), \"density\": 0.4, \"num_samples\": 5000},\n",
        "        {\"num_communities\": 5000, \"community_size_range\": (5, 100), \"calls_per_connection_range\": (1, 5), \"duration_range\": (30, 380), \"density\": 0.4, \"num_samples\": 50000}\n",
        "    ]\n",
        "\n",
        "    base_time = datetime(2024, 1, 1)\n",
        "\n",
        "    for i, params in enumerate(parameter_sets):\n",
        "        print(f\"\\nGenerating dataset for configuration {i + 1}: {params}\")\n",
        "\n",
        "        # Generate communities and call times\n",
        "        communities_df = generate_communities(\n",
        "            spark,\n",
        "            num_communities=params[\"num_communities\"],\n",
        "            community_size_range=params[\"community_size_range\"],\n",
        "            density=params[\"density\"],\n",
        "            extra_factor=2  # Generate more potential connections initially\n",
        "        )\n",
        "        calls_df = generate_call_times(\n",
        "            communities_df,\n",
        "            calls_per_connection_range=params[\"calls_per_connection_range\"],\n",
        "            duration_range=params[\"duration_range\"],\n",
        "            base_time=base_time,\n",
        "            num_samples=params[\"num_samples\"]\n",
        "        )\n",
        "\n",
        "        # Save dataset and print information\n",
        "        filename = f\"dataset_config_{i + 1}\"\n",
        "        final_path = save_dataset(calls_df, filename)\n",
        "        parameter_sets[i]['dataset_name'] = filename\n",
        "        parameter_sets[i]['csv_filename'] = final_path\n",
        "\n",
        "    df_datasets = pd.DataFrame(parameter_sets)\n",
        "    dataset_metadata_file_path = \"dataset_metadata.csv\"\n",
        "    df_datasets.to_csv(dataset_metadata_file_path, index=False)\n",
        "    files.download(dataset_metadata_file_path)\n",
        "    # Run the function to export all datasets\n",
        "    export_datasets_to_computer(\"/content/datasets\")"
      ],
      "metadata": {
        "id": "WJPkATltavF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5th cell - Initialize resource monitoring\n",
        "\n",
        "# Monitor CPU, Memory and running time\n",
        "def track_stage(stage_name):\n",
        "    def decorator(func):\n",
        "        @wraps(func)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            print(f\"Starting {stage_name}\")\n",
        "            stagemetrics.begin()  # Begin collecting metrics for this stage\n",
        "\n",
        "            result = func(*args, **kwargs)  # Run the actual function\n",
        "\n",
        "            stagemetrics.end()  # Stop collecting metrics for this stage\n",
        "\n",
        "            time.sleep(15)\n",
        "\n",
        "            # Generate metrics DataFrame\n",
        "            print(f\"Completed {stage_name}\\n\")\n",
        "            df_metrics_all = stagemetrics.create_stagemetrics_DF()\n",
        "            df_metrics_agg = stagemetrics.aggregate_stagemetrics_DF()\n",
        "            # Add stage_name column and join metrics and memory DataFrames\n",
        "            df_metrics_agg = df_metrics_agg.withColumn(\"stage_name\", pyspark.sql.functions.lit(stage_name))\n",
        "            df_metrics_agg = df_metrics_agg.withColumn(\"dataset\", pyspark.sql.functions.lit(dataset_file_path))\n",
        "            df_metrics_agg.show(truncate=False)\n",
        "\n",
        "            # Set write mode based on the stage\n",
        "            if \"Stage 1\" in stage_name and clear_csv:\n",
        "                write_mode = \"overwrite\"\n",
        "                header = \"true\"\n",
        "            else:\n",
        "                write_mode = \"append\"\n",
        "                header = \"true\"\n",
        "\n",
        "            # Write metrics to CSV with appropriate mode and header settings\n",
        "            df_metrics_agg.coalesce(1).write \\\n",
        "                .mode(write_mode) \\\n",
        "                .option(\"header\", header) \\\n",
        "                .csv(f\"{dataset_name}_stage_metrics\")\n",
        "\n",
        "            return result\n",
        "        return wrapper\n",
        "    return decorator"
      ],
      "metadata": {
        "id": "R9jMJMWzY2LP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6th cell - All the utilities functions for the project\n",
        "\n",
        "# Convert YYMMDDHHMM to a proper datetime object\n",
        "def calculate_duration_minutes(start_time, end_time):\n",
        "  \"\"\"\n",
        "  Calculate the duration between two times in minutes.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  start_time : str\n",
        "      The start time in HH:MM:SS format.\n",
        "  end_time : str\n",
        "      The end time in HH:MM:SS format.\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  duration_minutes : float\n",
        "      The duration between start_time and end_time in minutes.\n",
        "  \"\"\"\n",
        "  start_datetime = convert_to_datetime(start_time)\n",
        "  end_datetime = convert_to_datetime(end_time)\n",
        "  duration = end_datetime - start_datetime\n",
        "  duration_minutes = duration.total_seconds() / 60\n",
        "  return duration_minutes\n",
        "\n",
        "def convert_to_datetime(time_str):\n",
        "  \"\"\"\n",
        "  Convert a time string in '%y%m%d%H%M' format to a datetime object.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  time_str : str\n",
        "      The time string in '%y%m%d%H%M' format.\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  datetime_obj : datetime.datetime\n",
        "      The datetime object representing the given time string.\n",
        "  \"\"\"\n",
        "  # Use datetime.datetime.strptime to parse the time string\n",
        "  # This is the correct way to use strptime, avoiding the AttributeError\n",
        "  return datetime.strptime(str(time_str), '%y%m%d%H%M')\n",
        "\n",
        "# Define UDF for calculating duration in DDHHMM format\n",
        "def calculate_duration_string(start_time, end_time):\n",
        "    start_dt = convert_to_datetime(start_time)\n",
        "    end_dt = convert_to_datetime(end_time)\n",
        "    duration = end_dt - start_dt\n",
        "\n",
        "    days = duration.days\n",
        "    hours, remainder = divmod(duration.seconds, 3600)\n",
        "    minutes = remainder // 60\n",
        "    return f'{days:02d}{hours:02d}{minutes:02d}'\n",
        "\n",
        "\n",
        "'''Decorator and Function Definition:\n",
        "The @pandas_udf decorator marks this function as a Pandas UDF (User Defined Function) that will be applied on grouped data.\n",
        "GROUPED_MAP tells Spark that the function will receive a DataFrame for each group (grouped by community_id).\n",
        "The schema defines the expected output structure of the function, which is a DataFrame with community_id\n",
        "and a binary field containing the serialized matrix.\n",
        "The function converts the connections (edges) between clients into a CSR matrix and serializes it for storage.'''\n",
        "\n",
        "# Define the schema for the Pandas UDF output\n",
        "schema = StructType([\n",
        "    StructField(\"community_id\", IntegerType(), True),\n",
        "    StructField(\"csr_matrix\", BinaryType(), True)\n",
        "])\n",
        "\n",
        "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
        "def create_csr_matrix_from_edges(members_df, weight):\n",
        "    \"\"\"\n",
        "    Creates a serialized CSR matrix from a Spark DataFrame for each community.\n",
        "\n",
        "    Args:\n",
        "        members_df: Spark DataFrame with 'community_id' and 'members' columns.\n",
        "        weight: boolean if to take the weights into account or not\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with 'community_id' and a serialized CSR matrix as binary data.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract the community ID (assuming it's consistent within the group)\n",
        "    community_id = members_df['community_id'].iloc[0]\n",
        "    '''Since each members_df contains data for a single community (due to groupBy operation),\n",
        "    the function retrieves the community_id from the first row.\n",
        "    This ID will be included in the output so that each serialized CSR matrix can be linked back\n",
        "    to its respective community.'''\n",
        "    # Explode the members array to get each connection in separate rows\n",
        "    exploded_df = members_df.explode(\"members\").dropna().reset_index(drop=True)\n",
        "    exploded_df = pd.DataFrame({\n",
        "        'Client1': exploded_df['members'].apply(lambda x: x['Client1']),\n",
        "        'Client2': exploded_df['members'].apply(lambda x: x['Client2']),\n",
        "        'total_duration_minutes': exploded_df['members'].apply(lambda x: x['total_duration_minutes'])\n",
        "    })\n",
        "    '''Flattening and Extracting Connection Data:\n",
        "    The members_df contains a column with a list of connections (pairs of clients and call durations).\n",
        "    The function uses explode to convert this list into individual rows, making it easier to work with each connection.\n",
        "    It then creates a new DataFrame, exploded_df, with separate columns for Client1, Client2, and duration_minutes\n",
        "    extracted from the connection data.\n",
        "    This simplifies further processing by ensuring each row represents a single call between two clients.'''\n",
        "    # Get unique clients and create a mapping to indices\n",
        "    unique_clients = sorted(pd.concat([exploded_df['Client1'], exploded_df['Client2']]).unique())\n",
        "    client_to_index = {client: i for i, client in enumerate(unique_clients)}\n",
        "    num_clients = len(unique_clients)\n",
        "\n",
        "    # Extract data for CSR matrix\n",
        "    rows = exploded_df['Client1'].map(client_to_index).values\n",
        "    cols = exploded_df['Client2'].map(client_to_index).values\n",
        "    if use_weights:\n",
        "      data = exploded_df['total_duration_minutes'].values #if weight else [1] * len(rows)\n",
        "    else:\n",
        "      data = [1] * len(rows)\n",
        "\n",
        "    # Create CSR matrix\n",
        "    csr = csr_matrix((data, (rows, cols)), shape=(num_clients, num_clients))\n",
        "    '''Serializing the CSR Matrix: The function uses Python’s pickle module to serialize the CSR matrix.\n",
        "    This converts the matrix into a binary format, allowing it to be stored or transferred efficiently.\n",
        "    Serialization is necessary because Spark DataFrames cannot directly store complex Python objects like CSR matrices.'''\n",
        "    # Serialize CSR matrix to binary format\n",
        "    serialized_csr = pickle.dumps(csr)\n",
        "\n",
        "    # Return as DataFrame\n",
        "    return pd.DataFrame({\"community_id\": [community_id], \"csr_matrix\": [serialized_csr]})\n",
        "\n",
        "# prompt: print csr_matrix_result pretty\n",
        "def pretty_print_csr_matrix(csr_matrix_result):\n",
        "  \"\"\"Prints a CSR matrix in a readable format.\"\"\"\n",
        "\n",
        "  rows, cols = csr_matrix_result.nonzero()\n",
        "  data = csr_matrix_result.data\n",
        "\n",
        "  df = pd.DataFrame({\n",
        "      'Row': rows,\n",
        "      'Col': cols,\n",
        "      'Value': data\n",
        "  })\n",
        "\n",
        "  print(df)\n",
        "\n",
        "# Padding and calculating DeltaCon similarity\n",
        "def pad_csr_matrix(csr, max_shape):\n",
        "    current_rows, current_cols = csr.shape\n",
        "    max_rows, max_cols = max_shape\n",
        "    if current_rows < max_rows:\n",
        "        additional_rows = csr_matrix((max_rows - current_rows, current_cols))\n",
        "        csr = vstack([csr, additional_rows])\n",
        "    if current_cols < max_cols:\n",
        "        additional_cols = csr_matrix((csr.shape[0], max_cols - current_cols))\n",
        "        csr = hstack([csr, additional_cols])\n",
        "    return csr\n",
        "\n",
        "# Pad CSR matrices and calculate DeltaCon similarity using Spark DataFrame operations\n",
        "def process_csr_matrices(df, max_size):\n",
        "    def pad_and_calculate(row):\n",
        "        csr_matrix_padded = pad_csr_matrix(pickle.loads(row['csr_matrix']), max_size)\n",
        "        serialized_csr = pickle.dumps(csr_matrix_padded)\n",
        "        return (row['community_id'], serialized_csr)\n",
        "\n",
        "    return df.rdd.map(pad_and_calculate).toDF([\"community_id\", \"csr_matrix\"])\n",
        "\n",
        "def normalize_matrix(matrix):\n",
        "    \"\"\"\n",
        "    Normalize the matrix values to the range [0, 1].\n",
        "\n",
        "    Parameters:\n",
        "    matrix : csr_matrix\n",
        "        Sparse matrix to normalize.\n",
        "\n",
        "    Returns:\n",
        "    csr_matrix\n",
        "        Normalized sparse matrix.\n",
        "    \"\"\"\n",
        "    data = matrix.data\n",
        "    if len(data) == 0:  # Handle empty matrices\n",
        "        return matrix\n",
        "    min_val = np.min(data)\n",
        "    max_val = np.max(data)\n",
        "    normalized_data = (data - min_val) / (max_val - min_val) if max_val > min_val else data\n",
        "    return matrix.__class__((normalized_data, matrix.indices, matrix.indptr), shape=matrix.shape)\n",
        "\n",
        "def frobenius_norm(csr_1, csr_2):\n",
        "    \"\"\"\n",
        "    Compute Frobenius norm between two sparse matrices.\n",
        "\n",
        "    Parameters:\n",
        "    csr_1, csr_2 : csr_matrix\n",
        "        Sparse adjacency matrices of the graphs.\n",
        "\n",
        "    Returns:\n",
        "    float\n",
        "        Frobenius norm distance between the graphs.\n",
        "    \"\"\"\n",
        "    # csr_1 = log_transform_matrix(csr_1)\n",
        "    # csr_2 = log_transform_matrix(csr_2)\n",
        "    csr_1 = normalize_matrix(csr_1)\n",
        "    csr_2 = normalize_matrix(csr_2)\n",
        "    assert csr_1.shape == csr_2.shape, \"Adjacency matrices must have the same dimensions.\"\n",
        "    diff = csr_1 - csr_2\n",
        "    return np.sqrt((diff.power(2)).sum())\n",
        "\n",
        "def frobenius_sim(csr_1, csr_2):\n",
        "    \"\"\"\n",
        "    Adds a similarity column to the DataFrame based on Frobenius distance.\n",
        "\n",
        "    Parameters:\n",
        "    df (DataFrame): Input DataFrame containing 'frobenius_distance' column.\n",
        "\n",
        "    Returns:\n",
        "    DataFrame: A DataFrame with an additional 'similarity' column.\n",
        "    \"\"\"\n",
        "    dist=frobenius_norm(csr_1, csr_2)\n",
        "    return 1 / (1 + dist)\n",
        "\n",
        "\n",
        "def deltacon_similarity(csr_1, csr_2, epsilon=0.5):\n",
        "    # Ensure both matrices are of the same size\n",
        "    assert csr_1.shape == csr_2.shape, \"Adjacency matrices must be of the same size for comparison.\"\n",
        "    I = identity(csr_1.shape[0])\n",
        "    D1 = csr_1.sum(axis=1).A.flatten()\n",
        "    D1 = csr_matrix((D1, (range(csr_1.shape[0]), range(csr_1.shape[0]))))\n",
        "    D2 = csr_2.sum(axis=1).A.flatten()\n",
        "    D2 = csr_matrix((D2, (range(csr_2.shape[0]), range(csr_2.shape[0]))))\n",
        "\n",
        "    S1 = inv(I + epsilon**2 * D1 - epsilon * csr_1)\n",
        "    S2 = inv(I + epsilon**2 * D2 - epsilon * csr_2)\n",
        "    frobenius_norm = np.sqrt(((S1 - S2).power(2)).sum())\n",
        "    return 1 / (1 + frobenius_norm)\n",
        "\n",
        "# Define function to calculate Frobenius similarity\n",
        "def calculate_frobenius_similarity(grouped_df):\n",
        "    community_id_1 = grouped_df.iloc[0]['community_id']\n",
        "    community_id_2 = grouped_df.iloc[0]['community_id_2']\n",
        "    csr_1 = pickle.loads(grouped_df.iloc[0]['csr_matrix'])\n",
        "    csr_2 = pickle.loads(grouped_df.iloc[0]['csr_matrix_2'])\n",
        "    similarity_score_f = frobenius_sim(csr_1, csr_2)\n",
        "    return pd.DataFrame([{\n",
        "        \"community_id_1\": community_id_1,\n",
        "        \"community_id_2\": community_id_2,\n",
        "        \"frobenius_similarity\": similarity_score_f\n",
        "    }])\n",
        "\n",
        "# Define function to calculate DeltaCon similarity\n",
        "def calculate_deltacon_similarity(grouped_df):\n",
        "    community_id_1 = grouped_df.iloc[0]['community_id']\n",
        "    community_id_2 = grouped_df.iloc[0]['community_id_2']\n",
        "    csr_1 = pickle.loads(grouped_df.iloc[0]['csr_matrix'])\n",
        "    csr_2 = pickle.loads(grouped_df.iloc[0]['csr_matrix_2'])\n",
        "    similarity_score_d = deltacon_similarity(csr_1, csr_2)\n",
        "    return pd.DataFrame([{\n",
        "        \"community_id_1\": community_id_1,\n",
        "        \"community_id_2\": community_id_2,\n",
        "        \"deltacon\": similarity_score_d\n",
        "    }])\n",
        "\n",
        "# Comparison function for structural and weight-based similarities\n",
        "def cosine_sim(csr_1, csr_2):\n",
        "    # Compute cosine similarity\n",
        "    cosine_sim = cosine_similarity(csr_1, csr_2)\n",
        "    return cosine_sim\n",
        "\n",
        "\n",
        "def calculate_similarities(subgroup_community_members):\n",
        "  \"\"\"\n",
        "  Comparing CSR matrices to detect similarity\n",
        "  \"\"\"\n",
        "\n",
        "  max_size = result.rdd.map(lambda row: pickle.loads(row['csr_matrix']).shape).reduce(lambda x, y: (max(x[0], y[0]), max(x[1], y[1])))\n",
        "\n",
        "  padded_result_true = process_csr_matrices(subgroup_community_members, max_size)\n",
        "  padded_result_false = process_csr_matrices(subgroup_community_members, max_size)\n",
        "\n",
        "  # Step 1: Compute Frobenius Similarity (using padded_result_true)\n",
        "\n",
        "  # Rename columns from df2 to remove ambiguity for Frobenius similarity calculation\n",
        "  padded_result_true_renamed = padded_result_true.select(\n",
        "      col(\"community_id\").alias(\"community_id_2\"),\n",
        "      col(\"csr_matrix\").alias(\"csr_matrix_2\")\n",
        "  )\n",
        "\n",
        "  # Cross join to compare every community for Frobenius similarity\n",
        "  cross_joined_df_frobenius = padded_result_true.alias(\"df1\").crossJoin(padded_result_true_renamed.alias(\"df2\")) \\\n",
        "      .filter(col(\"df1.community_id\") < col(\"df2.community_id_2\"))\n",
        "\n",
        "  # Define schema for Frobenius similarity output\n",
        "  frobenius_similarity_schema = StructType([\n",
        "      StructField(\"community_id_1\", IntegerType(), True),\n",
        "      StructField(\"community_id_2\", IntegerType(), True),\n",
        "      StructField(\"frobenius_similarity\", DoubleType(), True)\n",
        "  ])\n",
        "\n",
        "  # Apply Frobenius similarity calculation\n",
        "  frobenius_similarity_df = cross_joined_df_frobenius.select(\n",
        "      \"df1.community_id\", \"df2.community_id_2\", \"df1.csr_matrix\", \"df2.csr_matrix_2\"\n",
        "  ).groupBy(\"community_id\", \"community_id_2\") \\\n",
        "      .applyInPandas(calculate_frobenius_similarity, schema=frobenius_similarity_schema)\n",
        "\n",
        "  #Step 2: Compute DeltaCon Similarity (using padded_result_false)\n",
        "\n",
        "  # Rename columns from df2 to remove ambiguity for DeltaCon similarity calculation\n",
        "  padded_result_false_renamed = padded_result_false.select(\n",
        "      col(\"community_id\").alias(\"community_id_2\"),\n",
        "      col(\"csr_matrix\").alias(\"csr_matrix_2\")\n",
        "  )\n",
        "\n",
        "  # Cross join to compare every community for DeltaCon similarity\n",
        "  cross_joined_df_deltacon = padded_result_false.alias(\"df1\").crossJoin(padded_result_false_renamed.alias(\"df2\")) \\\n",
        "      .filter(col(\"df1.community_id\") < col(\"df2.community_id_2\"))\n",
        "\n",
        "  # Define schema for DeltaCon similarity output\n",
        "  deltacon_similarity_schema = StructType([\n",
        "      StructField(\"community_id_1\", IntegerType(), True),\n",
        "      StructField(\"community_id_2\", IntegerType(), True),\n",
        "      StructField(\"deltacon\", DoubleType(), True)\n",
        "  ])\n",
        "\n",
        "  # Apply DeltaCon similarity calculation\n",
        "  deltacon_similarity_df = cross_joined_df_deltacon.select(\n",
        "      \"df1.community_id\", \"df2.community_id_2\", \"df1.csr_matrix\", \"df2.csr_matrix_2\"\n",
        "  ).groupBy(\"community_id\", \"community_id_2\") \\\n",
        "      .applyInPandas(calculate_deltacon_similarity, schema=deltacon_similarity_schema)\n",
        "\n",
        "  # Step 3: Join Results and Calculate Final Similarity Score\n",
        "  # Join the Frobenius and DeltaCon similarity DataFrames\n",
        "  combined_similarity_df = frobenius_similarity_df.join(\n",
        "      deltacon_similarity_df,\n",
        "      on=[\"community_id_1\", \"community_id_2\"],\n",
        "      how=\"inner\"\n",
        "  )\n",
        "\n",
        "  # Calculate the final similarity score as an average of Frobenius and DeltaCon similarities\n",
        "  final_similarity_df = combined_similarity_df.withColumn(\n",
        "      \"final_similarity\",\n",
        "      expr(\"0.5 * frobenius_similarity + 0.5 * deltacon\")\n",
        "  )\n",
        "\n",
        "  # Show the final results\n",
        "  final_similarity_df.show(truncate=False)\n",
        "  return final_similarity_df\n",
        "\n",
        "  @pandas_udf(schema_similarity, PandasUDFType.GROUPED_MAP)\n",
        "  def calculate_similarity(df):\n",
        "      csr_1 = pickle.loads(df['csr_matrix_1'].iloc[0])\n",
        "      csr_2 = pickle.loads(df['csr_matrix_2'].iloc[0])\n",
        "      similarity = deltacon_similarity(csr_1, csr_2)\n",
        "      return pd.DataFrame({\"community_id_1\": [df['community_id_1'].iloc[0]], \"community_id_2\": [df['community_id_2'].iloc[0]], \"similarity\": [similarity]})\n",
        "\n",
        "  cross_joined = cross_joined.select(\n",
        "      col(\"df1.community_id\").alias(\"community_id_1\"),\n",
        "      col(\"df2.community_id\").alias(\"community_id_2\"),\n",
        "      col(\"df1.csr_matrix\").alias(\"csr_matrix_1\"),\n",
        "      col(\"df2.csr_matrix\").alias(\"csr_matrix_2\")\n",
        "  )\n",
        "\n",
        "  similarities = cross_joined.groupBy(\"community_id_1\", \"community_id_2\").apply(calculate_similarity)\n",
        "\n",
        "  similarities.show(truncate=False)\n",
        "\n",
        "\n",
        "def create_adaptive_buckets(df, columns, min_size=2):\n",
        "    \"\"\"\n",
        "    Create adaptive buckets for specified columns based on natural grouping of close values.\n",
        "\n",
        "    Parameters:\n",
        "        df (DataFrame): The input DataFrame with community statistics.\n",
        "        columns (list): List of column names to bucketize.\n",
        "        min_size (int): Minimum number of communities required in each bucket.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: The DataFrame with additional columns for each adaptive bucket.\n",
        "    \"\"\"\n",
        "    for column in columns:\n",
        "        bucket_col = f\"{column}_bucket\"\n",
        "\n",
        "        # Calculate approximate quantiles for balanced bucketing\n",
        "        quantiles = [i / min_size for i in range(min_size + 1)]\n",
        "        boundaries = df.approxQuantile(column, quantiles, 0.05)\n",
        "\n",
        "        # Materialize boundaries into discrete bucket assignments\n",
        "        bucket_expr = F.when(F.col(column) <= boundaries[1], 0)\n",
        "        for i in range(1, len(boundaries) - 1):\n",
        "            bucket_expr = bucket_expr.when((F.col(column) > boundaries[i]) & (F.col(column) <= boundaries[i + 1]), i)\n",
        "\n",
        "        # Assign buckets to each row based on column values\n",
        "        df = df.withColumn(bucket_col, bucket_expr)\n",
        "\n",
        "        # Debug: Verify if the bucket column was created successfully\n",
        "        print(f\"Verifying creation of {bucket_col} column\")\n",
        "        df.select(column, bucket_col).show(truncate=False)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Step 2: Apply cross join within each bucket and calculate similarities\n",
        "def calculate_similarity_within_buckets(df, columns):\n",
        "    \"\"\"\n",
        "    Calculate similarities between communities within the same buckets for specified columns.\n",
        "\n",
        "    Parameters:\n",
        "        df (DataFrame): The input DataFrame with bucket columns for each specified attribute.\n",
        "        columns (list): List of column names for which buckets have been created.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: The DataFrame with similarity calculations for each pair within the same bucket.\n",
        "    \"\"\"\n",
        "    df_groups = None\n",
        "\n",
        "    # Register UDFs for similarity calculations\n",
        "    compare_structural_similarity_udf = F.udf(lambda csr_1, csr_2: compare_weighted_structural_similarity(csr_1, csr_2), DoubleType())\n",
        "    compare_weighted_similarity_udf = F.udf(lambda csr_1, csr_2: compare_weighted_structural_similarity(csr_1, csr_2), DoubleType())\n",
        "\n",
        "    # Create cross join within each bucket combination\n",
        "    bucket_columns = [f\"{col}_bucket\" for col in columns]\n",
        "\n",
        "    # Debug: Check bucket columns in DataFrame\n",
        "    print(\"Columns in DataFrame before cross join:\")\n",
        "    print(df.columns)\n",
        "\n",
        "    for bucket_combination in df.select(bucket_columns).distinct().collect():\n",
        "        # Filter the DataFrame based on the current bucket combination\n",
        "        filter_condition = F.lit(True)\n",
        "        for i, bucket_col in enumerate(bucket_columns):\n",
        "            filter_condition &= (F.col(bucket_col) == getattr(bucket_combination, bucket_col))\n",
        "        bucket_df = df.filter(filter_condition)\n",
        "\n",
        "        # Only proceed if there are at least two communities in the bucket\n",
        "        if bucket_df.count() >= 2:\n",
        "            print(\"This is the bucket df:\")\n",
        "            bucket_df.show(truncate=False)\n",
        "\n",
        "            # Perform a cross join within this bucket\n",
        "            cross_joined = bucket_df.alias(\"a\").crossJoin(bucket_df.alias(\"b\")) \\\n",
        "                .filter(F.col(\"a.community_id\") < F.col(\"b.community_id\")) \\\n",
        "                .select(\n",
        "                    F.col(\"a.community_id\").alias(\"community_id_1\"),\n",
        "                    F.col(\"b.community_id\").alias(\"community_id_2\"),\n",
        "                    compare_structural_similarity_udf(F.col(\"a.csr_matrix\"), F.col(\"b.csr_matrix\")).alias(\"unweighted_similarity_score\"),\n",
        "                    compare_weighted_similarity_udf(F.col(\"a.csr_matrix\"), F.col(\"b.csr_matrix\")).alias(\"weighted_similarity_score\")\n",
        "                )\n",
        "\n",
        "            # Append to df_groups\n",
        "            if df_groups is None:\n",
        "                df_groups = cross_joined\n",
        "            else:\n",
        "                df_groups = df_groups.union(cross_joined)\n",
        "\n",
        "    return df_groups"
      ],
      "metadata": {
        "id": "mhgOioE4ZU5t"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7th cell - driver of the application run all the functions for a given dataset\n",
        "\n",
        "@track_stage(\"Stage 1: Reading the calls dataset\")\n",
        "def read_csv_to_dataframe(file_path= 'toy_dataset.csv'):\n",
        "  \"\"\"\n",
        "  Read dataset from given path into a Spark DataFrame.\n",
        "  Parameters:\n",
        "    -----------\n",
        "    file_path : str\n",
        "        The name of the given dataset (unigrams or bigrams or both).\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    df_dataset : DataFrame\n",
        "        A DataFrame of calls with the given dataset info.\n",
        "  \"\"\"\n",
        "  df_dataset = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "  # convert start - end times to duration\n",
        "  # 1st - Register the UDFs in Spark\n",
        "  calculate_duration_minutes_udf = udf(calculate_duration_minutes, DoubleType())\n",
        "\n",
        "  # 2nd - use udfs to add columns for duration in minutes\n",
        "  df_dataset = df_dataset.withColumn('duration_minutes', calculate_duration_minutes_udf(col('Start_Time'), col('End_Time')))\n",
        "\n",
        "  #3rd - Adjust Client1 and Client2 to ensure Client1 is the smaller value and Client2 the larger\n",
        "  df_dataset = df_dataset.withColumn(\"Client1_min\", least(col(\"Client1\"), col(\"Client2\"))) \\\n",
        "       .withColumn(\"Client2_max\", greatest(col(\"Client1\"), col(\"Client2\"))) \\\n",
        "       .drop(\"Client1\", \"Client2\") \\\n",
        "       .withColumnRenamed(\"Client1_min\", \"Client1\") \\\n",
        "       .withColumnRenamed(\"Client2_max\", \"Client2\")\n",
        "\n",
        "\n",
        "  # 4th - Aggregate total duration for each unique pair (Client1, Client2)\n",
        "  df_aggregated = df_dataset.groupBy(\"Client1\", \"Client2\") \\\n",
        "    .agg(F.sum('duration_minutes').alias(\"total_duration_minutes\"))\n",
        "\n",
        "  # Join the aggregated total duration back to the original DataFrame\n",
        "  df_dataset = df_dataset.drop('duration_minutes') \\\n",
        "        .join(df_aggregated, on=[\"Client1\", \"Client2\"], how=\"left\")\n",
        "\n",
        "\n",
        "  print(\"The following dataframe has been read from the CSV file:\")\n",
        "  df_dataset.show()\n",
        "  return df_dataset\n",
        "\n",
        "@track_stage(\"Stage 2: Preprocessing and creating the graph\")\n",
        "def create_graph_from_dataframe(df_dataset):\n",
        "  \"\"\"\n",
        "  Create graph in GraphFrame from the calls in the current dataset.\n",
        "  Parameters:\n",
        "    -----------\n",
        "    df_dataset : DataFrame\n",
        "        A DataFrame of calls with the given dataset info.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    df_dataset : DataFrame\n",
        "        A DataFrame of calls with the given dataset info.\n",
        "  \"\"\"\n",
        "\n",
        "  # Create Graph using GraphFrames for community detection\n",
        "  vertices = df_dataset.selectExpr(\"Client1 as id\").union(df_dataset.selectExpr(\"Client2 as id\")).distinct()\n",
        "  edges = df_dataset.selectExpr(\"Client1 as src\", \"Client2 as dst\", \"total_duration_minutes as weight\")\n",
        "  # Cache vertices and edges\n",
        "  vertices.cache()\n",
        "  edges.cache()\n",
        "\n",
        "  # Create a GraphFrame\n",
        "  g = GraphFrame(vertices, edges)\n",
        "\n",
        "  # Find connected components (communities) using GraphFrames\n",
        "  connected_components_result = g.connectedComponents()\n",
        "\n",
        "  # Create a mapping from original community IDs to sequential ones\n",
        "  community_mapping = connected_components_result.select(\"component\").distinct() \\\n",
        "      .orderBy(\"component\") \\\n",
        "      .withColumn(\"new_id\", row_number().over(Window.orderBy(\"component\"))) \\\n",
        "      .cache()\n",
        "\n",
        "  # Join the result (community IDs) with the original dataframe and map to new sequential IDs\n",
        "  df_with_communities = df_dataset.join(connected_components_result, df_dataset['Client1'] == connected_components_result['id'], 'inner') \\\n",
        "      .join(community_mapping, connected_components_result['component'] == community_mapping['component'], 'inner') \\\n",
        "      .drop(connected_components_result['id']) \\\n",
        "      .drop(community_mapping['component']) \\\n",
        "      .withColumnRenamed('new_id', 'community_id')\n",
        "\n",
        "  # Calculate the number of unique clients (community size) per community\n",
        "  community_sizes = df_with_communities.select('community_id', 'Client1').union(df_with_communities.select('community_id', 'Client2')) \\\n",
        "      .distinct() \\\n",
        "      .groupBy('community_id').agg(countDistinct('Client1').alias('community_size'))\n",
        "\n",
        "  # Merge the community sizes into the main DataFrame\n",
        "  df_final = df_with_communities.join(community_sizes, 'community_id')\n",
        "\n",
        "  # Get list of tuples for each community member by considering both Client1 and Client2\n",
        "  community_members = df_final.select(\"community_id\", \"Client1\", \"Client2\", \"total_duration_minutes\") \\\n",
        "    .distinct() \\\n",
        "    .orderBy(\"Client1\") \\\n",
        "    .groupBy(\"community_id\") \\\n",
        "    .agg(F.collect_list(F.struct(\n",
        "        F.col(\"Client1\"),\n",
        "        F.col(\"Client2\"),\n",
        "        F.col(\"total_duration_minutes\")\n",
        "    )).alias(\"members\")) \\\n",
        "    .orderBy(\"community_id\")\n",
        "\n",
        "  # Show the final DataFrame with community IDs, duration, and community sizes\n",
        "  print(\"\\nFinal DataFrame with Sequential Community IDs:\")\n",
        "  df_final.select('Client1',\n",
        "                  'Client2',\n",
        "                  'total_duration_minutes',\n",
        "                  'community_id',\n",
        "                  'community_size') \\\n",
        "      .orderBy(\"community_id\") \\\n",
        "      .show()\n",
        "\n",
        "  # Show the list of community members as tuples\n",
        "  print(\"\\nCommunity Members with Sequential IDs:\")\n",
        "  community_members.show(truncate=False)\n",
        "\n",
        "  # Save results to CSV files\n",
        "  # Save the main analysis results\n",
        "  df_final.select('Client1',\n",
        "                  'Client2',\n",
        "                  'total_duration_minutes',\n",
        "                  'community_id',\n",
        "                  'community_size') \\\n",
        "      .orderBy(\"community_id\") \\\n",
        "      .write.mode(\"overwrite\").option(\"header\", \"true\") \\\n",
        "      .csv(f\"{dataset_name}_community_analysis_results\")\n",
        "\n",
        "  # Save community members in a flattened format\n",
        "  df_final.select('community_id',\n",
        "                  'Client1',\n",
        "                  'Client2',\n",
        "                  'total_duration_minutes') \\\n",
        "      .distinct() \\\n",
        "      .orderBy(\"community_id\") \\\n",
        "      .write.mode(\"overwrite\").option(\"header\", \"true\") \\\n",
        "      .csv(f\"{dataset_name}_community_members_results\")\n",
        "\n",
        "  # Optionally, if you want to save additional community statistics\n",
        "  community_stats = df_final.groupBy('community_id') \\\n",
        "      .agg(\n",
        "          countDistinct('Client1', 'Client2').alias('unique_members'),\n",
        "          count('*').alias('total_calls'),\n",
        "          sum('total_duration_minutes').alias('sum_duration_minutes'),\n",
        "          avg('total_duration_minutes').alias('avg_call_duration'),\n",
        "          percentile_approx('total_duration_minutes', 0.25).alias('duration_25th_percentile'),\n",
        "          percentile_approx('total_duration_minutes', 0.5).alias('median_call_duration'),\n",
        "          percentile_approx('total_duration_minutes', 0.75).alias('duration_75th_percentile')\n",
        "      ) \\\n",
        "      .orderBy('community_id')\n",
        "\n",
        "  community_stats.write.mode(\"overwrite\") \\\n",
        "      .option(\"header\", \"true\") \\\n",
        "      .csv(f\"{dataset_name}_community_statistics_results\")\n",
        "\n",
        "  print(\"This is the community stats:\")\n",
        "  community_stats.show(truncate=False)\n",
        "  return df_final, community_members, community_stats\n",
        "\n",
        "# Create CSR adjacency matrices for each community and serialize them\n",
        "@track_stage(\"Stage 3: Creating CSR matrices\")\n",
        "def format_members_to_csr_matrix(community_members, community_stats):\n",
        "  \"\"\"\n",
        "  Create CSR adjacency matrices for each community and serialize them.\n",
        "\n",
        "  Parameters:\n",
        "    community_members: Dataframe\n",
        "    A dataframe of a specific community's members\n",
        "    community_stats: Dataframe\n",
        "    A dataframe of all the communities statistics\n",
        "  \"\"\"\n",
        "  use_weights=True\n",
        "  # Use the function to generate a serialized CSR matrix for each community and show the results\n",
        "  result_true = community_members.groupBy(\"community_id\").apply(create_csr_matrix_from_edges)\n",
        "  print(f\"This is the csr formating results, weight = {use_weights}:\")\n",
        "  result_true.show(truncate=False)\n",
        "  use_weights=False\n",
        "  result_false = community_members.groupBy(\"community_id\").apply(create_csr_matrix_from_edges)\n",
        "  print(f\"This is the csr formating results, weight = {use_weights}:\")\n",
        "  result_false.show(truncate=False)\n",
        "\n",
        "  # Join the community statistics dataframe and the csr_matrix dataframe\n",
        "  # for final analysis\n",
        "  df_community_stats_csr = community_stats.join(df_csr_matrix_result,\n",
        "                                                on='community_id', how='inner')\n",
        "  print(\"This is the statsitcs and csr dataframe joined:\")\n",
        "  df_community_stats_csr.show(truncate=False)\n",
        "\n",
        "  return df_community_stats_csr\n",
        "\n",
        "\n",
        "# Function to create similarity-based subgroups by comparing multiple columns\n",
        "@track_stage(\"Stage 4: Calculate similarities between communities\")\n",
        "def create_similarity_subgroups(df, columns, tolerances):\n",
        "    \"\"\"\n",
        "    Create similarity-based subgroups based on specified columns and tolerances, then apply a custom function.\n",
        "\n",
        "    Parameters:\n",
        "        df (DataFrame): The Spark DataFrame with community data.\n",
        "        columns (list): List of column names to consider for similarity.\n",
        "        tolerances (dict): Dictionary specifying the tolerance (± range) for each column.\n",
        "    \"\"\"\n",
        "    # Collect the DataFrame into a list of rows\n",
        "    communities = df.collect()\n",
        "\n",
        "    # Initialize a list to store similarity groups\n",
        "    similarity_groups = []\n",
        "    df_groups = None\n",
        "\n",
        "    # Compare each pair of communities\n",
        "    for i, j in combinations(range(len(communities)), 2):\n",
        "        community_i = communities[i]\n",
        "        community_j = communities[j]\n",
        "\n",
        "        # Check if the communities are similar based on all specified columns and tolerances\n",
        "        is_similar = all(\n",
        "            abs(community_i[column] - community_j[column]) <= tolerances[column]\n",
        "            for column in columns\n",
        "        )\n",
        "\n",
        "        # If they are similar, add them to the same group\n",
        "        if is_similar:\n",
        "            found_group = False\n",
        "            for group in similarity_groups:\n",
        "                if community_i.community_id in group or community_j.community_id in group:\n",
        "                    group.add(community_i.community_id)\n",
        "                    group.add(community_j.community_id)\n",
        "                    found_group = True\n",
        "                    break\n",
        "            if not found_group:\n",
        "                similarity_groups.append({community_i.community_id, community_j.community_id})\n",
        "\n",
        "    # Create a DataFrame for each subgroup and apply the custom function\n",
        "    for group in similarity_groups:\n",
        "        subgroup_df = df.filter(F.col(\"community_id\").isin(group))\n",
        "        subgroup_cross_joined = calculate_similarities(subgroup_df)\n",
        "        # Initialize or append to df_groups\n",
        "        if df_groups is None:\n",
        "            df_groups = subgroup_cross_joined\n",
        "        else:\n",
        "            df_groups = df_groups.union(subgroup_cross_joined)\n",
        "\n",
        "\n",
        "    # export all found groups\n",
        "    if df_groups:\n",
        "      df_groups.write \\\n",
        "                  .mode(\"overwrite\") \\\n",
        "                  .option(\"header\", header) \\\n",
        "                  .csv(f\"{dataset_name}_df_groups.csv\")\n",
        "    else:\n",
        "      print(\"No groups found!\")\n",
        "    return df_groups\n",
        "\n",
        "def export_similarity_groups(final_similarity_df, df_clients_info):\n",
        "    # Set the similarity threshold\n",
        "    similarity_threshold = 0.55\n",
        "\n",
        "    # Filter similar pairs and define vertices and edges\n",
        "    similar_pairs = final_similarity_df.filter(F.col(\"final_similarity\") >= similarity_threshold)\n",
        "    vertices = similar_pairs.select(\"community_id_1\").union(similar_pairs.select(\"community_id_2\")).distinct() \\\n",
        "        .withColumnRenamed(\"community_id_1\", \"id\")\n",
        "    edges = similar_pairs.select(\n",
        "        F.col(\"community_id_1\").alias(\"src\"),\n",
        "        F.col(\"community_id_2\").alias(\"dst\")\n",
        "    )\n",
        "\n",
        "    # Create the graph and find connected components (clusters of communities)\n",
        "    g = GraphFrame(vertices, edges)\n",
        "    connected_components = g.connectedComponents()\n",
        "\n",
        "    # Group communities by connected component (cluster) and assign group numbers\n",
        "    grouped_communities = connected_components.groupBy(\"component\") \\\n",
        "        .agg(F.collect_list(\"id\").alias(\"community_group\")) \\\n",
        "        .withColumn(\"group_number\", F.row_number().over(Window.orderBy(\"component\")))\n",
        "\n",
        "    # Explode community groups to get individual community IDs with group numbers\n",
        "    exploded_communities = grouped_communities \\\n",
        "        .select(\"group_number\", F.explode(\"community_group\").alias(\"community_id\")) \\\n",
        "        .join(df_clients_info, on=\"community_id\", how=\"left\") \\\n",
        "        .select(\n",
        "            \"group_number\",\n",
        "            \"community_id\",\n",
        "            \"Client1\",\n",
        "            \"Client2\",\n",
        "            \"Start_Time\",\n",
        "            \"End_Time\"\n",
        "        )\n",
        "\n",
        "    # Format the output text lines with group and community information\n",
        "    output_df = exploded_communities \\\n",
        "        .withColumn(\"line\", F.concat_ws(\", \", \"Client1\", \"Client2\", \"Start_Time\", \"End_Time\")) \\\n",
        "        .groupBy(\"group_number\", \"community_id\") \\\n",
        "        .agg(F.collect_list(\"line\").alias(\"community_lines\")) \\\n",
        "        .groupBy(\"group_number\") \\\n",
        "        .agg(\n",
        "            F.concat_ws(\"\\n\",\n",
        "                        F.lit(\"Group \" + F.col(\"group_number\").cast(\"string\") + \":\"),\n",
        "                        F.concat_ws(\"\\n\",\n",
        "                                    F.col(\"community_id\").cast(\"string\"),\n",
        "                                    F.concat_ws(\"\\n\", \"community_lines\")\n",
        "                                    )\n",
        "                        )\n",
        "        )\n",
        "\n",
        "    # Write to text file in distributed manner\n",
        "    output_path = f\"{dataset_name}_similarity_groups.txt\"\n",
        "    output_df.select(\"formatted_output\").write.text(output_path)\n",
        "    print(f\"Exported similarity groups to: {output_path}\")"
      ],
      "metadata": {
        "id": "Ike2lfPxo19c"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8th cell - Read datasets dataframe, and iterate over each one to create communities and form similarity groups\n",
        "# Read the csv\n",
        "df_datasets = pd.read_csv(\"dataset_metadata.csv\")\n",
        "# Print a peak of the dataset\n",
        "print(\"These are the found datasets\")\n",
        "df_datasets.head(10)\n",
        "\n",
        "# set global params\n",
        "clear_csv = False\n",
        "dataset_file_path = \"toy_dataset.csv\"\n",
        "dataset_name = \"toy_dataset\"\n",
        "use_weights = False\n",
        "\n",
        "for i, dataset in df_datasets.iterrows():\n",
        "  print(f\"Starting to process {i+1} dataset with the following params: \\n{dataset}\")\n",
        "  # step 1 - read the dataset\n",
        "  dataset_file_path = dataset[\"csv_filename\"]\n",
        "  # Get the base name from the path (e.g., 'file.txt' from '/path/to/file.txt')\n",
        "  basename =  os.path.basename(dataset_file_path)\n",
        "  # Split the filename and extension\n",
        "  dataset_name = os.path.splitext(basename)[0]\n",
        "  clear_csv = i == 0 # only clear the if this is the 1st dataset\n",
        "  df_dataset = read_csv_to_dataframe(dataset_file_path)\n",
        "\n",
        "  # step 2 - preprocess (convert to duartion in min, create grpah, and find commutnies)\n",
        "  df_final, community_members, community_stats = create_graph_from_dataframe(df_dataset)\n",
        "\n",
        "  # step 3 - create CSR matrix for each communite\n",
        "  df_community_stats_csr = format_members_to_csr_matrix(community_members, community_stats)\n",
        "\n",
        "  # step 4 - calculate similarities between communties for find groups\n",
        "  # Define columns to use for bucketization\n",
        "  columns = ['unique_members', 'total_calls']\n",
        "\n",
        "  # Create adaptive buckets and calculate similarities\n",
        "  # Define columns to use for bucketization\n",
        "  columns = ['unique_members']\n",
        "  df_with_buckets = create_adaptive_buckets(df_community_stats_csr, columns, min_size=2)\n",
        "  df_groups = calculate_similarity_within_buckets(df_with_buckets, columns)"
      ],
      "metadata": {
        "id": "RAMTvsL2v2YH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "18362279-ac6d-49cb-95bf-4e6ae471ab27"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "These are the found datasets\n",
            "Starting to process 1 dataset with the following params: \n",
            "num_communities                                                5\n",
            "community_size_range                                      (3, 5)\n",
            "calls_per_connection_range                                (1, 2)\n",
            "duration_range                                         (30, 120)\n",
            "density                                                      0.5\n",
            "num_samples                                                   50\n",
            "dataset_name                                    dataset_config_1\n",
            "csv_filename                  /content/datasets/dataset_config_1\n",
            "Name: 0, dtype: object\n",
            "Starting Stage 1: Reading the calls dataset\n",
            "The following dataframe has been read from the CSV file:\n",
            "+-------+-------+----------+----------+----------------------+\n",
            "|Client1|Client2|Start_Time|  End_Time|total_duration_minutes|\n",
            "+-------+-------+----------+----------+----------------------+\n",
            "|      0|      1|2401010154|2401010350|                 147.0|\n",
            "|      0|      1|2401011701|2401011732|                 147.0|\n",
            "|      0|      2|2401010704|2401010835|                 166.0|\n",
            "|      0|      2|2401011354|2401011509|                 166.0|\n",
            "|      0|      3|2401012011|2401012159|                 220.0|\n",
            "|      0|      3|2401012213|2401020005|                 220.0|\n",
            "|      0|      4|2401012238|2401020005|                  87.0|\n",
            "|      1|      2|2401010258|2401010428|                  90.0|\n",
            "|      1|      3|2401011214|2401011329|                 187.0|\n",
            "|      1|      3|2401010120|2401010312|                 187.0|\n",
            "|      1|      4|2401010132|2401010303|                 161.0|\n",
            "|      1|      4|2401011254|2401011404|                 161.0|\n",
            "|      2|      3|2401011408|2401011540|                  92.0|\n",
            "|      2|      4|2401011239|2401011435|                 174.0|\n",
            "|      2|      4|2401011656|2401011754|                 174.0|\n",
            "|      3|      4|2401011802|2401011856|                 111.0|\n",
            "|      3|      4|2401012137|2401012234|                 111.0|\n",
            "|   1000|   1001|2401011314|2401011404|                  98.0|\n",
            "|   1000|   1001|2401011703|2401011751|                  98.0|\n",
            "|   1000|   1002|2401010729|2401010903|                 177.0|\n",
            "+-------+-------+----------+----------+----------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Completed Stage 1: Reading the calls dataset\n",
            "\n",
            "+---------+--------+-----------+-------------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+--------------------+----------------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+------------------+-------------------------+-------------------------+--------------------------+---------------------+---------------------+----------------------+----------------------------+-------------------+---------------------+----------------------------------+----------------------------------+\n",
            "|numStages|numTasks|elapsedTime|stageDuration|executorRunTime|executorCpuTime|executorDeserializeTime|executorDeserializeCpuTime|resultSerializationTime|jvmGCTime|shuffleFetchWaitTime|shuffleWriteTime|resultSize|diskBytesSpilled|memoryBytesSpilled|peakExecutionMemory|recordsRead|bytesRead|recordsWritten|bytesWritten|shuffleRecordsRead|shuffleTotalBlocksFetched|shuffleLocalBlocksFetched|shuffleRemoteBlocksFetched|shuffleTotalBytesRead|shuffleLocalBytesRead|shuffleRemoteBytesRead|shuffleRemoteBytesReadToDisk|shuffleBytesWritten|shuffleRecordsWritten|stage_name                        |dataset                           |\n",
            "+---------+--------+-----------+-------------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+--------------------+----------------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+------------------+-------------------------+-------------------------+--------------------------+---------------------+---------------------+----------------------+----------------------------+-------------------+---------------------+----------------------------------+----------------------------------+\n",
            "|5        |5       |686        |343          |298            |95             |12                     |11                        |0                      |0        |0                   |1               |6356      |0               |0                 |67371008           |123        |6220     |0             |0           |31                |1                        |1                        |0                         |924                  |924                  |0                     |0                           |924                |31                   |Stage 1: Reading the calls dataset|/content/datasets/dataset_config_1|\n",
            "+---------+--------+-----------+-------------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+--------------------+----------------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+------------------+-------------------------+-------------------------+--------------------------+---------------------+---------------------+----------------------+----------------------------+-------------------+---------------------+----------------------------------+----------------------------------+\n",
            "\n",
            "Starting Stage 2: Preprocessing and creating the graph\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
            "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final DataFrame with Sequential Community IDs:\n",
            "+-------+-------+----------------------+------------+--------------+\n",
            "|Client1|Client2|total_duration_minutes|community_id|community_size|\n",
            "+-------+-------+----------------------+------------+--------------+\n",
            "|      0|      4|                  87.0|           1|             5|\n",
            "|      0|      3|                 220.0|           1|             5|\n",
            "|      0|      3|                 220.0|           1|             5|\n",
            "|      0|      2|                 166.0|           1|             5|\n",
            "|      0|      2|                 166.0|           1|             5|\n",
            "|      0|      1|                 147.0|           1|             5|\n",
            "|      0|      1|                 147.0|           1|             5|\n",
            "|      3|      4|                 111.0|           1|             5|\n",
            "|      3|      4|                 111.0|           1|             5|\n",
            "|      1|      4|                 161.0|           1|             5|\n",
            "|      1|      4|                 161.0|           1|             5|\n",
            "|      1|      3|                 187.0|           1|             5|\n",
            "|      1|      3|                 187.0|           1|             5|\n",
            "|      1|      2|                  90.0|           1|             5|\n",
            "|      2|      4|                 174.0|           1|             5|\n",
            "|      2|      4|                 174.0|           1|             5|\n",
            "|      2|      3|                  92.0|           1|             5|\n",
            "|   1002|   1003|                 220.0|           2|             4|\n",
            "|   1002|   1003|                 220.0|           2|             4|\n",
            "|   1001|   1003|                  61.0|           2|             4|\n",
            "+-------+-------+----------------------+------------+--------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "\n",
            "Community Members with Sequential IDs:\n",
            "+------------+---------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|community_id|members                                                                                                                                            |\n",
            "+------------+---------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|1           |[{0, 4, 87.0}, {0, 3, 220.0}, {0, 1, 147.0}, {0, 2, 166.0}, {1, 3, 187.0}, {1, 4, 161.0}, {1, 2, 90.0}, {2, 4, 174.0}, {2, 3, 92.0}, {3, 4, 111.0}]|\n",
            "|2           |[{1000, 1001, 98.0}, {1000, 1003, 141.0}, {1000, 1002, 177.0}, {1001, 1003, 61.0}, {1001, 1002, 132.0}, {1002, 1003, 220.0}]                       |\n",
            "|3           |[{2000, 2002, 61.0}, {2000, 2001, 127.0}, {2001, 2002, 135.0}]                                                                                     |\n",
            "|4           |[{3000, 3003, 53.0}, {3000, 3002, 113.0}, {3000, 3001, 201.0}, {3001, 3002, 78.0}, {3001, 3003, 87.0}, {3002, 3003, 131.0}]                        |\n",
            "|5           |[{4000, 4004, 123.0}, {4000, 4002, 98.0}, {4000, 4003, 110.0}, {4000, 4001, 229.0}, {4001, 4002, 120.0}, {4001, 4003, 108.0}]                      |\n",
            "+------------+---------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "This is the community stats:\n",
            "+------------+--------------+-----------+--------------------+------------------+------------------------+--------------------+------------------------+\n",
            "|community_id|unique_members|total_calls|sum_duration_minutes|avg_call_duration |duration_25th_percentile|median_call_duration|duration_75th_percentile|\n",
            "+------------+--------------+-----------+--------------------+------------------+------------------------+--------------------+------------------------+\n",
            "|1           |10            |17         |2601.0              |153.0             |111.0                   |161.0               |174.0                   |\n",
            "|2           |6             |11         |1597.0              |145.1818181818182 |98.0                    |141.0               |177.0                   |\n",
            "|3           |3             |5          |585.0               |117.0             |127.0                   |127.0               |135.0                   |\n",
            "|4           |6             |9          |1108.0              |123.11111111111111|87.0                    |113.0               |131.0                   |\n",
            "|5           |6             |8          |1140.0              |142.5             |108.0                   |120.0               |123.0                   |\n",
            "+------------+--------------+-----------+--------------------+------------------+------------------------+--------------------+------------------------+\n",
            "\n",
            "Completed Stage 2: Preprocessing and creating the graph\n",
            "\n",
            "+---------+--------+-----------+-------------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+--------------------+----------------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+------------------+-------------------------+-------------------------+--------------------------+---------------------+---------------------+----------------------+----------------------------+-------------------+---------------------+---------------------------------------------+----------------------------------+\n",
            "|numStages|numTasks|elapsedTime|stageDuration|executorRunTime|executorCpuTime|executorDeserializeTime|executorDeserializeCpuTime|resultSerializationTime|jvmGCTime|shuffleFetchWaitTime|shuffleWriteTime|resultSize|diskBytesSpilled|memoryBytesSpilled|peakExecutionMemory|recordsRead|bytesRead|recordsWritten|bytesWritten|shuffleRecordsRead|shuffleTotalBlocksFetched|shuffleLocalBlocksFetched|shuffleRemoteBlocksFetched|shuffleTotalBytesRead|shuffleLocalBytesRead|shuffleRemoteBytesRead|shuffleRemoteBytesReadToDisk|shuffleBytesWritten|shuffleRecordsWritten|stage_name                                   |dataset                           |\n",
            "+---------+--------+-----------+-------------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+--------------------+----------------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+------------------+-------------------------+-------------------------+--------------------------+---------------------+---------------------+----------------------+----------------------------+-------------------+---------------------+---------------------------------------------+----------------------------------+\n",
            "|119      |387     |51057      |13255        |9105           |2519           |3576                   |2325                      |69                     |0        |0                   |264             |467975    |0               |0                 |68193280           |1294       |162044   |102           |2722        |919               |228                      |228                      |0                         |38608                |38608                |0                     |0                           |32737              |788                  |Stage 2: Preprocessing and creating the graph|/content/datasets/dataset_config_1|\n",
            "+---------+--------+-----------+-------------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+--------------------+----------------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+------------------+-------------------------+-------------------------+--------------------------+---------------------+---------------------+----------------------+----------------------------+-------------------+---------------------+---------------------------------------------+----------------------------------+\n",
            "\n",
            "Starting Stage 3: Creating CSR matrices\n",
            "This is the csr formating results, weight = True:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/group_ops.py:104: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PythonException",
          "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"<ipython-input-14-593420c939d1>\", line 83, in create_csr_matrix_from_edges\nTypeError: tuple indices must be integers or slices, not str\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-cc4650c7cb07>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0;31m# step 3 - create CSR matrix for each communite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m   \u001b[0mdf_community_stats_csr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_members_to_csr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommunity_members\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommunity_stats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;31m# step 4 - calculate similarities between communties for find groups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-4d68803df0d1>\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mstagemetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Begin collecting metrics for this stage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Run the actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mstagemetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Stop collecting metrics for this stage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-8606507fbdc0>\u001b[0m in \u001b[0;36mformat_members_to_csr_matrix\u001b[0;34m(community_members, community_stats)\u001b[0m\n\u001b[1;32m    178\u001b[0m   \u001b[0mresult_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommunity_members\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"community_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_csr_matrix_from_edges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"This is the csr formating results, weight = {use_weights}:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m   \u001b[0mresult_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m   \u001b[0muse_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m   \u001b[0mresult_false\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommunity_members\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"community_id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_csr_matrix_from_edges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mBob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m         \"\"\"\n\u001b[0;32m--> 947\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     def _show_string(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    976\u001b[0m                 )\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint_truncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"<ipython-input-14-593420c939d1>\", line 83, in create_csr_matrix_from_edges\nTypeError: tuple indices must be integers or slices, not str\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9th cell - Plot the resources usgae of current datasets\n",
        "df_monitor = spark.read.csv(\"stage_metrics\",  header=True)\n",
        "df_monitor = df_monitor.orderBy('stage_name')\n",
        "print(f\"count: {df_monitor.count()}\")\n",
        "df_monitor.show(truncate=False)\n",
        "\n",
        "# Convert the Spark DataFrame to a Pandas DataFrame for plotting\n",
        "pdf_monitor = df_monitor.toPandas()\n",
        "\n",
        "# Select the columns of interest for plotting\n",
        "columns_of_interest = ['stage_name', 'numTasks', 'stageDuration', 'peakExecutionMemory', 'executorCpuTime']\n",
        "pdf_plot = pdf_monitor[columns_of_interest].copy()  # Create a copy to avoid the warning\n",
        "\n",
        "# Format memory and time measurements using .loc\n",
        "pdf_plot.loc[:, 'peakExecutionMemory'] = pdf_plot['peakExecutionMemory'].astype(float) / (1024**3)  # Bytes to GB\n",
        "pdf_plot.loc[:, 'stageDuration'] = pdf_plot['stageDuration'].astype(float) / (1000 * 60)  # ms to minutes\n",
        "pdf_plot.loc[:, 'executorCpuTime'] = pdf_plot['executorCpuTime'].astype(float) / (1000 * 60)  # ms to minutes\n",
        "pdf_plot.loc[:, 'numTasks'] = pdf_plot['numTasks'].astype(int)\n",
        "\n",
        "# Extract stage numbers from stage_name\n",
        "pdf_plot['stage_number'] = pdf_plot['stage_name'].apply(lambda x: int(re.search(r'\\d+', x).group()))\n",
        "\n",
        "# Melt the DataFrame for easier plotting with Seaborn\n",
        "pdf_plot_melted = pd.melt(pdf_plot, id_vars=['stage_name', 'stage_number'], var_name='Metric', value_name='Value')\n",
        "\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "# Create the plot with facets, stage numbers as x-axis, and legend with full stage names\n",
        "g = sns.FacetGrid(pdf_plot_melted, col='Metric',\n",
        "                   height=6, aspect=1.5,\n",
        "                  col_wrap=2, sharey=False, sharex=False)\n",
        "g.map(sns.barplot, 'stage_number', 'Value', palette='hls', hue='stage_name',\n",
        "      data=pdf_plot_melted, dodge=False)  # Pass data argument\n",
        "g.set_xticklabels(pdf_plot['stage_number'].unique(), size=16)\n",
        "g.set_titles(\"{col_name}\", size=18)\n",
        "g.fig.suptitle('Spark Stage Metrics', y=1.02, size=18)\n",
        "g.add_legend(loc='upper right', bbox_to_anchor=(1.2, 0.92))\n",
        "g.legend.set_title('Stage Names', prop={'weight': 'bold', 'size': 22})  # Add title and format it\n",
        "for text in g.legend.get_texts():\n",
        "    text.set_fontsize(18)  # Legend label font size# Set y-axis labels with units and add x-axis label\n",
        "for ax in g.axes.flat:\n",
        "    metric = ax.get_title()\n",
        "    if metric == 'peakExecutionMemory':\n",
        "        ax.set_ylabel('Peak Execution Memory (GB)', size=18)\n",
        "    elif metric in ('stageDuration', 'executorCpuTime'):\n",
        "        ax.set_ylabel('Time (minutes)', size=18)\n",
        "    else:\n",
        "        ax.set_ylabel(metric, size=18)\n",
        "\n",
        "    ax.set_xlabel('Stage Number', size=18)  # Change x-axis label to \"Stage Number\"\n",
        "\n",
        "g.set_yticklabels(fontsize=18)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8_ZLxdC5nDWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 10th cell - kill spark\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "fBqhVzK-Jfgh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}