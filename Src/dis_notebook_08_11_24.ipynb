{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BarGinger/DIS-Assignment/blob/main/Src/dis_notebook_08_11_24.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QFDENKMNs-52",
        "outputId": "fa2520c9-601c-477e-e4be-e7286dc42dec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "The following additional packages will be installed:\n",
            "  libxtst6 openjdk-8-jre-headless\n",
            "Suggested packages:\n",
            "  openjdk-8-demo openjdk-8-source libnss-mdns fonts-dejavu-extra fonts-nanum fonts-ipafont-gothic\n",
            "  fonts-ipafont-mincho fonts-wqy-microhei fonts-wqy-zenhei fonts-indic\n",
            "The following NEW packages will be installed:\n",
            "  libxtst6 openjdk-8-jdk-headless openjdk-8-jre-headless\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 39.6 MB of archives.\n",
            "After this operation, 144 MB of additional disk space will be used.\n",
            "Selecting previously unselected package libxtst6:amd64.\n",
            "(Reading database ... 123623 files and directories currently installed.)\n",
            "Preparing to unpack .../libxtst6_2%3a1.2.3-1build4_amd64.deb ...\n",
            "Unpacking libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Selecting previously unselected package openjdk-8-jre-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jre-headless_8u422-b05-1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jre-headless:amd64 (8u422-b05-1~22.04) ...\n",
            "Selecting previously unselected package openjdk-8-jdk-headless:amd64.\n",
            "Preparing to unpack .../openjdk-8-jdk-headless_8u422-b05-1~22.04_amd64.deb ...\n",
            "Unpacking openjdk-8-jdk-headless:amd64 (8u422-b05-1~22.04) ...\n",
            "Setting up libxtst6:amd64 (2:1.2.3-1build4) ...\n",
            "Setting up openjdk-8-jre-headless:amd64 (8u422-b05-1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/orbd to provide /usr/bin/orbd (orbd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/servertool to provide /usr/bin/servertool (servertool) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/tnameserv to provide /usr/bin/tnameserv (tnameserv) in auto mode\n",
            "Setting up openjdk-8-jdk-headless:amd64 (8u422-b05-1~22.04) ...\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/clhsdb to provide /usr/bin/clhsdb (clhsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/extcheck to provide /usr/bin/extcheck (extcheck) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/hsdb to provide /usr/bin/hsdb (hsdb) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/idlj to provide /usr/bin/idlj (idlj) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/javah to provide /usr/bin/javah (javah) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jhat to provide /usr/bin/jhat (jhat) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/jsadebugd to provide /usr/bin/jsadebugd (jsadebugd) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/native2ascii to provide /usr/bin/native2ascii (native2ascii) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/schemagen to provide /usr/bin/schemagen (schemagen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsgen to provide /usr/bin/wsgen (wsgen) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/wsimport to provide /usr/bin/wsimport (wsimport) in auto mode\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/bin/xjc to provide /usr/bin/xjc (xjc) in auto mode\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "Collecting graphframes\n",
            "  Downloading graphframes-0.6-py2.py3-none-any.whl.metadata (934 bytes)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from graphframes) (1.26.4)\n",
            "Collecting nose (from graphframes)\n",
            "  Downloading nose-1.3.7-py3-none-any.whl.metadata (1.7 kB)\n",
            "Downloading graphframes-0.6-py2.py3-none-any.whl (18 kB)\n",
            "Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nose, graphframes\n",
            "Successfully installed graphframes-0.6 nose-1.3.7\n",
            "Collecting sparkmeasure==0.24\n",
            "  Downloading sparkmeasure-0.24.0-py2.py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading sparkmeasure-0.24.0-py2.py3-none-any.whl (5.8 kB)\n",
            "Installing collected packages: sparkmeasure\n",
            "Successfully installed sparkmeasure-0.24.0\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "# 1st cell - Install requirements\n",
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "!pip install graphframes\n",
        "!pip install sparkmeasure==0.24\n",
        "!pip install matplotlib seaborn\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2nd cell - Import libraries\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession, Row, DataFrame\n",
        "from pyspark.sql.functions import (\n",
        "    col,\n",
        "    udf,\n",
        "    row_number,\n",
        "    countDistinct,\n",
        "    collect_list,\n",
        "    struct,\n",
        "    count,\n",
        "    sum,\n",
        "    avg,\n",
        "    expr,\n",
        "    percentile_approx,\n",
        "    max as spark_max,\n",
        "    explode,\n",
        "    round,\n",
        "    rand,\n",
        "    monotonically_increasing_id,\n",
        "    array,\n",
        "    lit,\n",
        "    broadcast,\n",
        "    lag,\n",
        "    pandas_udf,\n",
        "    PandasUDFType,\n",
        "    least,\n",
        "    greatest\n",
        ")\n",
        "import pyspark.sql.functions as F\n",
        "from sparkmeasure import StageMetrics\n",
        "from pyspark.sql.types import (\n",
        "    StringType, IntegerType, BinaryType, DoubleType,\n",
        "    ArrayType, StructType, StructField, LongType, TimestampType\n",
        ")\n",
        "from pyspark.sql import Window\n",
        "from datetime import datetime, timedelta\n",
        "from graphframes import GraphFrame\n",
        "from scipy.sparse import csr_matrix, vstack, hstack\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import gc\n",
        "import base64\n",
        "from sparkmeasure import StageMetrics # for resources monitoring\n",
        "from functools import wraps\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import random\n",
        "from operator import truediv\n",
        "from google.colab import files\n",
        "from itertools import combinations\n",
        "from scipy.sparse.linalg import inv\n",
        "from scipy.sparse import identity\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import shutil\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "gjrP64v5QyEd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3rd cell - Initialize Spark session\n",
        "spark, stagemetrics = None, None\n",
        "\n",
        "def create_spark():\n",
        "  spark = SparkSession.builder \\\n",
        "  .appName(\"PhoneCallsCommunityDetection\") \\\n",
        "  .master(\"local[*]\") \\\n",
        "  .config(\"spark.jars.packages\", \"ch.cern.sparkmeasure:spark-measure_2.12:0.24,graphframes:graphframes:0.8.2-spark3.1-s_2.12\") \\\n",
        "  .config(\"spark.executor.memory\", \"8g\") \\\n",
        "  .config(\"spark.driver.memory\", \"8g\") \\\n",
        "  .config(\"spark.executor.memoryOverhead\", \"1G\") \\\n",
        "  .config(\"spark.default.parallelism\", \"100\") \\\n",
        "  .config(\"spark.sql.shuffle.partitions\", \"5\") \\\n",
        "  .config(\"spark.driver.maxResultSize\", \"2G\") \\\n",
        "  .getOrCreate()\n",
        "\n",
        "  # Initialize StageMetrics\n",
        "  stagemetrics = StageMetrics(spark)\n",
        "\n",
        "  # Optional: Set logging level to reduce verbosity\n",
        "  spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "  # Set a checkpoint directory for Spark\n",
        "  spark.sparkContext.setCheckpointDir(\"/tmp/spark-checkpoints\")\n",
        "  return spark, stagemetrics\n",
        "\n",
        "def kill_spark(spark):\n",
        "  spark.stop()\n",
        "\n",
        "spark, stagemetrics = create_spark()"
      ],
      "metadata": {
        "id": "xSdU6FWDinu3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4th cell - Generate datasets - PLEASE only run this if datasets folder is empty / does not exists\n",
        "def generate_communities(spark, num_communities, community_size_range, density=0.3, extra_factor=1.5):\n",
        "    \"\"\"\n",
        "    Generate isolated communities with controlled sizes and connections.\n",
        "    Ensures enough connections for the sample count by using an extra factor.\n",
        "    \"\"\"\n",
        "    communities = []\n",
        "    for community_id in range(num_communities):\n",
        "        size = random.randint(community_size_range[0], community_size_range[1])\n",
        "        base_id = community_id * 1000\n",
        "        community_clients = [(community_id, base_id + i, base_id + j)\n",
        "                             for i in range(size) for j in range(i + 1, size) if random.random() < density * extra_factor]\n",
        "        communities.extend(community_clients)\n",
        "\n",
        "    return spark.createDataFrame(communities, [\"community_id\", \"client1\", \"client2\"])\n",
        "\n",
        "def generate_call_times(communities_df, calls_per_connection_range, duration_range, base_time, num_samples):\n",
        "    \"\"\"\n",
        "    Generate call start and end times for each client connection, ensuring total number of samples matches `num_samples`.\n",
        "    \"\"\"\n",
        "    calls_df = communities_df.withColumn(\n",
        "        \"num_calls\",\n",
        "        F.expr(f\"floor(rand() * ({calls_per_connection_range[1]} - {calls_per_connection_range[0]} + 1)) + {calls_per_connection_range[0]}\")\n",
        "    ).withColumn(\n",
        "        \"call_id\", F.monotonically_increasing_id()\n",
        "    ).withColumn(\n",
        "        \"calls\", F.expr(\"sequence(1, num_calls)\")\n",
        "    ).select(\"client1\", \"client2\", \"call_id\", F.explode(\"calls\").alias(\"call_num\"))\n",
        "\n",
        "    def generate_times():\n",
        "        start_time = base_time + timedelta(minutes=random.randint(0, 1440))\n",
        "        duration = random.randint(duration_range[0], duration_range[1])\n",
        "        end_time = start_time + timedelta(minutes=duration)\n",
        "        return start_time.strftime('%y%m%d%H%M'), end_time.strftime('%y%m%d%H%M')\n",
        "\n",
        "    time_udf = F.udf(lambda: generate_times(), \"struct<Start_Time:string, End_Time:string>\")\n",
        "    calls_df = calls_df.withColumn(\"call_times\", time_udf())\n",
        "\n",
        "    # Ensure consistent schema for the final DataFrame\n",
        "    calls_df = calls_df.select(\n",
        "        \"client1\", \"client2\", calls_df[\"call_times.Start_Time\"].alias(\"Start_Time\"), calls_df[\"call_times.End_Time\"].alias(\"End_Time\")\n",
        "    )\n",
        "\n",
        "    # Limit to the specified number of samples\n",
        "    final_calls_df = calls_df.limit(num_samples)\n",
        "\n",
        "    # Retry generation if the sample count isn't met\n",
        "    while final_calls_df.count() < num_samples:\n",
        "        additional_df = calls_df.limit(num_samples - final_calls_df.count()).select(\"client1\", \"client2\", \"Start_Time\", \"End_Time\")\n",
        "        final_calls_df = final_calls_df.union(additional_df).limit(num_samples)\n",
        "\n",
        "    return final_calls_df\n",
        "\n",
        "# Function to delete all generated datasets\n",
        "def delete_generated_datasets():\n",
        "    folder_path = \"/content/datasets/\"\n",
        "    deleted_files = []\n",
        "\n",
        "    if os.path.exists(folder_path):\n",
        "        # Loop through each item in the folder\n",
        "        for item in os.listdir(folder_path):\n",
        "            item_path = os.path.join(folder_path, item)\n",
        "            # Check if it's a directory and remove it with shutil.rmtree\n",
        "            if os.path.isdir(item_path):\n",
        "                shutil.rmtree(item_path)\n",
        "            else:\n",
        "                os.remove(item_path)\n",
        "            deleted_files.append(item)\n",
        "\n",
        "        # Print the results\n",
        "        if deleted_files:\n",
        "            print(\"Deleted the following items:\")\n",
        "            for item in deleted_files:\n",
        "                print(item)\n",
        "        else:\n",
        "            print(\"No files found in the folder to delete.\")\n",
        "    else:\n",
        "        print(\"The folder does not exist.\")\n",
        "\n",
        "def save_dataset(dataset, filename):\n",
        "    \"\"\"\n",
        "    Save the generated dataset to a temporary directory and then move to the final directory.\n",
        "\n",
        "    Parameters:\n",
        "        dataset (DataFrame): The DataFrame to save, containing generated call data.\n",
        "        filename (str): Base name for the dataset file.\n",
        "    \"\"\"\n",
        "    # Define the directories\n",
        "    final_dir = \"/content/datasets\"\n",
        "    temp_dir = f\"{final_dir}/{filename}_temp\"\n",
        "    final_path = os.path.join(final_dir, filename)\n",
        "\n",
        "    # Create directories if they don't exist\n",
        "    os.makedirs(final_dir, exist_ok=True)\n",
        "\n",
        "    # Write to the temporary directory\n",
        "    dataset.write.mode(\"overwrite\").option(\"header\", \"true\").csv(temp_dir)\n",
        "\n",
        "    # Move the content to the final directory\n",
        "    if os.path.exists(final_path):\n",
        "        shutil.rmtree(final_path)\n",
        "    shutil.move(temp_dir, final_path)\n",
        "\n",
        "    # Clean up by removing the temporary directory\n",
        "    if os.path.exists(temp_dir):\n",
        "        shutil.rmtree(temp_dir)\n",
        "\n",
        "    print(f\"Dataset saved as {final_path}\")\n",
        "    # files.download(final_path)\n",
        "    return final_path\n",
        "\n",
        "\n",
        "def export_datasets_to_computer(folder_path=\"/content/datasets\"):\n",
        "    \"\"\"\n",
        "    Compresses and exports each dataset in the specified folder to download to the local computer.\n",
        "\n",
        "    Parameters:\n",
        "        folder_path (str): The path to the folder containing datasets.\n",
        "    \"\"\"\n",
        "    # Check if the folder exists\n",
        "    if not os.path.exists(folder_path):\n",
        "        print(f\"The folder '{folder_path}' does not exist.\")\n",
        "        return\n",
        "\n",
        "    # Iterate over each file/directory in the folder\n",
        "    for item in os.listdir(folder_path):\n",
        "        item_path = os.path.join(folder_path, item)\n",
        "\n",
        "        # If it's a directory (dataset in CSV format is usually saved as a directory)\n",
        "        if os.path.isdir(item_path):\n",
        "            # Create a zip file of the dataset directory\n",
        "            zip_filename = f\"{item}.zip\"\n",
        "            shutil.make_archive(item_path, 'zip', item_path)\n",
        "            print(f\"Compressed '{item}' as '{zip_filename}'.\")\n",
        "\n",
        "            # Download the zip file\n",
        "            files.download(f\"{item_path}.zip\")\n",
        "        else:\n",
        "            # Download individual files if they are directly in the folder\n",
        "            files.download(item_path)\n",
        "\n",
        "    print(\"All datasets have been exported to your computer.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    delete_generated_datasets()\n",
        "    # spark = create_spark_session()\n",
        "\n",
        "    # Define parameter configurations with num_samples\n",
        "    parameter_sets = [\n",
        "        {\"num_communities\": 5, \"community_size_range\": (3, 5), \"calls_per_connection_range\": (1, 2), \"duration_range\": (30, 120), \"density\": 0.5, \"num_samples\": 50},\n",
        "        {\"num_communities\": 10, \"community_size_range\": (5, 7), \"calls_per_connection_range\": (1, 3), \"duration_range\": (30, 180), \"density\": 0.4, \"num_samples\": 100},\n",
        "        {\"num_communities\": 100, \"community_size_range\": (5, 100), \"calls_per_connection_range\": (1, 5), \"duration_range\": (30, 380), \"density\": 0.4, \"num_samples\": 5000},\n",
        "        {\"num_communities\": 5000, \"community_size_range\": (5, 100), \"calls_per_connection_range\": (1, 5), \"duration_range\": (30, 380), \"density\": 0.4, \"num_samples\": 50000}\n",
        "    ]\n",
        "\n",
        "    base_time = datetime(2024, 1, 1)\n",
        "\n",
        "    for i, params in enumerate(parameter_sets):\n",
        "        print(f\"\\nGenerating dataset for configuration {i + 1}: {params}\")\n",
        "\n",
        "        # Generate communities and call times\n",
        "        communities_df = generate_communities(\n",
        "            spark,\n",
        "            num_communities=params[\"num_communities\"],\n",
        "            community_size_range=params[\"community_size_range\"],\n",
        "            density=params[\"density\"],\n",
        "            extra_factor=2  # Generate more potential connections initially\n",
        "        )\n",
        "        calls_df = generate_call_times(\n",
        "            communities_df,\n",
        "            calls_per_connection_range=params[\"calls_per_connection_range\"],\n",
        "            duration_range=params[\"duration_range\"],\n",
        "            base_time=base_time,\n",
        "            num_samples=params[\"num_samples\"]\n",
        "        )\n",
        "\n",
        "        # Save dataset and print information\n",
        "        filename = f\"dataset_config_{i + 1}\"\n",
        "        final_path = save_dataset(calls_df, filename)\n",
        "        parameter_sets[i]['dataset_name'] = filename\n",
        "        parameter_sets[i]['csv_filename'] = final_path\n",
        "\n",
        "    df_datasets = pd.DataFrame(parameter_sets)\n",
        "    dataset_metadata_file_path = \"dataset_metadata.csv\"\n",
        "    df_datasets.to_csv(dataset_metadata_file_path, index=False)\n",
        "    files.download(dataset_metadata_file_path)\n",
        "    # Run the function to export all datasets\n",
        "    export_datasets_to_computer(\"/content/datasets\")"
      ],
      "metadata": {
        "id": "WJPkATltavF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5th cell - Uplode datasets into collab\n",
        "\n",
        "def import_datasets_from_computer(folder_path=\"/content/datasets\"):\n",
        "    \"\"\"\n",
        "    Uploads zip files from the local computer and unzips each into the specified folder on Colab.\n",
        "\n",
        "    Parameters:\n",
        "        folder_path (str): The path to the folder where datasets will be saved after unzipping.\n",
        "    \"\"\"\n",
        "    # Ensure the target folder exists\n",
        "    os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "    # Upload zip files from the local computer\n",
        "    uploaded_files = files.upload()\n",
        "\n",
        "    # Process each uploaded zip file\n",
        "    for filename in uploaded_files.keys():\n",
        "        if filename.endswith(\".zip\"):\n",
        "            zip_path = os.path.join(folder_path, filename)\n",
        "\n",
        "            # Move uploaded zip file to the target folder\n",
        "            os.rename(filename, zip_path)\n",
        "\n",
        "            # Unzip the file into a subdirectory named after the zip file (without .zip extension)\n",
        "            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "                extract_dir = os.path.join(folder_path, filename.replace(\".zip\", \"\"))\n",
        "                os.makedirs(extract_dir, exist_ok=True)\n",
        "                zip_ref.extractall(extract_dir)\n",
        "\n",
        "            print(f\"Imported and unzipped '{filename}' into '{extract_dir}'.\")\n",
        "        else:\n",
        "            print(f\"Skipped non-zip file '{filename}'.\")\n",
        "\n",
        "    print(\"All datasets have been imported and unzipped.\")\n",
        "\n",
        "import_datasets_from_computer()"
      ],
      "metadata": {
        "id": "ngGMcOzACF35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "outputId": "f8d3c948-4af9-4e65-c638-658d89bfc673"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d5e2062c-3a82-45ab-b043-99dbc79979f7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d5e2062c-3a82-45ab-b043-99dbc79979f7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving dataset_config_1.zip to dataset_config_1.zip\n",
            "Saving dataset_config_2.zip to dataset_config_2.zip\n",
            "Saving dataset_config_3.zip to dataset_config_3.zip\n",
            "Saving dataset_config_4.zip to dataset_config_4.zip\n",
            "Imported and unzipped 'dataset_config_1.zip' into '/content/datasets/dataset_config_1'.\n",
            "Imported and unzipped 'dataset_config_2.zip' into '/content/datasets/dataset_config_2'.\n",
            "Imported and unzipped 'dataset_config_3.zip' into '/content/datasets/dataset_config_3'.\n",
            "Imported and unzipped 'dataset_config_4.zip' into '/content/datasets/dataset_config_4'.\n",
            "All datasets have been imported and unzipped.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6th cell - Initialize resource monitoring\n",
        "\n",
        "# Monitor CPU, Memory and running time\n",
        "def track_stage(stage_name):\n",
        "    def decorator(func):\n",
        "        @wraps(func)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            print(f\"Starting {stage_name}\")\n",
        "            stagemetrics.begin()  # Begin collecting metrics for this stage\n",
        "\n",
        "            result = func(*args, **kwargs)  # Run the actual function\n",
        "\n",
        "            stagemetrics.end()  # Stop collecting metrics for this stage\n",
        "\n",
        "            time.sleep(15)\n",
        "\n",
        "            # Generate metrics DataFrame\n",
        "            print(f\"Completed {stage_name}\\n\")\n",
        "            df_metrics_all = stagemetrics.create_stagemetrics_DF()\n",
        "            df_metrics_agg = stagemetrics.aggregate_stagemetrics_DF()\n",
        "            # Add stage_name column and join metrics and memory DataFrames\n",
        "            df_metrics_agg = df_metrics_agg.withColumn(\"stage_name\", pyspark.sql.functions.lit(stage_name))\n",
        "            df_metrics_agg = df_metrics_agg.withColumn(\"dataset\", pyspark.sql.functions.lit(dataset_file_path))\n",
        "            df_metrics_agg.show(truncate=False)\n",
        "\n",
        "            # Set write mode based on the stage\n",
        "            if \"Stage 1\" in stage_name and clear_csv:\n",
        "                write_mode = \"overwrite\"\n",
        "                header = \"true\"\n",
        "            else:\n",
        "                write_mode = \"append\"\n",
        "                header = \"true\"\n",
        "\n",
        "            # Write metrics to CSV with appropriate mode and header settings\n",
        "            df_metrics_agg.coalesce(1).write \\\n",
        "                .mode(write_mode) \\\n",
        "                .option(\"header\", header) \\\n",
        "                .csv(f\"{dataset_name}_stage_metrics\")\n",
        "\n",
        "            return result\n",
        "        return wrapper\n",
        "    return decorator"
      ],
      "metadata": {
        "id": "R9jMJMWzY2LP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7th cell - All the utilities functions for the project\n",
        "\n",
        "# Convert YYMMDDHHMM to a proper datetime object\n",
        "def calculate_duration_minutes(start_time, end_time):\n",
        "  \"\"\"\n",
        "  Calculate the duration between two times in minutes.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  start_time : str\n",
        "      The start time in HH:MM:SS format.\n",
        "  end_time : str\n",
        "      The end time in HH:MM:SS format.\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  duration_minutes : float\n",
        "      The duration between start_time and end_time in minutes.\n",
        "  \"\"\"\n",
        "  start_datetime = convert_to_datetime(start_time)\n",
        "  end_datetime = convert_to_datetime(end_time)\n",
        "  duration = end_datetime - start_datetime\n",
        "  duration_minutes = duration.total_seconds() / 60\n",
        "  return duration_minutes\n",
        "\n",
        "def convert_to_datetime(time_str):\n",
        "  \"\"\"\n",
        "  Convert a time string in '%y%m%d%H%M' format to a datetime object.\n",
        "\n",
        "  Parameters:\n",
        "  -----------\n",
        "  time_str : str\n",
        "      The time string in '%y%m%d%H%M' format.\n",
        "\n",
        "  Returns:\n",
        "  --------\n",
        "  datetime_obj : datetime.datetime\n",
        "      The datetime object representing the given time string.\n",
        "  \"\"\"\n",
        "  # Use datetime.datetime.strptime to parse the time string\n",
        "  # This is the correct way to use strptime, avoiding the AttributeError\n",
        "  return datetime.strptime(str(time_str), '%y%m%d%H%M')\n",
        "\n",
        "# Define UDF for calculating duration in DDHHMM format\n",
        "def calculate_duration_string(start_time, end_time):\n",
        "    start_dt = convert_to_datetime(start_time)\n",
        "    end_dt = convert_to_datetime(end_time)\n",
        "    duration = end_dt - start_dt\n",
        "\n",
        "    days = duration.days\n",
        "    hours, remainder = divmod(duration.seconds, 3600)\n",
        "    minutes = remainder // 60\n",
        "    return f'{days:02d}{hours:02d}{minutes:02d}'\n",
        "\n",
        "\n",
        "'''Decorator and Function Definition:\n",
        "The @pandas_udf decorator marks this function as a Pandas UDF (User Defined Function) that will be applied on grouped data.\n",
        "GROUPED_MAP tells Spark that the function will receive a DataFrame for each group (grouped by community_id).\n",
        "The schema defines the expected output structure of the function, which is a DataFrame with community_id\n",
        "and a binary field containing the serialized matrix.\n",
        "The function converts the connections (edges) between clients into a CSR matrix and serializes it for storage.'''\n",
        "\n",
        "\n",
        "# Define the schema for the Pandas UDF output\n",
        "schema = StructType([\n",
        "    StructField(\"community_id\", IntegerType(), True),\n",
        "    StructField(\"csr_matrix\", BinaryType(), True)\n",
        "])\n",
        "\n",
        "@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
        "def create_csr_matrix_from_edges(members_df):\n",
        "    \"\"\"\n",
        "    Creates a serialized CSR matrix from a Spark DataFrame for each community.\n",
        "\n",
        "    Args:\n",
        "        members_df (DataFrame): Spark DataFrame with 'community_id' and 'members' columns.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: A DataFrame with 'community_id' and a serialized CSR matrix as binary data.\n",
        "    \"\"\"\n",
        "    # try:\n",
        "    # Extract the community ID (assuming it's consistent within the group)\n",
        "    # members_df.show(truncate=False)\n",
        "    community_id = members_df['community_id'].iloc[0]\n",
        "\n",
        "    '''Since each members_df contains data for a single community (due to groupBy operation),\n",
        "    the function retrieves the community_id from the first row.\n",
        "    This ID will be included in the output so that each serialized CSR matrix can be linked back\n",
        "    to its respective community.'''\n",
        "    # Explode the members array to get each connection in separate rows\n",
        "    exploded_df = members_df.explode(\"members\").dropna().reset_index(drop=True)\n",
        "    exploded_df = pd.DataFrame({\n",
        "        'Client1': exploded_df['members'].apply(lambda x: x['Client1']),\n",
        "        'Client2': exploded_df['members'].apply(lambda x: x['Client2']),\n",
        "        'total_duration_minutes': exploded_df['members'].apply(lambda x: x['total_duration_minutes'])\n",
        "    })\n",
        "    '''Flattening and Extracting Connection Data:\n",
        "    The members_df contains a column with a list of connections (pairs of clients and call durations).\n",
        "    The function uses explode to convert this list into individual rows, making it easier to work with each connection.\n",
        "    It then creates a new DataFrame, exploded_df, with separate columns for Client1, Client2, and duration_minutes\n",
        "    extracted from the connection data.\n",
        "    This simplifies further processing by ensuring each row represents a single call between two clients.'''\n",
        "    # Get unique clients and create a mapping to indices\n",
        "    unique_clients = sorted(pd.concat([exploded_df['Client1'], exploded_df['Client2']]).unique())\n",
        "    client_to_index = {client: i for i, client in enumerate(unique_clients)}\n",
        "    num_clients = len(unique_clients)\n",
        "\n",
        "    # Extract data for CSR matrix\n",
        "    rows = exploded_df['Client1'].map(client_to_index).values\n",
        "    cols = exploded_df['Client2'].map(client_to_index).values\n",
        "    if use_weights:\n",
        "      data = exploded_df['total_duration_minutes'].values #if weight else [1] * len(rows)\n",
        "    else:\n",
        "      data = [1] * len(rows)\n",
        "\n",
        "    # Create CSR matrix\n",
        "    csr = csr_matrix((data, (rows, cols)), shape=(num_clients, num_clients))\n",
        "    '''Serializing the CSR Matrix: The function uses Python’s pickle module to serialize the CSR matrix.\n",
        "    This converts the matrix into a binary format, allowing it to be stored or transferred efficiently.\n",
        "    Serialization is necessary because Spark DataFrames cannot directly store complex Python objects like CSR matrices.'''\n",
        "    # Serialize CSR matrix to binary format\n",
        "    serialized_csr = pickle.dumps(csr)\n",
        "\n",
        "    # Return as DataFrame\n",
        "    return pd.DataFrame({\"community_id\": [community_id], \"csr_matrix\": [serialized_csr]})\n",
        "\n",
        "\n",
        "# prompt: print csr_matrix_result pretty\n",
        "def pretty_print_csr_matrix(csr_matrix_result):\n",
        "  \"\"\"Prints a CSR matrix in a readable format.\"\"\"\n",
        "\n",
        "  rows, cols = csr_matrix_result.nonzero()\n",
        "  data = csr_matrix_result.data\n",
        "\n",
        "  df = pd.DataFrame({\n",
        "      'Row': rows,\n",
        "      'Col': cols,\n",
        "      'Value': data\n",
        "  })\n",
        "\n",
        "  print(df)\n",
        "\n",
        "# Padding and calculating DeltaCon similarity\n",
        "def pad_csr_matrix(csr, max_shape):\n",
        "    current_rows, current_cols = csr.shape\n",
        "    max_rows, max_cols = max_shape\n",
        "    if current_rows < max_rows:\n",
        "        additional_rows = csr_matrix((max_rows - current_rows, current_cols))\n",
        "        csr = vstack([csr, additional_rows])\n",
        "    if current_cols < max_cols:\n",
        "        additional_cols = csr_matrix((csr.shape[0], max_cols - current_cols))\n",
        "        csr = hstack([csr, additional_cols])\n",
        "    return csr\n",
        "\n",
        "# Pad CSR matrices and calculate DeltaCon similarity using Spark DataFrame operations\n",
        "def process_csr_matrices(df, max_size):\n",
        "    def pad_and_calculate(row):\n",
        "        csr_matrix_padded = pad_csr_matrix(pickle.loads(row['csr_matrix']), max_size)\n",
        "        serialized_csr = pickle.dumps(csr_matrix_padded)\n",
        "        return (row['community_id'], serialized_csr)\n",
        "\n",
        "    return df.rdd.map(pad_and_calculate).toDF([\"community_id\", \"csr_matrix\"])\n",
        "\n",
        "def normalize_matrix(matrix):\n",
        "    \"\"\"\n",
        "    Normalize the matrix values to the range [0, 1].\n",
        "\n",
        "    Parameters:\n",
        "    matrix : csr_matrix\n",
        "        Sparse matrix to normalize.\n",
        "\n",
        "    Returns:\n",
        "    csr_matrix\n",
        "        Normalized sparse matrix.\n",
        "    \"\"\"\n",
        "    data = matrix.data\n",
        "    if len(data) == 0:  # Handle empty matrices\n",
        "        return matrix\n",
        "    min_val = np.min(data)\n",
        "    max_val = np.max(data)\n",
        "    normalized_data = (data - min_val) / (max_val - min_val) if max_val > min_val else data\n",
        "    return matrix.__class__((normalized_data, matrix.indices, matrix.indptr), shape=matrix.shape)\n",
        "\n",
        "def frobenius_norm(csr_1, csr_2):\n",
        "    \"\"\"\n",
        "    Compute Frobenius norm between two sparse matrices.\n",
        "\n",
        "    Parameters:\n",
        "    csr_1, csr_2 : csr_matrix\n",
        "        Sparse adjacency matrices of the graphs.\n",
        "\n",
        "    Returns:\n",
        "    float\n",
        "        Frobenius norm distance between the graphs.\n",
        "    \"\"\"\n",
        "    # csr_1 = log_transform_matrix(csr_1)\n",
        "    # csr_2 = log_transform_matrix(csr_2)\n",
        "    csr_1 = normalize_matrix(csr_1)\n",
        "    csr_2 = normalize_matrix(csr_2)\n",
        "    assert csr_1.shape == csr_2.shape, \"Adjacency matrices must have the same dimensions.\"\n",
        "    diff = csr_1 - csr_2\n",
        "    return np.sqrt((diff.power(2)).sum())\n",
        "\n",
        "def frobenius_sim(csr_1, csr_2):\n",
        "    \"\"\"\n",
        "    Adds a similarity column to the DataFrame based on Frobenius distance.\n",
        "\n",
        "    Parameters:\n",
        "    df (DataFrame): Input DataFrame containing 'frobenius_distance' column.\n",
        "\n",
        "    Returns:\n",
        "    DataFrame: A DataFrame with an additional 'similarity' column.\n",
        "    \"\"\"\n",
        "    dist=frobenius_norm(csr_1, csr_2)\n",
        "    return 1 / (1 + dist)\n",
        "\n",
        "\n",
        "def deltacon_similarity(csr_1, csr_2, epsilon=0.5):\n",
        "    # Ensure both matrices are of the same size\n",
        "    assert csr_1.shape == csr_2.shape, \"Adjacency matrices must be of the same size for comparison.\"\n",
        "    I = identity(csr_1.shape[0])\n",
        "    D1 = csr_1.sum(axis=1).A.flatten()\n",
        "    D1 = csr_matrix((D1, (range(csr_1.shape[0]), range(csr_1.shape[0]))))\n",
        "    D2 = csr_2.sum(axis=1).A.flatten()\n",
        "    D2 = csr_matrix((D2, (range(csr_2.shape[0]), range(csr_2.shape[0]))))\n",
        "\n",
        "    S1 = inv(I + epsilon**2 * D1 - epsilon * csr_1)\n",
        "    S2 = inv(I + epsilon**2 * D2 - epsilon * csr_2)\n",
        "    frobenius_norm = np.sqrt(((S1 - S2).power(2)).sum())\n",
        "    return 1 / (1 + frobenius_norm)\n",
        "\n",
        "# Define function to calculate Frobenius similarity\n",
        "def calculate_frobenius_similarity(grouped_df):\n",
        "    community_id_1 = grouped_df.iloc[0]['community_id']\n",
        "    community_id_2 = grouped_df.iloc[0]['community_id_2']\n",
        "    csr_1 = pickle.loads(grouped_df.iloc[0]['csr_matrix'])\n",
        "    csr_2 = pickle.loads(grouped_df.iloc[0]['csr_matrix_2'])\n",
        "    similarity_score_f = frobenius_sim(csr_1, csr_2)\n",
        "    return pd.DataFrame([{\n",
        "        \"community_id_1\": community_id_1,\n",
        "        \"community_id_2\": community_id_2,\n",
        "        \"frobenius_similarity\": similarity_score_f\n",
        "    }])\n",
        "\n",
        "# Define function to calculate DeltaCon similarity\n",
        "def calculate_deltacon_similarity(grouped_df):\n",
        "    community_id_1 = grouped_df.iloc[0]['community_id']\n",
        "    community_id_2 = grouped_df.iloc[0]['community_id_2']\n",
        "    csr_1 = pickle.loads(grouped_df.iloc[0]['csr_matrix'])\n",
        "    csr_2 = pickle.loads(grouped_df.iloc[0]['csr_matrix_2'])\n",
        "    similarity_score_d = deltacon_similarity(csr_1, csr_2)\n",
        "    return pd.DataFrame([{\n",
        "        \"community_id_1\": community_id_1,\n",
        "        \"community_id_2\": community_id_2,\n",
        "        \"deltacon\": similarity_score_d\n",
        "    }])\n",
        "\n",
        "# Comparison function for structural and weight-based similarities\n",
        "def cosine_sim(csr_1, csr_2):\n",
        "    # Compute cosine similarity\n",
        "    cosine_sim = cosine_similarity(csr_1, csr_2)\n",
        "    return cosine_sim\n",
        "\n",
        "\n",
        "def calculate_similarities(subgroup_community_members):\n",
        "  \"\"\"\n",
        "  Comparing CSR matrices to detect similarity\n",
        "  \"\"\"\n",
        "\n",
        "  max_size = subgroup_community_members.rdd.map(lambda row: pickle.loads(row['csr_matrix']).shape).reduce(lambda x, y: (max(x[0], y[0]), max(x[1], y[1])))\n",
        "\n",
        "  padded_result_true = process_csr_matrices(subgroup_community_members, max_size)\n",
        "  padded_result_false = process_csr_matrices(subgroup_community_members, max_size)\n",
        "\n",
        "  # Step 1: Compute Frobenius Similarity (using padded_result_true)\n",
        "\n",
        "  # Rename columns from df2 to remove ambiguity for Frobenius similarity calculation\n",
        "  padded_result_true_renamed = padded_result_true.select(\n",
        "      col(\"community_id\").alias(\"community_id_2\"),\n",
        "      col(\"csr_matrix\").alias(\"csr_matrix_2\")\n",
        "  )\n",
        "\n",
        "  # Cross join to compare every community for Frobenius similarity\n",
        "  cross_joined_df_frobenius = padded_result_true.alias(\"df1\").crossJoin(padded_result_true_renamed.alias(\"df2\")) \\\n",
        "      .filter(col(\"df1.community_id\") < col(\"df2.community_id_2\"))\n",
        "\n",
        "  # Define schema for Frobenius similarity output\n",
        "  frobenius_similarity_schema = StructType([\n",
        "      StructField(\"community_id_1\", IntegerType(), True),\n",
        "      StructField(\"community_id_2\", IntegerType(), True),\n",
        "      StructField(\"frobenius_similarity\", DoubleType(), True)\n",
        "  ])\n",
        "\n",
        "  # Apply Frobenius similarity calculation\n",
        "  frobenius_similarity_df = cross_joined_df_frobenius.select(\n",
        "      \"df1.community_id\", \"df2.community_id_2\", \"df1.csr_matrix\", \"df2.csr_matrix_2\"\n",
        "  ).groupBy(\"community_id\", \"community_id_2\") \\\n",
        "      .applyInPandas(calculate_frobenius_similarity, schema=frobenius_similarity_schema)\n",
        "\n",
        "  #Step 2: Compute DeltaCon Similarity (using padded_result_false)\n",
        "\n",
        "  # Rename columns from df2 to remove ambiguity for DeltaCon similarity calculation\n",
        "  padded_result_false_renamed = padded_result_false.select(\n",
        "      col(\"community_id\").alias(\"community_id_2\"),\n",
        "      col(\"csr_matrix\").alias(\"csr_matrix_2\")\n",
        "  )\n",
        "\n",
        "  # Cross join to compare every community for DeltaCon similarity\n",
        "  cross_joined_df_deltacon = padded_result_false.alias(\"df1\").crossJoin(padded_result_false_renamed.alias(\"df2\")) \\\n",
        "      .filter(col(\"df1.community_id\") < col(\"df2.community_id_2\"))\n",
        "\n",
        "  # Define schema for DeltaCon similarity output\n",
        "  deltacon_similarity_schema = StructType([\n",
        "      StructField(\"community_id_1\", IntegerType(), True),\n",
        "      StructField(\"community_id_2\", IntegerType(), True),\n",
        "      StructField(\"deltacon\", DoubleType(), True)\n",
        "  ])\n",
        "\n",
        "  # Apply DeltaCon similarity calculation\n",
        "  deltacon_similarity_df = cross_joined_df_deltacon.select(\n",
        "      \"df1.community_id\", \"df2.community_id_2\", \"df1.csr_matrix\", \"df2.csr_matrix_2\"\n",
        "  ).groupBy(\"community_id\", \"community_id_2\") \\\n",
        "      .applyInPandas(calculate_deltacon_similarity, schema=deltacon_similarity_schema)\n",
        "\n",
        "  # Step 3: Join Results and Calculate Final Similarity Score\n",
        "  # Join the Frobenius and DeltaCon similarity DataFrames\n",
        "  combined_similarity_df = frobenius_similarity_df.join(\n",
        "      deltacon_similarity_df,\n",
        "      on=[\"community_id_1\", \"community_id_2\"],\n",
        "      how=\"inner\"\n",
        "  )\n",
        "\n",
        "  # Calculate the final similarity score as an average of Frobenius and DeltaCon similarities\n",
        "  final_similarity_df = combined_similarity_df.withColumn(\n",
        "      \"final_similarity\",\n",
        "      expr(\"0.5 * frobenius_similarity + 0.5 * deltacon\")\n",
        "  )\n",
        "\n",
        "  # Show the final results\n",
        "  final_similarity_df.show(truncate=False)\n",
        "  return final_similarity_df\n",
        "\n",
        "  @pandas_udf(schema_similarity, PandasUDFType.GROUPED_MAP)\n",
        "  def calculate_similarity(df):\n",
        "      csr_1 = pickle.loads(df['csr_matrix_1'].iloc[0])\n",
        "      csr_2 = pickle.loads(df['csr_matrix_2'].iloc[0])\n",
        "      similarity = deltacon_similarity(csr_1, csr_2)\n",
        "      return pd.DataFrame({\"community_id_1\": [df['community_id_1'].iloc[0]], \"community_id_2\": [df['community_id_2'].iloc[0]], \"similarity\": [similarity]})\n",
        "\n",
        "  cross_joined = cross_joined.select(\n",
        "      col(\"df1.community_id\").alias(\"community_id_1\"),\n",
        "      col(\"df2.community_id\").alias(\"community_id_2\"),\n",
        "      col(\"df1.csr_matrix\").alias(\"csr_matrix_1\"),\n",
        "      col(\"df2.csr_matrix\").alias(\"csr_matrix_2\")\n",
        "  )\n",
        "\n",
        "  similarities = cross_joined.groupBy(\"community_id_1\", \"community_id_2\").apply(calculate_similarity)\n",
        "\n",
        "  similarities.show(truncate=False)\n",
        "\n",
        "\n",
        "def create_adaptive_buckets(df, columns, min_size=2):\n",
        "    \"\"\"\n",
        "    Create adaptive buckets for specified columns based on natural grouping of close values.\n",
        "\n",
        "    Parameters:\n",
        "        df (DataFrame): The input DataFrame with community statistics.\n",
        "        columns (list): List of column names to bucketize.\n",
        "        min_size (int): Minimum number of communities required in each bucket.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: The DataFrame with additional columns for each adaptive bucket.\n",
        "    \"\"\"\n",
        "    for column in columns:\n",
        "        bucket_col = f\"{column}_bucket\"\n",
        "\n",
        "        # Calculate approximate quantiles for balanced bucketing\n",
        "        quantiles = [i / min_size for i in range(min_size + 1)]\n",
        "        boundaries = df.approxQuantile(column, quantiles, 0.05)\n",
        "\n",
        "        # Materialize boundaries into discrete bucket assignments\n",
        "        bucket_expr = F.when(F.col(column) <= boundaries[1], 0)\n",
        "        for i in range(1, len(boundaries) - 1):\n",
        "            bucket_expr = bucket_expr.when((F.col(column) > boundaries[i]) & (F.col(column) <= boundaries[i + 1]), i)\n",
        "\n",
        "        # Assign buckets to each row based on column values\n",
        "        df = df.withColumn(bucket_col, bucket_expr)\n",
        "\n",
        "        # Debug: Verify if the bucket column was created successfully\n",
        "        print(f\"Verifying creation of {bucket_col} column\")\n",
        "        df.select(column, bucket_col).show(truncate=False)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Step 2: Apply cross join within each bucket and calculate similarities\n",
        "def calculate_similarity_within_buckets(df, columns):\n",
        "    \"\"\"\n",
        "    Calculate similarities between communities within the same buckets for specified columns.\n",
        "\n",
        "    Parameters:\n",
        "        df (DataFrame): The input DataFrame with bucket columns for each specified attribute.\n",
        "        columns (list): List of column names for which buckets have been created.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: The DataFrame with similarity calculations for each pair within the same bucket.\n",
        "    \"\"\"\n",
        "    df_groups = None\n",
        "\n",
        "    # # Register UDFs for similarity calculations\n",
        "    # compare_structural_similarity_udf = F.udf(lambda csr_1, csr_2: compare_weighted_structural_similarity(csr_1, csr_2), DoubleType())\n",
        "    # compare_weighted_similarity_udf = F.udf(lambda csr_1, csr_2: compare_weighted_structural_similarity(csr_1, csr_2), DoubleType())\n",
        "\n",
        "    # Create cross join within each bucket combination\n",
        "    bucket_columns = [f\"{col}_bucket\" for col in columns]\n",
        "\n",
        "    # Debug: Check bucket columns in DataFrame\n",
        "    print(\"Columns in DataFrame before cross join:\")\n",
        "    print(df.columns)\n",
        "\n",
        "    for bucket_combination in df.select(bucket_columns).distinct().collect():\n",
        "        # Filter the DataFrame based on the current bucket combination\n",
        "        filter_condition = F.lit(True)\n",
        "        for i, bucket_col in enumerate(bucket_columns):\n",
        "            filter_condition &= (F.col(bucket_col) == getattr(bucket_combination, bucket_col))\n",
        "        bucket_df = df.filter(filter_condition)\n",
        "\n",
        "        # Only proceed if there are at least two communities in the bucket\n",
        "        if bucket_df.count() >= 2:\n",
        "            print(\"This is the bucket df:\")\n",
        "            bucket_df.show(truncate=False)\n",
        "            cross_joined = calculate_similarities(bucket_df)\n",
        "\n",
        "            # Append to df_groups\n",
        "            if df_groups is None:\n",
        "                df_groups = cross_joined\n",
        "            else:\n",
        "                df_groups = df_groups.union(cross_joined)\n",
        "\n",
        "    return df_groups"
      ],
      "metadata": {
        "id": "mhgOioE4ZU5t"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8th cell - driver of the application run all the functions for a given dataset\n",
        "\n",
        "@track_stage(\"Stage 1: Reading the calls dataset\")\n",
        "def read_csv_to_dataframe(file_path= 'toy_dataset.csv'):\n",
        "  \"\"\"\n",
        "  Read dataset from given path into a Spark DataFrame.\n",
        "  Parameters:\n",
        "    -----------\n",
        "    file_path : str\n",
        "        The name of the given dataset (unigrams or bigrams or both).\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    df_dataset : DataFrame\n",
        "        A DataFrame of calls with the given dataset info.\n",
        "  \"\"\"\n",
        "  df_dataset = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "  # convert start - end times to duration\n",
        "  # 1st - Register the UDFs in Spark\n",
        "  calculate_duration_minutes_udf = udf(calculate_duration_minutes, DoubleType())\n",
        "\n",
        "  # 2nd - use udfs to add columns for duration in minutes\n",
        "  df_dataset = df_dataset.withColumn('duration_minutes', calculate_duration_minutes_udf(col('Start_Time'), col('End_Time')))\n",
        "\n",
        "  #3rd - Adjust Client1 and Client2 to ensure Client1 is the smaller value and Client2 the larger\n",
        "  df_dataset = df_dataset.withColumn(\"Client1_min\", least(col(\"Client1\"), col(\"Client2\"))) \\\n",
        "       .withColumn(\"Client2_max\", greatest(col(\"Client1\"), col(\"Client2\"))) \\\n",
        "       .drop(\"Client1\", \"Client2\") \\\n",
        "       .withColumnRenamed(\"Client1_min\", \"Client1\") \\\n",
        "       .withColumnRenamed(\"Client2_max\", \"Client2\")\n",
        "\n",
        "\n",
        "  # 4th - Aggregate total duration for each unique pair (Client1, Client2)\n",
        "  df_aggregated = df_dataset.groupBy(\"Client1\", \"Client2\") \\\n",
        "    .agg(F.sum('duration_minutes').alias(\"total_duration_minutes\"))\n",
        "\n",
        "  # Join the aggregated total duration back to the original DataFrame\n",
        "  df_dataset = df_dataset.drop('duration_minutes') \\\n",
        "        .join(df_aggregated, on=[\"Client1\", \"Client2\"], how=\"left\")\n",
        "\n",
        "\n",
        "  print(\"The following dataframe has been read from the CSV file:\")\n",
        "  df_dataset.show()\n",
        "  return df_dataset\n",
        "\n",
        "@track_stage(\"Stage 2: Preprocessing and creating the graph\")\n",
        "def create_graph_from_dataframe(df_dataset):\n",
        "  \"\"\"\n",
        "  Create graph in GraphFrame from the calls in the current dataset.\n",
        "  Parameters:\n",
        "    -----------\n",
        "    df_dataset : DataFrame\n",
        "        A DataFrame of calls with the given dataset info.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    df_dataset : DataFrame\n",
        "        A DataFrame of calls with the given dataset info.\n",
        "  \"\"\"\n",
        "\n",
        "  # Create Graph using GraphFrames for community detection\n",
        "  vertices = df_dataset.selectExpr(\"Client1 as id\").union(df_dataset.selectExpr(\"Client2 as id\")).distinct()\n",
        "  edges = df_dataset.selectExpr(\"Client1 as src\", \"Client2 as dst\", \"total_duration_minutes as weight\")\n",
        "  # Cache vertices and edges\n",
        "  vertices.cache()\n",
        "  edges.cache()\n",
        "\n",
        "  # Create a GraphFrame\n",
        "  g = GraphFrame(vertices, edges)\n",
        "\n",
        "  # Find connected components (communities) using GraphFrames\n",
        "  connected_components_result = g.connectedComponents()\n",
        "\n",
        "  # Create a mapping from original community IDs to sequential ones\n",
        "  community_mapping = connected_components_result.select(\"component\").distinct() \\\n",
        "      .orderBy(\"component\") \\\n",
        "      .withColumn(\"new_id\", row_number().over(Window.orderBy(\"component\"))) \\\n",
        "      .cache()\n",
        "\n",
        "  # Join the result (community IDs) with the original dataframe and map to new sequential IDs\n",
        "  df_with_communities = df_dataset.join(connected_components_result, df_dataset['Client1'] == connected_components_result['id'], 'inner') \\\n",
        "      .join(community_mapping, connected_components_result['component'] == community_mapping['component'], 'inner') \\\n",
        "      .drop(connected_components_result['id']) \\\n",
        "      .drop(community_mapping['component']) \\\n",
        "      .withColumnRenamed('new_id', 'community_id')\n",
        "\n",
        "  # Calculate the number of unique clients (community size) per community\n",
        "  community_sizes = df_with_communities.select('community_id', 'Client1').union(df_with_communities.select('community_id', 'Client2')) \\\n",
        "      .distinct() \\\n",
        "      .groupBy('community_id').agg(countDistinct('Client1').alias('community_size'))\n",
        "\n",
        "  # Merge the community sizes into the main DataFrame\n",
        "  df_final = df_with_communities.join(community_sizes, 'community_id')\n",
        "\n",
        "  # Get list of tuples for each community member by considering both Client1 and Client2\n",
        "  community_members = df_final.select(\"community_id\", \"Client1\", \"Client2\", \"total_duration_minutes\") \\\n",
        "    .distinct() \\\n",
        "    .orderBy(\"Client1\") \\\n",
        "    .groupBy(\"community_id\") \\\n",
        "    .agg(F.collect_list(F.struct(\n",
        "        F.col(\"Client1\"),\n",
        "        F.col(\"Client2\"),\n",
        "        F.col(\"total_duration_minutes\")\n",
        "    )).alias(\"members\")) \\\n",
        "    .orderBy(\"community_id\")\n",
        "\n",
        "  # Show the final DataFrame with community IDs, duration, and community sizes\n",
        "  print(\"\\nFinal DataFrame with Sequential Community IDs:\")\n",
        "  df_final.select('Client1',\n",
        "                  'Client2',\n",
        "                  'total_duration_minutes',\n",
        "                  'community_id',\n",
        "                  'community_size') \\\n",
        "      .orderBy(\"community_id\") \\\n",
        "      .show()\n",
        "\n",
        "  # Show the list of community members as tuples\n",
        "  print(\"\\nCommunity Members with Sequential IDs:\")\n",
        "  community_members.show(truncate=False)\n",
        "\n",
        "  # Save results to CSV files\n",
        "  # Save the main analysis results\n",
        "  df_final.select('Client1',\n",
        "                  'Client2',\n",
        "                  'total_duration_minutes',\n",
        "                  'community_id',\n",
        "                  'community_size') \\\n",
        "      .orderBy(\"community_id\") \\\n",
        "      .write.mode(\"overwrite\").option(\"header\", \"true\") \\\n",
        "      .csv(f\"{dataset_name}_community_analysis_results\")\n",
        "\n",
        "  # Save community members in a flattened format\n",
        "  df_final.select('community_id',\n",
        "                  'Client1',\n",
        "                  'Client2',\n",
        "                  'total_duration_minutes') \\\n",
        "      .distinct() \\\n",
        "      .orderBy(\"community_id\") \\\n",
        "      .write.mode(\"overwrite\").option(\"header\", \"true\") \\\n",
        "      .csv(f\"{dataset_name}_community_members_results\")\n",
        "  # add to .cache to avoid memory overflow\n",
        "  df_final.cache()\n",
        "\n",
        "  # Optionally, if you want to save additional community statistics\n",
        "  community_stats = df_final.groupBy('community_id') \\\n",
        "      .agg(\n",
        "          countDistinct('Client1', 'Client2').alias('unique_members'),\n",
        "          count('*').alias('total_calls'),\n",
        "          sum('total_duration_minutes').alias('sum_duration_minutes'),\n",
        "          avg('total_duration_minutes').alias('avg_call_duration'),\n",
        "          percentile_approx('total_duration_minutes', 0.25).alias('duration_25th_percentile'),\n",
        "          percentile_approx('total_duration_minutes', 0.5).alias('median_call_duration'),\n",
        "          percentile_approx('total_duration_minutes', 0.75).alias('duration_75th_percentile')\n",
        "      ) \\\n",
        "      .orderBy('community_id')\n",
        "\n",
        "  community_stats.write.mode(\"overwrite\") \\\n",
        "      .option(\"header\", \"true\") \\\n",
        "      .csv(f\"{dataset_name}_community_statistics_results\")\n",
        "\n",
        "  print(\"This is the community stats:\")\n",
        "  community_stats.show(truncate=False)\n",
        "  # add to .cache to avoid memory overflow\n",
        "  community_stats.cache()\n",
        "  return df_final, community_members, community_stats\n",
        "\n",
        "# Create CSR adjacency matrices for each community and serialize them\n",
        "@track_stage(\"Stage 3: Creating CSR matrices\")\n",
        "def format_members_to_csr_matrix(community_members, community_stats):\n",
        "  \"\"\"\n",
        "  Create CSR adjacency matrices for each community and serialize them.\n",
        "\n",
        "  Parameters:\n",
        "    community_members: Dataframe\n",
        "    A dataframe of a specific community's members\n",
        "    community_stats: Dataframe\n",
        "    A dataframe of all the communities statistics\n",
        "  \"\"\"\n",
        "  use_weights=True\n",
        "  # Use the function to generate a serialized CSR matrix for each community and show the results\n",
        "  result_true = community_members.groupBy(\"community_id\").apply(create_csr_matrix_from_edges)\n",
        "  # print(f\"This is the csr formating results, weight = {use_weights}:\")\n",
        "  # result_true.show(truncate=False)\n",
        "  use_weights=False\n",
        "  result_false = community_members.groupBy(\"community_id\").apply(create_csr_matrix_from_edges)\n",
        "  # print(f\"This is the csr formating results, weight = {use_weights}:\")\n",
        "  # result_false.show(truncate=False)\n",
        "\n",
        "  # Join the community statistics dataframe and the csr_matrix dataframe\n",
        "  # for final analysis\n",
        "  df_community_stats_csr = community_stats.join(result_true,\n",
        "                                                on='community_id', how='inner')\n",
        "  print(\"This is the statsitcs and csr dataframe joined:\")\n",
        "  df_community_stats_csr.show(truncate=False)\n",
        "  # add to .cache to avoid memory overflow\n",
        "  df_community_stats_csr.cache()\n",
        "\n",
        "  return df_community_stats_csr\n",
        "\n",
        "\n",
        "# Function to create similarity-based subgroups by comparing multiple columns\n",
        "@track_stage(\"Stage 4: Calculate similarities between communities\")\n",
        "def create_similarity_subgroups(df, columns, tolerances):\n",
        "    \"\"\"\n",
        "    Create similarity-based subgroups based on specified columns and tolerances, then apply a custom function.\n",
        "\n",
        "    Parameters:\n",
        "        df (DataFrame): The Spark DataFrame with community data.\n",
        "        columns (list): List of column names to consider for similarity.\n",
        "        tolerances (dict): Dictionary specifying the tolerance (± range) for each column.\n",
        "    \"\"\"\n",
        "    # Collect the DataFrame into a list of rows\n",
        "    communities = df.collect()\n",
        "\n",
        "    # Initialize a list to store similarity groups\n",
        "    similarity_groups = []\n",
        "    df_groups = None\n",
        "\n",
        "    # Compare each pair of communities\n",
        "    for i, j in combinations(range(len(communities)), 2):\n",
        "        community_i = communities[i]\n",
        "        community_j = communities[j]\n",
        "\n",
        "        # Check if the communities are similar based on all specified columns and tolerances\n",
        "        is_similar = all(\n",
        "            abs(community_i[column] - community_j[column]) <= tolerances[column]\n",
        "            for column in columns\n",
        "        )\n",
        "\n",
        "        # If they are similar, add them to the same group\n",
        "        if is_similar:\n",
        "            found_group = False\n",
        "            for group in similarity_groups:\n",
        "                if community_i.community_id in group or community_j.community_id in group:\n",
        "                    group.add(community_i.community_id)\n",
        "                    group.add(community_j.community_id)\n",
        "                    found_group = True\n",
        "                    break\n",
        "            if not found_group:\n",
        "                similarity_groups.append({community_i.community_id, community_j.community_id})\n",
        "\n",
        "    # Create a DataFrame for each subgroup and apply the custom function\n",
        "    for group in similarity_groups:\n",
        "        subgroup_df = df.filter(F.col(\"community_id\").isin(group))\n",
        "        subgroup_cross_joined = calculate_similarities(subgroup_df)\n",
        "        # Initialize or append to df_groups\n",
        "        if df_groups is None:\n",
        "            df_groups = subgroup_cross_joined\n",
        "        else:\n",
        "            df_groups = df_groups.union(subgroup_cross_joined)\n",
        "\n",
        "\n",
        "    # export all found groups\n",
        "    if df_groups:\n",
        "      df_groups.write \\\n",
        "                  .mode(\"overwrite\") \\\n",
        "                  .option(\"header\", header) \\\n",
        "                  .csv(f\"{dataset_name}_df_groups.csv\")\n",
        "    else:\n",
        "      print(\"No groups found!\")\n",
        "    return df_groups\n",
        "\n",
        "\n",
        "@track_stage(\"Stage 5: Export groups results\")\n",
        "def export_similarity_groups(final_similarity_df, df_clients_info, dataset_name=\"dataset\"):\n",
        "    # Set similarity threshold\n",
        "    similarity_threshold = 0.55\n",
        "\n",
        "    # Filter pairs above the threshold and define vertices and edges\n",
        "    similar_pairs = final_similarity_df.filter(F.col(\"final_similarity\") >= similarity_threshold)\n",
        "    vertices = similar_pairs.select(\"community_id_1\").union(similar_pairs.select(\"community_id_2\")).distinct() \\\n",
        "        .withColumnRenamed(\"community_id_1\", \"id\").cache()\n",
        "    edges = similar_pairs.select(\n",
        "        F.col(\"community_id_1\").alias(\"src\"),\n",
        "        F.col(\"community_id_2\").alias(\"dst\")\n",
        "    ).cache()\n",
        "\n",
        "    # Create graph and find connected components (groups of communities)\n",
        "    g = GraphFrame(vertices, edges)\n",
        "    connected_components = g.connectedComponents()\n",
        "\n",
        "    # Assign group numbers to communities found in groups\n",
        "    grouped_communities = connected_components.groupBy(\"component\") \\\n",
        "        .agg(F.collect_list(\"id\").alias(\"community_group\")) \\\n",
        "        .withColumn(\"group_number\", F.row_number().over(Window.orderBy(\"component\")))\n",
        "\n",
        "    # Communities not found in any group will be assigned their own group, ordered by community_id\n",
        "    all_communities = df_clients_info.select(\"community_id\").distinct()\n",
        "    unmatched_communities = all_communities.join(\n",
        "        grouped_communities.select(F.explode(\"community_group\").alias(\"community_id\")),\n",
        "        on=\"community_id\",\n",
        "        how=\"left_anti\"\n",
        "    ).withColumn(\"group_number\", F.monotonically_increasing_id() + grouped_communities.count() + 1)\\\n",
        "     .orderBy(\"community_id\")  # Sort unmatched communities by community_id\n",
        "\n",
        "    # Combine matched and sorted unmatched communities into a single DataFrame\n",
        "    all_grouped_communities = grouped_communities.select(\"group_number\", F.explode(\"community_group\").alias(\"community_id\")) \\\n",
        "                            .union(unmatched_communities.select(\"group_number\", \"community_id\")) \\\n",
        "                            .orderBy(\"group_number\", \"community_id\")  # Sort final output by group_number and community_id\n",
        "\n",
        "    # Prepare the final output structure by joining call details\n",
        "    exploded_communities = all_grouped_communities \\\n",
        "        .join(df_clients_info, on=\"community_id\", how=\"left\") \\\n",
        "        .select(\n",
        "            \"group_number\",\n",
        "            \"community_id\",\n",
        "            \"Client1\",\n",
        "            \"Client2\",\n",
        "            \"Start_Time\",\n",
        "            \"End_Time\"\n",
        "        )\n",
        "\n",
        "    # Step 1: Create community-level formatted output\n",
        "    community_output_df = exploded_communities \\\n",
        "        .withColumn(\"line\", F.concat_ws(\", \", \"Client1\", \"Client2\", \"Start_Time\", \"End_Time\")) \\\n",
        "        .groupBy(\"group_number\", \"community_id\") \\\n",
        "        .agg(F.collect_list(\"line\").alias(\"community_lines\"))\n",
        "\n",
        "    # Step 2: Collect data to construct the final output using Python\n",
        "    community_data = community_output_df.collect()\n",
        "\n",
        "    # Step 3: Construct the output manually in Python, ensuring communities are sorted by community_id\n",
        "    output_dict = {}\n",
        "    for row in community_data:\n",
        "        group_number = row[\"group_number\"]\n",
        "        community_id = row[\"community_id\"]\n",
        "        community_lines = row[\"community_lines\"]\n",
        "\n",
        "        if group_number not in output_dict:\n",
        "            output_dict[group_number] = []\n",
        "        output_dict[group_number].append(f\"Community {community_id}:\\n\" + \"\\n\".join(community_lines))\n",
        "\n",
        "    # Create final output string, sorted by group_number\n",
        "    output_lines = []\n",
        "    for group_number in sorted(output_dict.keys()):\n",
        "        group_header = f\"Group {group_number}:\"\n",
        "        communities_output = \"\\n\".join(output_dict[group_number])\n",
        "        output_lines.append(f\"{group_header}\\n{communities_output}\")\n",
        "\n",
        "    final_output = \"\\n\\n\".join(output_lines)\n",
        "\n",
        "    # Print each line as it would appear in the text file\n",
        "    print(\"Preview of the similarity groups output:\\n\")\n",
        "    print(final_output)\n",
        "\n",
        "    # Save the output as a text file\n",
        "    output_path = f\"{dataset_name}_similarity_groups.txt\"\n",
        "    with open(output_path, \"w\") as file:\n",
        "        file.write(final_output)\n",
        "\n",
        "    print(f\"\\nExported similarity groups to: {output_path}\")\n",
        "\n",
        "\n",
        "def process_dataset(dataset_file_path):\n",
        "   # Get the base name from the path (e.g., 'file.txt' from '/path/to/file.txt')\n",
        "  basename =  os.path.basename(dataset_file_path)\n",
        "  # Split the filename and extension\n",
        "  dataset_name = os.path.splitext(basename)[0]\n",
        "  clear_csv = i == 0 # only clear the if this is the 1st dataset\n",
        "\n",
        "  # step 1 - read the dataset\n",
        "  df_dataset = read_csv_to_dataframe(dataset_file_path)\n",
        "\n",
        "  # step 2 - preprocess (convert to duartion in min, create grpah, and find commutnies)\n",
        "  df_final, community_members, community_stats = create_graph_from_dataframe(df_dataset)\n",
        "  gc.collect()\n",
        "\n",
        "  # step 3 - create CSR matrix for each communite\n",
        "  df_community_stats_csr = format_members_to_csr_matrix(community_members, community_stats)\n",
        "  gc.collect()\n",
        "\n",
        "  # step 4 - calculate similarities between communties for find groups\n",
        "  # Define columns to use for bucketization\n",
        "  columns = ['unique_members']\n",
        "  # Create adaptive buckets and calculate similarities\n",
        "  df_with_buckets = create_adaptive_buckets(df_community_stats_csr, columns, min_size=2)\n",
        "  df_groups = calculate_similarity_within_buckets(df_with_buckets, columns)\n",
        "\n",
        "  # step 5 - export the found groups\n",
        "  export_similarity_groups(final_similarity_df=df_groups, df_clients_info=df_final, dataset_name=dataset_name)"
      ],
      "metadata": {
        "id": "Ike2lfPxo19c"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9th cell - Read datasets dataframe, and iterate over each one to create communities and form similarity groups\n",
        "# Read the datasets csv\n",
        "df_datasets = pd.read_csv(\"dataset_metadata.csv\")\n",
        "# Print a peak of the dataset\n",
        "print(\"These are the found datasets\")\n",
        "df_datasets.head(10)\n",
        "# set global params\n",
        "clear_csv = False\n",
        "dataset_file_path = \"toy_dataset.csv\"\n",
        "dataset_name = \"toy_dataset\"\n",
        "use_weights = False\n",
        "\n",
        "for i, dataset in df_datasets.iterrows():\n",
        "  spark, stagemetrics = create_spark()\n",
        "  print(f\"Starting to process {i+1} dataset with the following params: \\n{dataset}\")\n",
        "\n",
        "  dataset_file_path = dataset[\"csv_filename\"]\n",
        "  process_dataset(dataset_file_path)\n",
        "  gc.collect()\n",
        "  kill_spark(spark)"
      ],
      "metadata": {
        "id": "RAMTvsL2v2YH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "a7dd96c9-ab8b-40be-809d-d51124443aac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "These are the found datasets\n",
            "Starting to process 1 dataset with the following params: \n",
            "num_communities                                                5\n",
            "community_size_range                                      (3, 5)\n",
            "calls_per_connection_range                                (1, 2)\n",
            "duration_range                                         (30, 120)\n",
            "density                                                      0.5\n",
            "num_samples                                                   50\n",
            "dataset_name                                    dataset_config_1\n",
            "csv_filename                  /content/datasets/dataset_config_1\n",
            "Name: 0, dtype: object\n",
            "Starting Stage 1: Reading the calls dataset\n",
            "The following dataframe has been read from the CSV file:\n",
            "+-------+-------+----------+----------+----------------------+\n",
            "|Client1|Client2|Start_Time|  End_Time|total_duration_minutes|\n",
            "+-------+-------+----------+----------+----------------------+\n",
            "|      0|      1|2401011955|2401012118|                  83.0|\n",
            "|      0|      2|2401012029|2401012123|                 116.0|\n",
            "|      0|      2|2401010039|2401010141|                 116.0|\n",
            "|      0|      3|2401011915|2401012103|                 228.0|\n",
            "|      0|      3|2401010843|2401011043|                 228.0|\n",
            "|      0|      4|2401012029|2401012205|                 168.0|\n",
            "|      0|      4|2401010757|2401010909|                 168.0|\n",
            "|      1|      2|2401010002|2401010051|                  49.0|\n",
            "|      1|      3|2401012251|2401012354|                  63.0|\n",
            "|      1|      4|2401011242|2401011402|                 110.0|\n",
            "|      1|      4|2401011034|2401011104|                 110.0|\n",
            "|      2|      3|2401010028|2401010120|                  52.0|\n",
            "|      2|      4|2401011525|2401011635|                  70.0|\n",
            "|      3|      4|2401011549|2401011725|                  96.0|\n",
            "|   1000|   1001|2401010316|2401010509|                 149.0|\n",
            "|   1000|   1001|2401010822|2401010858|                 149.0|\n",
            "|   1000|   1002|2401011456|2401011647|                 111.0|\n",
            "|   1000|   1003|2401011102|2401011237|                 210.0|\n",
            "|   1000|   1003|2401010938|2401011133|                 210.0|\n",
            "|   1000|   1004|2401010314|2401010502|                 175.0|\n",
            "+-------+-------+----------+----------+----------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Completed Stage 1: Reading the calls dataset\n",
            "\n",
            "+---------+--------+-----------+-------------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+--------------------+----------------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+------------------+-------------------------+-------------------------+--------------------------+---------------------+---------------------+----------------------+----------------------------+-------------------+---------------------+----------------------------------+----------------------------------+\n",
            "|numStages|numTasks|elapsedTime|stageDuration|executorRunTime|executorCpuTime|executorDeserializeTime|executorDeserializeCpuTime|resultSerializationTime|jvmGCTime|shuffleFetchWaitTime|shuffleWriteTime|resultSize|diskBytesSpilled|memoryBytesSpilled|peakExecutionMemory|recordsRead|bytesRead|recordsWritten|bytesWritten|shuffleRecordsRead|shuffleTotalBlocksFetched|shuffleLocalBlocksFetched|shuffleRemoteBlocksFetched|shuffleTotalBytesRead|shuffleLocalBytesRead|shuffleRemoteBytesRead|shuffleRemoteBytesReadToDisk|shuffleBytesWritten|shuffleRecordsWritten|stage_name                        |dataset                           |\n",
            "+---------+--------+-----------+-------------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+--------------------+----------------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+------------------+-------------------------+-------------------------+--------------------------+---------------------+---------------------+----------------------+----------------------------+-------------------+---------------------+----------------------------------+----------------------------------+\n",
            "|5        |5       |9777       |5765         |3908           |771            |1035                   |658                       |13                     |110      |0                   |21              |6432      |0               |0                 |67371008           |123        |6308     |0             |0           |34                |1                        |1                        |0                         |724                  |724                  |0                     |0                           |724                |34                   |Stage 1: Reading the calls dataset|/content/datasets/dataset_config_1|\n",
            "+---------+--------+-----------+-------------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+--------------------+----------------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+------------------+-------------------------+-------------------------+--------------------------+---------------------+---------------------+----------------------+----------------------------+-------------------+---------------------+----------------------------------+----------------------------------+\n",
            "\n",
            "Starting Stage 2: Preprocessing and creating the graph\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:168: UserWarning: DataFrame.sql_ctx is an internal property, and will be removed in future releases. Use DataFrame.sparkSession instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py:147: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
            "  warnings.warn(\"DataFrame constructor is internal. Do not directly use it.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final DataFrame with Sequential Community IDs:\n",
            "+-------+-------+----------------------+------------+--------------+\n",
            "|Client1|Client2|total_duration_minutes|community_id|community_size|\n",
            "+-------+-------+----------------------+------------+--------------+\n",
            "|      0|      4|                 168.0|           1|             5|\n",
            "|      0|      4|                 168.0|           1|             5|\n",
            "|      0|      3|                 228.0|           1|             5|\n",
            "|      0|      3|                 228.0|           1|             5|\n",
            "|      0|      2|                 116.0|           1|             5|\n",
            "|      0|      2|                 116.0|           1|             5|\n",
            "|      0|      1|                  83.0|           1|             5|\n",
            "|      3|      4|                  96.0|           1|             5|\n",
            "|      1|      4|                 110.0|           1|             5|\n",
            "|      1|      4|                 110.0|           1|             5|\n",
            "|      1|      3|                  63.0|           1|             5|\n",
            "|      1|      2|                  49.0|           1|             5|\n",
            "|      2|      4|                  70.0|           1|             5|\n",
            "|      2|      3|                  52.0|           1|             5|\n",
            "|   1001|   1004|                  40.0|           2|             5|\n",
            "|   1001|   1003|                 113.0|           2|             5|\n",
            "|   1001|   1003|                 113.0|           2|             5|\n",
            "|   1001|   1002|                 196.0|           2|             5|\n",
            "|   1001|   1002|                 196.0|           2|             5|\n",
            "|   1003|   1004|                 106.0|           2|             5|\n",
            "+-------+-------+----------------------+------------+--------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "\n",
            "Community Members with Sequential IDs:\n",
            "+------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|community_id|members                                                                                                                                                                                                         |\n",
            "+------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|1           |[{0, 4, 168.0}, {0, 2, 116.0}, {0, 3, 228.0}, {0, 1, 83.0}, {1, 3, 63.0}, {1, 4, 110.0}, {1, 2, 49.0}, {2, 4, 70.0}, {2, 3, 52.0}, {3, 4, 96.0}]                                                                |\n",
            "|2           |[{1000, 1003, 210.0}, {1000, 1002, 111.0}, {1000, 1001, 149.0}, {1000, 1004, 175.0}, {1001, 1002, 196.0}, {1001, 1004, 40.0}, {1001, 1003, 113.0}, {1002, 1004, 43.0}, {1002, 1003, 136.0}, {1003, 1004, 106.0}]|\n",
            "|3           |[{2000, 2002, 157.0}, {2000, 2003, 109.0}, {2000, 2001, 76.0}, {2001, 2003, 31.0}, {2001, 2002, 159.0}, {2002, 2003, 51.0}]                                                                                     |\n",
            "|4           |[{3000, 3002, 192.0}, {3000, 3001, 92.0}, {3001, 3002, 32.0}]                                                                                                                                                   |\n",
            "|5           |[{4000, 4003, 100.0}, {4000, 4004, 31.0}, {4000, 4002, 73.0}, {4000, 4001, 138.0}, {4001, 4002, 125.0}]                                                                                                         |\n",
            "+------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "This is the community stats:\n",
            "+------------+--------------+-----------+--------------------+------------------+------------------------+--------------------+------------------------+\n",
            "|community_id|unique_members|total_calls|sum_duration_minutes|avg_call_duration |duration_25th_percentile|median_call_duration|duration_75th_percentile|\n",
            "+------------+--------------+-----------+--------------------+------------------+------------------------+--------------------+------------------------+\n",
            "|1           |10            |14         |1657.0              |118.35714285714286|70.0                    |110.0               |168.0                   |\n",
            "|2           |10            |16         |2258.0              |141.125           |111.0                   |136.0               |175.0                   |\n",
            "|3           |6             |8          |899.0               |112.375           |51.0                    |109.0               |157.0                   |\n",
            "|4           |3             |4          |508.0               |127.0             |32.0                    |92.0                |192.0                   |\n",
            "|5           |5             |8          |830.0               |103.75            |73.0                    |100.0               |125.0                   |\n",
            "+------------+--------------+-----------+--------------------+------------------+------------------------+--------------------+------------------------+\n",
            "\n",
            "Completed Stage 2: Preprocessing and creating the graph\n",
            "\n",
            "+---------+--------+-----------+-------------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+--------------------+----------------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+------------------+-------------------------+-------------------------+--------------------------+---------------------+---------------------+----------------------+----------------------------+-------------------+---------------------+---------------------------------------------+----------------------------------+\n",
            "|numStages|numTasks|elapsedTime|stageDuration|executorRunTime|executorCpuTime|executorDeserializeTime|executorDeserializeCpuTime|resultSerializationTime|jvmGCTime|shuffleFetchWaitTime|shuffleWriteTime|resultSize|diskBytesSpilled|memoryBytesSpilled|peakExecutionMemory|recordsRead|bytesRead|recordsWritten|bytesWritten|shuffleRecordsRead|shuffleTotalBlocksFetched|shuffleLocalBlocksFetched|shuffleRemoteBlocksFetched|shuffleTotalBytesRead|shuffleLocalBytesRead|shuffleRemoteBytesRead|shuffleRemoteBytesReadToDisk|shuffleBytesWritten|shuffleRecordsWritten|stage_name                                   |dataset                           |\n",
            "+---------+--------+-----------+-------------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+--------------------+----------------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+------------------+-------------------------+-------------------------+--------------------------+---------------------+---------------------+----------------------+----------------------------+-------------------+---------------------+---------------------------------------------+----------------------------------+\n",
            "|137      |341     |82216      |23282        |17312          |5874           |6094                   |2909                      |351                    |1183     |0                   |366             |234131    |0               |0                 |336855040          |1341       |133782   |106           |2780        |1119              |262                      |262                      |0                         |48132                |48132                |0                     |0                           |43232              |1028                 |Stage 2: Preprocessing and creating the graph|/content/datasets/dataset_config_1|\n",
            "+---------+--------+-----------+-------------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+--------------------+----------------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+------------------+-------------------------+-------------------------+--------------------------+---------------------+---------------------+----------------------+----------------------------+-------------------+---------------------+---------------------------------------------+----------------------------------+\n",
            "\n",
            "Starting Stage 3: Creating CSR matrices\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/group_ops.py:104: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is the statsitcs and csr dataframe joined:\n",
            "+------------+--------------+-----------+--------------------+------------------+------------------------+--------------------+------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|community_id|unique_members|total_calls|sum_duration_minutes|avg_call_duration |duration_25th_percentile|median_call_duration|duration_75th_percentile|csr_matrix                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
            "+------------+--------------+-----------+--------------------+------------------+------------------------+--------------------+------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|1           |10            |14         |1657.0              |118.35714285714286|70.0                    |110.0               |168.0                   |[80 04 95 07 02 00 00 00 00 00 00 8C 11 73 63 69 70 79 2E 73 70 61 72 73 65 2E 5F 63 73 72 94 8C 0A 63 73 72 5F 6D 61 74 72 69 78 94 93 94 29 81 94 7D 94 28 8C 06 5F 73 68 61 70 65 94 4B 05 4B 05 86 94 8C 08 6D 61 78 70 72 69 6E 74 94 4B 32 8C 06 69 6E 64 70 74 72 94 8C 15 6E 75 6D 70 79 2E 63 6F 72 65 2E 6D 75 6C 74 69 61 72 72 61 79 94 8C 0C 5F 72 65 63 6F 6E 73 74 72 75 63 74 94 93 94 8C 05 6E 75 6D 70 79 94 8C 07 6E 64 61 72 72 61 79 94 93 94 4B 00 85 94 43 01 62 94 87 94 52 94 28 4B 01 4B 06 85 94 68 0C 8C 05 64 74 79 70 65 94 93 94 8C 02 69 34 94 89 88 87 94 52 94 28 4B 03 8C 01 3C 94 4E 4E 4E 4A FF FF FF FF 4A FF FF FF FF 4B 00 74 94 62 89 43 18 00 00 00 00 04 00 00 00 07 00 00 00 09 00 00 00 0A 00 00 00 0A 00 00 00 94 74 94 62 8C 07 69 6E 64 69 63 65 73 94 68 0B 68 0E 4B 00 85 94 68 10 87 94 52 94 28 4B 01 4B 0A 85 94 68 18 89 43 28 01 00 00 00 02 00 00 00 03 00 00 00 04 00 00 00 02 00 00 00 03 00 00 00 04 00 00 00 03 00 00 00 04 00 00 00 04 00 00 00 94 74 94 62 8C 04 64 61 74 61 94 68 0B 68 0E 4B 00 85 94 68 10 87 94 52 94 28 4B 01 4B 0A 85 94 68 15 8C 02 69 38 94 89 88 87 94 52 94 28 4B 03 68 19 4E 4E 4E 4A FF FF FF FF 4A FF FF FF FF 4B 00 74 94 62 89 43 50 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 94 74 94 62 8C 15 5F 68 61 73 5F 63 61 6E 6F 6E 69 63 61 6C 5F 66 6F 72 6D 61 74 94 88 8C 13 5F 68 61 73 5F 73 6F 72 74 65 64 5F 69 6E 64 69 63 65 73 94 88 75 62 2E]|\n",
            "|2           |10            |16         |2258.0              |141.125           |111.0                   |136.0               |175.0                   |[80 04 95 07 02 00 00 00 00 00 00 8C 11 73 63 69 70 79 2E 73 70 61 72 73 65 2E 5F 63 73 72 94 8C 0A 63 73 72 5F 6D 61 74 72 69 78 94 93 94 29 81 94 7D 94 28 8C 06 5F 73 68 61 70 65 94 4B 05 4B 05 86 94 8C 08 6D 61 78 70 72 69 6E 74 94 4B 32 8C 06 69 6E 64 70 74 72 94 8C 15 6E 75 6D 70 79 2E 63 6F 72 65 2E 6D 75 6C 74 69 61 72 72 61 79 94 8C 0C 5F 72 65 63 6F 6E 73 74 72 75 63 74 94 93 94 8C 05 6E 75 6D 70 79 94 8C 07 6E 64 61 72 72 61 79 94 93 94 4B 00 85 94 43 01 62 94 87 94 52 94 28 4B 01 4B 06 85 94 68 0C 8C 05 64 74 79 70 65 94 93 94 8C 02 69 34 94 89 88 87 94 52 94 28 4B 03 8C 01 3C 94 4E 4E 4E 4A FF FF FF FF 4A FF FF FF FF 4B 00 74 94 62 89 43 18 00 00 00 00 04 00 00 00 07 00 00 00 09 00 00 00 0A 00 00 00 0A 00 00 00 94 74 94 62 8C 07 69 6E 64 69 63 65 73 94 68 0B 68 0E 4B 00 85 94 68 10 87 94 52 94 28 4B 01 4B 0A 85 94 68 18 89 43 28 01 00 00 00 02 00 00 00 03 00 00 00 04 00 00 00 02 00 00 00 03 00 00 00 04 00 00 00 03 00 00 00 04 00 00 00 04 00 00 00 94 74 94 62 8C 04 64 61 74 61 94 68 0B 68 0E 4B 00 85 94 68 10 87 94 52 94 28 4B 01 4B 0A 85 94 68 15 8C 02 69 38 94 89 88 87 94 52 94 28 4B 03 68 19 4E 4E 4E 4A FF FF FF FF 4A FF FF FF FF 4B 00 74 94 62 89 43 50 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 94 74 94 62 8C 15 5F 68 61 73 5F 63 61 6E 6F 6E 69 63 61 6C 5F 66 6F 72 6D 61 74 94 88 8C 13 5F 68 61 73 5F 73 6F 72 74 65 64 5F 69 6E 64 69 63 65 73 94 88 75 62 2E]|\n",
            "|3           |6             |8          |899.0               |112.375           |51.0                    |109.0               |157.0                   |[80 04 95 D3 01 00 00 00 00 00 00 8C 11 73 63 69 70 79 2E 73 70 61 72 73 65 2E 5F 63 73 72 94 8C 0A 63 73 72 5F 6D 61 74 72 69 78 94 93 94 29 81 94 7D 94 28 8C 06 5F 73 68 61 70 65 94 4B 04 4B 04 86 94 8C 08 6D 61 78 70 72 69 6E 74 94 4B 32 8C 06 69 6E 64 70 74 72 94 8C 15 6E 75 6D 70 79 2E 63 6F 72 65 2E 6D 75 6C 74 69 61 72 72 61 79 94 8C 0C 5F 72 65 63 6F 6E 73 74 72 75 63 74 94 93 94 8C 05 6E 75 6D 70 79 94 8C 07 6E 64 61 72 72 61 79 94 93 94 4B 00 85 94 43 01 62 94 87 94 52 94 28 4B 01 4B 05 85 94 68 0C 8C 05 64 74 79 70 65 94 93 94 8C 02 69 34 94 89 88 87 94 52 94 28 4B 03 8C 01 3C 94 4E 4E 4E 4A FF FF FF FF 4A FF FF FF FF 4B 00 74 94 62 89 43 14 00 00 00 00 03 00 00 00 05 00 00 00 06 00 00 00 06 00 00 00 94 74 94 62 8C 07 69 6E 64 69 63 65 73 94 68 0B 68 0E 4B 00 85 94 68 10 87 94 52 94 28 4B 01 4B 06 85 94 68 18 89 43 18 01 00 00 00 02 00 00 00 03 00 00 00 02 00 00 00 03 00 00 00 03 00 00 00 94 74 94 62 8C 04 64 61 74 61 94 68 0B 68 0E 4B 00 85 94 68 10 87 94 52 94 28 4B 01 4B 06 85 94 68 15 8C 02 69 38 94 89 88 87 94 52 94 28 4B 03 68 19 4E 4E 4E 4A FF FF FF FF 4A FF FF FF FF 4B 00 74 94 62 89 43 30 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 94 74 94 62 8C 15 5F 68 61 73 5F 63 61 6E 6F 6E 69 63 61 6C 5F 66 6F 72 6D 61 74 94 88 8C 13 5F 68 61 73 5F 73 6F 72 74 65 64 5F 69 6E 64 69 63 65 73 94 88 75 62 2E]                                                                                                                                                            |\n",
            "|4           |3             |4          |508.0               |127.0             |32.0                    |92.0                |192.0                   |[80 04 95 AB 01 00 00 00 00 00 00 8C 11 73 63 69 70 79 2E 73 70 61 72 73 65 2E 5F 63 73 72 94 8C 0A 63 73 72 5F 6D 61 74 72 69 78 94 93 94 29 81 94 7D 94 28 8C 06 5F 73 68 61 70 65 94 4B 03 4B 03 86 94 8C 08 6D 61 78 70 72 69 6E 74 94 4B 32 8C 06 69 6E 64 70 74 72 94 8C 15 6E 75 6D 70 79 2E 63 6F 72 65 2E 6D 75 6C 74 69 61 72 72 61 79 94 8C 0C 5F 72 65 63 6F 6E 73 74 72 75 63 74 94 93 94 8C 05 6E 75 6D 70 79 94 8C 07 6E 64 61 72 72 61 79 94 93 94 4B 00 85 94 43 01 62 94 87 94 52 94 28 4B 01 4B 04 85 94 68 0C 8C 05 64 74 79 70 65 94 93 94 8C 02 69 34 94 89 88 87 94 52 94 28 4B 03 8C 01 3C 94 4E 4E 4E 4A FF FF FF FF 4A FF FF FF FF 4B 00 74 94 62 89 43 10 00 00 00 00 02 00 00 00 03 00 00 00 03 00 00 00 94 74 94 62 8C 07 69 6E 64 69 63 65 73 94 68 0B 68 0E 4B 00 85 94 68 10 87 94 52 94 28 4B 01 4B 03 85 94 68 18 89 43 0C 01 00 00 00 02 00 00 00 02 00 00 00 94 74 94 62 8C 04 64 61 74 61 94 68 0B 68 0E 4B 00 85 94 68 10 87 94 52 94 28 4B 01 4B 03 85 94 68 15 8C 02 69 38 94 89 88 87 94 52 94 28 4B 03 68 19 4E 4E 4E 4A FF FF FF FF 4A FF FF FF FF 4B 00 74 94 62 89 43 18 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 94 74 94 62 8C 15 5F 68 61 73 5F 63 61 6E 6F 6E 69 63 61 6C 5F 66 6F 72 6D 61 74 94 88 8C 13 5F 68 61 73 5F 73 6F 72 74 65 64 5F 69 6E 64 69 63 65 73 94 88 75 62 2E]                                                                                                                                                                                                                                                                                    |\n",
            "|5           |5             |8          |830.0               |103.75            |73.0                    |100.0               |125.0                   |[80 04 95 CB 01 00 00 00 00 00 00 8C 11 73 63 69 70 79 2E 73 70 61 72 73 65 2E 5F 63 73 72 94 8C 0A 63 73 72 5F 6D 61 74 72 69 78 94 93 94 29 81 94 7D 94 28 8C 06 5F 73 68 61 70 65 94 4B 05 4B 05 86 94 8C 08 6D 61 78 70 72 69 6E 74 94 4B 32 8C 06 69 6E 64 70 74 72 94 8C 15 6E 75 6D 70 79 2E 63 6F 72 65 2E 6D 75 6C 74 69 61 72 72 61 79 94 8C 0C 5F 72 65 63 6F 6E 73 74 72 75 63 74 94 93 94 8C 05 6E 75 6D 70 79 94 8C 07 6E 64 61 72 72 61 79 94 93 94 4B 00 85 94 43 01 62 94 87 94 52 94 28 4B 01 4B 06 85 94 68 0C 8C 05 64 74 79 70 65 94 93 94 8C 02 69 34 94 89 88 87 94 52 94 28 4B 03 8C 01 3C 94 4E 4E 4E 4A FF FF FF FF 4A FF FF FF FF 4B 00 74 94 62 89 43 18 00 00 00 00 04 00 00 00 05 00 00 00 05 00 00 00 05 00 00 00 05 00 00 00 94 74 94 62 8C 07 69 6E 64 69 63 65 73 94 68 0B 68 0E 4B 00 85 94 68 10 87 94 52 94 28 4B 01 4B 05 85 94 68 18 89 43 14 01 00 00 00 02 00 00 00 03 00 00 00 04 00 00 00 02 00 00 00 94 74 94 62 8C 04 64 61 74 61 94 68 0B 68 0E 4B 00 85 94 68 10 87 94 52 94 28 4B 01 4B 05 85 94 68 15 8C 02 69 38 94 89 88 87 94 52 94 28 4B 03 68 19 4E 4E 4E 4A FF FF FF FF 4A FF FF FF FF 4B 00 74 94 62 89 43 28 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 94 74 94 62 8C 15 5F 68 61 73 5F 63 61 6E 6F 6E 69 63 61 6C 5F 66 6F 72 6D 61 74 94 88 8C 13 5F 68 61 73 5F 73 6F 72 74 65 64 5F 69 6E 64 69 63 65 73 94 88 75 62 2E]                                                                                                                                                                                    |\n",
            "+------------+--------------+-----------+--------------------+------------------+------------------------+--------------------+------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "Completed Stage 3: Creating CSR matrices\n",
            "\n",
            "+---------+--------+-----------+-------------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+--------------------+----------------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+------------------+-------------------------+-------------------------+--------------------------+---------------------+---------------------+----------------------+----------------------------+-------------------+---------------------+------------------------------+----------------------------------+\n",
            "|numStages|numTasks|elapsedTime|stageDuration|executorRunTime|executorCpuTime|executorDeserializeTime|executorDeserializeCpuTime|resultSerializationTime|jvmGCTime|shuffleFetchWaitTime|shuffleWriteTime|resultSize|diskBytesSpilled|memoryBytesSpilled|peakExecutionMemory|recordsRead|bytesRead|recordsWritten|bytesWritten|shuffleRecordsRead|shuffleTotalBlocksFetched|shuffleLocalBlocksFetched|shuffleRemoteBlocksFetched|shuffleTotalBytesRead|shuffleLocalBytesRead|shuffleRemoteBytesRead|shuffleRemoteBytesReadToDisk|shuffleBytesWritten|shuffleRecordsWritten|stage_name                    |dataset                           |\n",
            "+---------+--------+-----------+-------------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+--------------------+----------------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+------------------+-------------------------+-------------------------+--------------------------+---------------------+---------------------+----------------------+----------------------------+-------------------+---------------------+------------------------------+----------------------------------+\n",
            "|13       |29      |3637       |2628         |2553           |586            |251                    |176                       |0                      |0        |0                   |27              |54480     |0               |0                 |335872000          |15         |16856    |0             |0           |166               |26                       |26                       |0                         |12323                |12323                |0                     |0                           |8954               |122                  |Stage 3: Creating CSR matrices|/content/datasets/dataset_config_1|\n",
            "+---------+--------+-----------+-------------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+--------------------+----------------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+------------------+-------------------------+-------------------------+--------------------------+---------------------+---------------------+----------------------+----------------------------+-------------------+---------------------+------------------------------+----------------------------------+\n",
            "\n",
            "Verifying creation of unique_members_bucket column\n",
            "+--------------+---------------------+\n",
            "|unique_members|unique_members_bucket|\n",
            "+--------------+---------------------+\n",
            "|10            |1                    |\n",
            "|10            |1                    |\n",
            "|6             |0                    |\n",
            "|3             |0                    |\n",
            "|5             |0                    |\n",
            "+--------------+---------------------+\n",
            "\n",
            "Columns in DataFrame before cross join:\n",
            "['community_id', 'unique_members', 'total_calls', 'sum_duration_minutes', 'avg_call_duration', 'duration_25th_percentile', 'median_call_duration', 'duration_75th_percentile', 'csr_matrix', 'unique_members_bucket']\n",
            "This is the bucket df:\n",
            "+------------+--------------+-----------+--------------------+------------------+------------------------+--------------------+------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
            "|community_id|unique_members|total_calls|sum_duration_minutes|avg_call_duration |duration_25th_percentile|median_call_duration|duration_75th_percentile|csr_matrix                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |unique_members_bucket|\n",
            "+------------+--------------+-----------+--------------------+------------------+------------------------+--------------------+------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
            "|1           |10            |14         |1657.0              |118.35714285714286|70.0                    |110.0               |168.0                   |[80 04 95 07 02 00 00 00 00 00 00 8C 11 73 63 69 70 79 2E 73 70 61 72 73 65 2E 5F 63 73 72 94 8C 0A 63 73 72 5F 6D 61 74 72 69 78 94 93 94 29 81 94 7D 94 28 8C 06 5F 73 68 61 70 65 94 4B 05 4B 05 86 94 8C 08 6D 61 78 70 72 69 6E 74 94 4B 32 8C 06 69 6E 64 70 74 72 94 8C 15 6E 75 6D 70 79 2E 63 6F 72 65 2E 6D 75 6C 74 69 61 72 72 61 79 94 8C 0C 5F 72 65 63 6F 6E 73 74 72 75 63 74 94 93 94 8C 05 6E 75 6D 70 79 94 8C 07 6E 64 61 72 72 61 79 94 93 94 4B 00 85 94 43 01 62 94 87 94 52 94 28 4B 01 4B 06 85 94 68 0C 8C 05 64 74 79 70 65 94 93 94 8C 02 69 34 94 89 88 87 94 52 94 28 4B 03 8C 01 3C 94 4E 4E 4E 4A FF FF FF FF 4A FF FF FF FF 4B 00 74 94 62 89 43 18 00 00 00 00 04 00 00 00 07 00 00 00 09 00 00 00 0A 00 00 00 0A 00 00 00 94 74 94 62 8C 07 69 6E 64 69 63 65 73 94 68 0B 68 0E 4B 00 85 94 68 10 87 94 52 94 28 4B 01 4B 0A 85 94 68 18 89 43 28 01 00 00 00 02 00 00 00 03 00 00 00 04 00 00 00 02 00 00 00 03 00 00 00 04 00 00 00 03 00 00 00 04 00 00 00 04 00 00 00 94 74 94 62 8C 04 64 61 74 61 94 68 0B 68 0E 4B 00 85 94 68 10 87 94 52 94 28 4B 01 4B 0A 85 94 68 15 8C 02 69 38 94 89 88 87 94 52 94 28 4B 03 68 19 4E 4E 4E 4A FF FF FF FF 4A FF FF FF FF 4B 00 74 94 62 89 43 50 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 94 74 94 62 8C 15 5F 68 61 73 5F 63 61 6E 6F 6E 69 63 61 6C 5F 66 6F 72 6D 61 74 94 88 8C 13 5F 68 61 73 5F 73 6F 72 74 65 64 5F 69 6E 64 69 63 65 73 94 88 75 62 2E]|1                    |\n",
            "|2           |10            |16         |2258.0              |141.125           |111.0                   |136.0               |175.0                   |[80 04 95 07 02 00 00 00 00 00 00 8C 11 73 63 69 70 79 2E 73 70 61 72 73 65 2E 5F 63 73 72 94 8C 0A 63 73 72 5F 6D 61 74 72 69 78 94 93 94 29 81 94 7D 94 28 8C 06 5F 73 68 61 70 65 94 4B 05 4B 05 86 94 8C 08 6D 61 78 70 72 69 6E 74 94 4B 32 8C 06 69 6E 64 70 74 72 94 8C 15 6E 75 6D 70 79 2E 63 6F 72 65 2E 6D 75 6C 74 69 61 72 72 61 79 94 8C 0C 5F 72 65 63 6F 6E 73 74 72 75 63 74 94 93 94 8C 05 6E 75 6D 70 79 94 8C 07 6E 64 61 72 72 61 79 94 93 94 4B 00 85 94 43 01 62 94 87 94 52 94 28 4B 01 4B 06 85 94 68 0C 8C 05 64 74 79 70 65 94 93 94 8C 02 69 34 94 89 88 87 94 52 94 28 4B 03 8C 01 3C 94 4E 4E 4E 4A FF FF FF FF 4A FF FF FF FF 4B 00 74 94 62 89 43 18 00 00 00 00 04 00 00 00 07 00 00 00 09 00 00 00 0A 00 00 00 0A 00 00 00 94 74 94 62 8C 07 69 6E 64 69 63 65 73 94 68 0B 68 0E 4B 00 85 94 68 10 87 94 52 94 28 4B 01 4B 0A 85 94 68 18 89 43 28 01 00 00 00 02 00 00 00 03 00 00 00 04 00 00 00 02 00 00 00 03 00 00 00 04 00 00 00 03 00 00 00 04 00 00 00 04 00 00 00 94 74 94 62 8C 04 64 61 74 61 94 68 0B 68 0E 4B 00 85 94 68 10 87 94 52 94 28 4B 01 4B 0A 85 94 68 15 8C 02 69 38 94 89 88 87 94 52 94 28 4B 03 68 19 4E 4E 4E 4A FF FF FF FF 4A FF FF FF FF 4B 00 74 94 62 89 43 50 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 94 74 94 62 8C 15 5F 68 61 73 5F 63 61 6E 6F 6E 69 63 61 6C 5F 66 6F 72 6D 61 74 94 88 8C 13 5F 68 61 73 5F 73 6F 72 74 65 64 5F 69 6E 64 69 63 65 73 94 88 75 62 2E]|1                    |\n",
            "+------------+--------------+-----------+--------------------+------------------+------------------------+--------------------+------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
            "\n",
            "+--------------+--------------+--------------------+--------+----------------+\n",
            "|community_id_1|community_id_2|frobenius_similarity|deltacon|final_similarity|\n",
            "+--------------+--------------+--------------------+--------+----------------+\n",
            "|1             |2             |1.0                 |1.0     |1.0             |\n",
            "+--------------+--------------+--------------------+--------+----------------+\n",
            "\n",
            "This is the bucket df:\n",
            "+------------+--------------+-----------+--------------------+-----------------+------------------------+--------------------+------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
            "|community_id|unique_members|total_calls|sum_duration_minutes|avg_call_duration|duration_25th_percentile|median_call_duration|duration_75th_percentile|csr_matrix                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |unique_members_bucket|\n",
            "+------------+--------------+-----------+--------------------+-----------------+------------------------+--------------------+------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
            "|3           |6             |8          |899.0               |112.375          |51.0                    |109.0               |157.0                   |[80 04 95 D3 01 00 00 00 00 00 00 8C 11 73 63 69 70 79 2E 73 70 61 72 73 65 2E 5F 63 73 72 94 8C 0A 63 73 72 5F 6D 61 74 72 69 78 94 93 94 29 81 94 7D 94 28 8C 06 5F 73 68 61 70 65 94 4B 04 4B 04 86 94 8C 08 6D 61 78 70 72 69 6E 74 94 4B 32 8C 06 69 6E 64 70 74 72 94 8C 15 6E 75 6D 70 79 2E 63 6F 72 65 2E 6D 75 6C 74 69 61 72 72 61 79 94 8C 0C 5F 72 65 63 6F 6E 73 74 72 75 63 74 94 93 94 8C 05 6E 75 6D 70 79 94 8C 07 6E 64 61 72 72 61 79 94 93 94 4B 00 85 94 43 01 62 94 87 94 52 94 28 4B 01 4B 05 85 94 68 0C 8C 05 64 74 79 70 65 94 93 94 8C 02 69 34 94 89 88 87 94 52 94 28 4B 03 8C 01 3C 94 4E 4E 4E 4A FF FF FF FF 4A FF FF FF FF 4B 00 74 94 62 89 43 14 00 00 00 00 03 00 00 00 05 00 00 00 06 00 00 00 06 00 00 00 94 74 94 62 8C 07 69 6E 64 69 63 65 73 94 68 0B 68 0E 4B 00 85 94 68 10 87 94 52 94 28 4B 01 4B 06 85 94 68 18 89 43 18 01 00 00 00 02 00 00 00 03 00 00 00 02 00 00 00 03 00 00 00 03 00 00 00 94 74 94 62 8C 04 64 61 74 61 94 68 0B 68 0E 4B 00 85 94 68 10 87 94 52 94 28 4B 01 4B 06 85 94 68 15 8C 02 69 38 94 89 88 87 94 52 94 28 4B 03 68 19 4E 4E 4E 4A FF FF FF FF 4A FF FF FF FF 4B 00 74 94 62 89 43 30 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 94 74 94 62 8C 15 5F 68 61 73 5F 63 61 6E 6F 6E 69 63 61 6C 5F 66 6F 72 6D 61 74 94 88 8C 13 5F 68 61 73 5F 73 6F 72 74 65 64 5F 69 6E 64 69 63 65 73 94 88 75 62 2E]|0                    |\n",
            "|4           |3             |4          |508.0               |127.0            |32.0                    |92.0                |192.0                   |[80 04 95 AB 01 00 00 00 00 00 00 8C 11 73 63 69 70 79 2E 73 70 61 72 73 65 2E 5F 63 73 72 94 8C 0A 63 73 72 5F 6D 61 74 72 69 78 94 93 94 29 81 94 7D 94 28 8C 06 5F 73 68 61 70 65 94 4B 03 4B 03 86 94 8C 08 6D 61 78 70 72 69 6E 74 94 4B 32 8C 06 69 6E 64 70 74 72 94 8C 15 6E 75 6D 70 79 2E 63 6F 72 65 2E 6D 75 6C 74 69 61 72 72 61 79 94 8C 0C 5F 72 65 63 6F 6E 73 74 72 75 63 74 94 93 94 8C 05 6E 75 6D 70 79 94 8C 07 6E 64 61 72 72 61 79 94 93 94 4B 00 85 94 43 01 62 94 87 94 52 94 28 4B 01 4B 04 85 94 68 0C 8C 05 64 74 79 70 65 94 93 94 8C 02 69 34 94 89 88 87 94 52 94 28 4B 03 8C 01 3C 94 4E 4E 4E 4A FF FF FF FF 4A FF FF FF FF 4B 00 74 94 62 89 43 10 00 00 00 00 02 00 00 00 03 00 00 00 03 00 00 00 94 74 94 62 8C 07 69 6E 64 69 63 65 73 94 68 0B 68 0E 4B 00 85 94 68 10 87 94 52 94 28 4B 01 4B 03 85 94 68 18 89 43 0C 01 00 00 00 02 00 00 00 02 00 00 00 94 74 94 62 8C 04 64 61 74 61 94 68 0B 68 0E 4B 00 85 94 68 10 87 94 52 94 28 4B 01 4B 03 85 94 68 15 8C 02 69 38 94 89 88 87 94 52 94 28 4B 03 68 19 4E 4E 4E 4A FF FF FF FF 4A FF FF FF FF 4B 00 74 94 62 89 43 18 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 94 74 94 62 8C 15 5F 68 61 73 5F 63 61 6E 6F 6E 69 63 61 6C 5F 66 6F 72 6D 61 74 94 88 8C 13 5F 68 61 73 5F 73 6F 72 74 65 64 5F 69 6E 64 69 63 65 73 94 88 75 62 2E]                                                                                                                        |0                    |\n",
            "|5           |5             |8          |830.0               |103.75           |73.0                    |100.0               |125.0                   |[80 04 95 CB 01 00 00 00 00 00 00 8C 11 73 63 69 70 79 2E 73 70 61 72 73 65 2E 5F 63 73 72 94 8C 0A 63 73 72 5F 6D 61 74 72 69 78 94 93 94 29 81 94 7D 94 28 8C 06 5F 73 68 61 70 65 94 4B 05 4B 05 86 94 8C 08 6D 61 78 70 72 69 6E 74 94 4B 32 8C 06 69 6E 64 70 74 72 94 8C 15 6E 75 6D 70 79 2E 63 6F 72 65 2E 6D 75 6C 74 69 61 72 72 61 79 94 8C 0C 5F 72 65 63 6F 6E 73 74 72 75 63 74 94 93 94 8C 05 6E 75 6D 70 79 94 8C 07 6E 64 61 72 72 61 79 94 93 94 4B 00 85 94 43 01 62 94 87 94 52 94 28 4B 01 4B 06 85 94 68 0C 8C 05 64 74 79 70 65 94 93 94 8C 02 69 34 94 89 88 87 94 52 94 28 4B 03 8C 01 3C 94 4E 4E 4E 4A FF FF FF FF 4A FF FF FF FF 4B 00 74 94 62 89 43 18 00 00 00 00 04 00 00 00 05 00 00 00 05 00 00 00 05 00 00 00 05 00 00 00 94 74 94 62 8C 07 69 6E 64 69 63 65 73 94 68 0B 68 0E 4B 00 85 94 68 10 87 94 52 94 28 4B 01 4B 05 85 94 68 18 89 43 14 01 00 00 00 02 00 00 00 03 00 00 00 04 00 00 00 02 00 00 00 94 74 94 62 8C 04 64 61 74 61 94 68 0B 68 0E 4B 00 85 94 68 10 87 94 52 94 28 4B 01 4B 05 85 94 68 15 8C 02 69 38 94 89 88 87 94 52 94 28 4B 03 68 19 4E 4E 4E 4A FF FF FF FF 4A FF FF FF FF 4B 00 74 94 62 89 43 28 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 01 00 00 00 00 00 00 00 94 74 94 62 8C 15 5F 68 61 73 5F 63 61 6E 6F 6E 69 63 61 6C 5F 66 6F 72 6D 61 74 94 88 8C 13 5F 68 61 73 5F 73 6F 72 74 65 64 5F 69 6E 64 69 63 65 73 94 88 75 62 2E]                        |0                    |\n",
            "+------------+--------------+-----------+--------------------+-----------------+------------------------+--------------------+------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------+\n",
            "\n",
            "+--------------+--------------+--------------------+------------------+-------------------+\n",
            "|community_id_1|community_id_2|frobenius_similarity|deltacon          |final_similarity   |\n",
            "+--------------+--------------+--------------------+------------------+-------------------+\n",
            "|3             |4             |0.36602540378443865 |0.5311989129794585|0.44861215838194857|\n",
            "|3             |5             |0.36602540378443865 |0.5628233896933225|0.4644243967388806 |\n",
            "|4             |5             |0.4142135623730951  |0.7075538987884525|0.5608837305807738 |\n",
            "+--------------+--------------+--------------------+------------------+-------------------+\n",
            "\n",
            "Starting Stage 5: Export groups results\n",
            "Preview of the similarity groups output:\n",
            "\n",
            "Group 1:\n",
            "Community 2:\n",
            "1002, 1003, 2401012242, 2401012322\n",
            "1002, 1003, 2401010449, 2401010625\n",
            "1002, 1004, 2401012148, 2401012231\n",
            "1000, 1001, 2401010316, 2401010509\n",
            "1000, 1001, 2401010822, 2401010858\n",
            "1000, 1002, 2401011456, 2401011647\n",
            "1000, 1003, 2401011102, 2401011237\n",
            "1000, 1003, 2401010938, 2401011133\n",
            "1000, 1004, 2401010314, 2401010502\n",
            "1000, 1004, 2401010733, 2401010840\n",
            "1003, 1004, 2401010005, 2401010151\n",
            "1001, 1002, 2401011435, 2401011622\n",
            "1001, 1002, 2401011003, 2401011132\n",
            "1001, 1003, 2401010252, 2401010410\n",
            "1001, 1003, 2401010756, 2401010831\n",
            "1001, 1004, 2401010904, 2401010944\n",
            "Community 1:\n",
            "2, 3, 2401010028, 2401010120\n",
            "2, 4, 2401011525, 2401011635\n",
            "1, 2, 2401010002, 2401010051\n",
            "1, 3, 2401012251, 2401012354\n",
            "1, 4, 2401011242, 2401011402\n",
            "1, 4, 2401011034, 2401011104\n",
            "3, 4, 2401011549, 2401011725\n",
            "0, 1, 2401011955, 2401012118\n",
            "0, 2, 2401012029, 2401012123\n",
            "0, 2, 2401010039, 2401010141\n",
            "0, 3, 2401011915, 2401012103\n",
            "0, 3, 2401010843, 2401011043\n",
            "0, 4, 2401012029, 2401012205\n",
            "0, 4, 2401010757, 2401010909\n",
            "\n",
            "Group 2:\n",
            "Community 4:\n",
            "3001, 3002, 2401010225, 2401010257\n",
            "3000, 3001, 2401011736, 2401011908\n",
            "3000, 3002, 2401010832, 2401011025\n",
            "3000, 3002, 2401010713, 2401010832\n",
            "Community 5:\n",
            "4000, 4001, 2401012338, 2401020048\n",
            "4000, 4001, 2401012331, 2401020039\n",
            "4000, 4002, 2401011043, 2401011156\n",
            "4000, 4003, 2401011532, 2401011609\n",
            "4000, 4003, 2401012305, 2401020008\n",
            "4000, 4004, 2401010238, 2401010309\n",
            "4001, 4002, 2401011522, 2401011600\n",
            "4001, 4002, 2401010910, 2401011037\n",
            "\n",
            "Group 3:\n",
            "Community 3:\n",
            "2000, 2001, 2401012125, 2401012241\n",
            "2000, 2002, 2401012311, 2401020110\n",
            "2000, 2002, 2401012244, 2401012322\n",
            "2000, 2003, 2401012037, 2401012226\n",
            "2002, 2003, 2401010902, 2401010953\n",
            "2001, 2002, 2401012350, 2401020035\n",
            "2001, 2002, 2401010143, 2401010337\n",
            "2001, 2003, 2401010458, 2401010529\n",
            "\n",
            "Exported similarity groups to: dataset_config_1_similarity_groups.txt\n",
            "Completed Stage 5: Export groups results\n",
            "\n",
            "+---------+--------+-----------+-------------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+--------------------+----------------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+------------------+-------------------------+-------------------------+--------------------------+---------------------+---------------------+----------------------+----------------------------+-------------------+---------------------+------------------------------+----------------------------------+\n",
            "|numStages|numTasks|elapsedTime|stageDuration|executorRunTime|executorCpuTime|executorDeserializeTime|executorDeserializeCpuTime|resultSerializationTime|jvmGCTime|shuffleFetchWaitTime|shuffleWriteTime|resultSize|diskBytesSpilled|memoryBytesSpilled|peakExecutionMemory|recordsRead|bytesRead|recordsWritten|bytesWritten|shuffleRecordsRead|shuffleTotalBlocksFetched|shuffleLocalBlocksFetched|shuffleRemoteBlocksFetched|shuffleTotalBytesRead|shuffleLocalBytesRead|shuffleRemoteBytesRead|shuffleRemoteBytesReadToDisk|shuffleBytesWritten|shuffleRecordsWritten|stage_name                    |dataset                           |\n",
            "+---------+--------+-----------+-------------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+--------------------+----------------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+------------------+-------------------------+-------------------------+--------------------------+---------------------+---------------------+----------------------+----------------------------+-------------------+---------------------+------------------------------+----------------------------------+\n",
            "|65       |417     |219041     |291696       |170537         |4100           |6916                   |4146                      |17                     |148      |0                   |52              |388695    |0               |0                 |273940752          |468        |626957   |2             |743         |84                |79                       |79                       |0                         |15451                |15451                |0                     |0                           |13267              |77                   |Stage 5: Export groups results|/content/datasets/dataset_config_1|\n",
            "+---------+--------+-----------+-------------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+--------------------+----------------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+------------------+-------------------------+-------------------------+--------------------------+---------------------+---------------------+----------------------+----------------------------+-------------------+---------------------+------------------------------+----------------------------------+\n",
            "\n",
            "Starting to process 2 dataset with the following params: \n",
            "num_communities                                               10\n",
            "community_size_range                                      (5, 7)\n",
            "calls_per_connection_range                                (1, 3)\n",
            "duration_range                                         (30, 180)\n",
            "density                                                      0.4\n",
            "num_samples                                                  100\n",
            "dataset_name                                    dataset_config_2\n",
            "csv_filename                  /content/datasets/dataset_config_2\n",
            "Name: 1, dtype: object\n",
            "Starting Stage 1: Reading the calls dataset\n",
            "The following dataframe has been read from the CSV file:\n",
            "+-------+-------+----------+----------+----------------------+\n",
            "|Client1|Client2|Start_Time|  End_Time|total_duration_minutes|\n",
            "+-------+-------+----------+----------+----------------------+\n",
            "|      0|      1|2401011632|2401011822|                 252.0|\n",
            "|      0|      1|2401010458|2401010613|                 252.0|\n",
            "|      0|      1|2401010603|2401010710|                 252.0|\n",
            "|      0|      2|2401010727|2401010828|                 285.0|\n",
            "|      0|      2|2401011133|2401011217|                 285.0|\n",
            "|      0|      2|2401011323|2401011623|                 285.0|\n",
            "|      0|      3|2401012045|2401012319|                 368.0|\n",
            "|      0|      3|2401010204|2401010308|                 368.0|\n",
            "|      0|      3|2401010952|2401011222|                 368.0|\n",
            "|      0|      4|2401011838|2401011936|                 240.0|\n",
            "|      0|      4|2401010346|2401010429|                 240.0|\n",
            "|      0|      4|2401011800|2401012019|                 240.0|\n",
            "|      0|      5|2401011058|2401011317|                 139.0|\n",
            "|      0|      6|2401011901|2401011949|                  86.0|\n",
            "|      0|      6|2401011025|2401011103|                  86.0|\n",
            "|      1|      3|2401011318|2401011535|                 137.0|\n",
            "|      1|      4|2401011053|2401011337|                 164.0|\n",
            "|      1|      5|2401011151|2401011254|                 250.0|\n",
            "|      1|      5|2401011842|2401012005|                 250.0|\n",
            "|      1|      5|2401010400|2401010544|                 250.0|\n",
            "+-------+-------+----------+----------+----------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Completed Stage 1: Reading the calls dataset\n",
            "\n",
            "+---------+--------+-----------+-------------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+--------------------+----------------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+------------------+-------------------------+-------------------------+--------------------------+---------------------+---------------------+----------------------+----------------------------+-------------------+---------------------+----------------------------------+----------------------------------+\n",
            "|numStages|numTasks|elapsedTime|stageDuration|executorRunTime|executorCpuTime|executorDeserializeTime|executorDeserializeCpuTime|resultSerializationTime|jvmGCTime|shuffleFetchWaitTime|shuffleWriteTime|resultSize|diskBytesSpilled|memoryBytesSpilled|peakExecutionMemory|recordsRead|bytesRead|recordsWritten|bytesWritten|shuffleRecordsRead|shuffleTotalBlocksFetched|shuffleLocalBlocksFetched|shuffleRemoteBlocksFetched|shuffleTotalBytesRead|shuffleLocalBytesRead|shuffleRemoteBytesRead|shuffleRemoteBytesReadToDisk|shuffleBytesWritten|shuffleRecordsWritten|stage_name                        |dataset                           |\n",
            "+---------+--------+-----------+-------------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+--------------------+----------------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+------------------+-------------------------+-------------------------+--------------------------+---------------------+---------------------+----------------------+----------------------------+-------------------+---------------------+----------------------------------+----------------------------------+\n",
            "|5        |5       |2058       |1418         |1312           |198            |36                     |18                        |0                      |0        |0                   |5               |6540      |0               |0                 |67371008           |223        |12236    |0             |0           |48                |1                        |1                        |0                         |879                  |879                  |0                     |0                           |879                |48                   |Stage 1: Reading the calls dataset|/content/datasets/dataset_config_2|\n",
            "+---------+--------+-----------+-------------+---------------+---------------+-----------------------+--------------------------+-----------------------+---------+--------------------+----------------+----------+----------------+------------------+-------------------+-----------+---------+--------------+------------+------------------+-------------------------+-------------------------+--------------------------+---------------------+---------------------+----------------------+----------------------------+-------------------+---------------------+----------------------------------+----------------------------------+\n",
            "\n",
            "Starting Stage 2: Preprocessing and creating the graph\n",
            "\n",
            "Final DataFrame with Sequential Community IDs:\n",
            "+-------+-------+----------------------+------------+--------------+\n",
            "|Client1|Client2|total_duration_minutes|community_id|community_size|\n",
            "+-------+-------+----------------------+------------+--------------+\n",
            "|      0|      4|                 240.0|           1|             7|\n",
            "|      0|      6|                  86.0|           1|             7|\n",
            "|      0|      6|                  86.0|           1|             7|\n",
            "|      0|      5|                 139.0|           1|             7|\n",
            "|      0|      4|                 240.0|           1|             7|\n",
            "|      0|      4|                 240.0|           1|             7|\n",
            "|      0|      3|                 368.0|           1|             7|\n",
            "|      0|      3|                 368.0|           1|             7|\n",
            "|      0|      3|                 368.0|           1|             7|\n",
            "|      0|      2|                 285.0|           1|             7|\n",
            "|      0|      2|                 285.0|           1|             7|\n",
            "|      0|      2|                 285.0|           1|             7|\n",
            "|      0|      1|                 252.0|           1|             7|\n",
            "|      0|      1|                 252.0|           1|             7|\n",
            "|      0|      1|                 252.0|           1|             7|\n",
            "|      3|      6|                  90.0|           1|             7|\n",
            "|      3|      4|                 426.0|           1|             7|\n",
            "|      3|      4|                 426.0|           1|             7|\n",
            "|      3|      4|                 426.0|           1|             7|\n",
            "|      5|      6|                 176.0|           1|             7|\n",
            "+-------+-------+----------------------+------------+--------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "\n",
            "Community Members with Sequential IDs:\n",
            "+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|community_id|members                                                                                                                                                                                                                                                                         |\n",
            "+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|1           |[{0, 6, 86.0}, {0, 5, 139.0}, {0, 4, 240.0}, {0, 2, 285.0}, {0, 1, 252.0}, {0, 3, 368.0}, {1, 3, 137.0}, {1, 6, 169.0}, {1, 5, 250.0}, {1, 4, 164.0}, {2, 3, 271.0}, {2, 5, 93.0}, {2, 6, 164.0}, {3, 6, 90.0}, {3, 4, 426.0}, {4, 6, 263.0}, {5, 6, 176.0}]                    |\n",
            "|2           |[{1000, 1004, 204.0}, {1000, 1003, 92.0}, {1000, 1001, 123.0}, {1001, 1004, 80.0}, {1001, 1003, 147.0}, {1001, 1002, 338.0}, {1002, 1004, 238.0}, {1003, 1004, 274.0}]                                                                                                          |\n",
            "|3           |[{2000, 2005, 128.0}, {2000, 2002, 343.0}, {2000, 2004, 132.0}, {2000, 2001, 221.0}, {2000, 2003, 323.0}, {2001, 2005, 128.0}, {2001, 2004, 416.0}, {2001, 2003, 296.0}, {2001, 2002, 215.0}, {2002, 2005, 222.0}, {2002, 2003, 81.0}, {2003, 2005, 278.0}, {2003, 2004, 128.0}]|\n",
            "|4           |[{3000, 3005, 163.0}, {3000, 3001, 350.0}, {3000, 3004, 330.0}, {3000, 3002, 313.0}, {3001, 3004, 359.0}, {3001, 3005, 116.0}, {3001, 3003, 337.0}, {3001, 3002, 156.0}, {3002, 3005, 161.0}, {3002, 3003, 442.0}]                                                              |\n",
            "+------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "\n",
            "This is the community stats:\n",
            "+------------+--------------+-----------+--------------------+------------------+------------------------+--------------------+------------------------+\n",
            "|community_id|unique_members|total_calls|sum_duration_minutes|avg_call_duration |duration_25th_percentile|median_call_duration|duration_75th_percentile|\n",
            "+------------+--------------+-----------+--------------------+------------------+------------------------+--------------------+------------------------+\n",
            "|1           |17            |35         |8446.0              |241.31428571428572|164.0                   |250.0               |285.0                   |\n",
            "|2           |8             |15         |3154.0              |210.26666666666668|123.0                   |238.0               |274.0                   |\n",
            "|3           |13            |25         |6603.0              |264.12            |215.0                   |278.0               |323.0                   |\n",
            "|4           |10            |25         |7431.0              |297.24            |163.0                   |330.0               |350.0                   |\n",
            "+------------+--------------+-----------+--------------------+------------------+------------------------+--------------------+------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10th cell - Plot the resources usgae of current datasets\n",
        "df_monitor = spark.read.csv(\"stage_metrics\",  header=True)\n",
        "df_monitor = df_monitor.orderBy('stage_name')\n",
        "print(f\"count: {df_monitor.count()}\")\n",
        "df_monitor.show(truncate=False)\n",
        "\n",
        "# Convert the Spark DataFrame to a Pandas DataFrame for plotting\n",
        "pdf_monitor = df_monitor.toPandas()\n",
        "\n",
        "# Select the columns of interest for plotting\n",
        "columns_of_interest = ['stage_name', 'numTasks', 'stageDuration', 'peakExecutionMemory', 'executorCpuTime']\n",
        "pdf_plot = pdf_monitor[columns_of_interest].copy()  # Create a copy to avoid the warning\n",
        "\n",
        "# Format memory and time measurements using .loc\n",
        "pdf_plot.loc[:, 'peakExecutionMemory'] = pdf_plot['peakExecutionMemory'].astype(float) / (1024**3)  # Bytes to GB\n",
        "pdf_plot.loc[:, 'stageDuration'] = pdf_plot['stageDuration'].astype(float) / (1000 * 60)  # ms to minutes\n",
        "pdf_plot.loc[:, 'executorCpuTime'] = pdf_plot['executorCpuTime'].astype(float) / (1000 * 60)  # ms to minutes\n",
        "pdf_plot.loc[:, 'numTasks'] = pdf_plot['numTasks'].astype(int)\n",
        "\n",
        "# Extract stage numbers from stage_name\n",
        "pdf_plot['stage_number'] = pdf_plot['stage_name'].apply(lambda x: int(re.search(r'\\d+', x).group()))\n",
        "\n",
        "# Melt the DataFrame for easier plotting with Seaborn\n",
        "pdf_plot_melted = pd.melt(pdf_plot, id_vars=['stage_name', 'stage_number'], var_name='Metric', value_name='Value')\n",
        "\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "# Create the plot with facets, stage numbers as x-axis, and legend with full stage names\n",
        "g = sns.FacetGrid(pdf_plot_melted, col='Metric',\n",
        "                   height=6, aspect=1.5,\n",
        "                  col_wrap=2, sharey=False, sharex=False)\n",
        "g.map(sns.barplot, 'stage_number', 'Value', palette='hls', hue='stage_name',\n",
        "      data=pdf_plot_melted, dodge=False)  # Pass data argument\n",
        "g.set_xticklabels(pdf_plot['stage_number'].unique(), size=16)\n",
        "g.set_titles(\"{col_name}\", size=18)\n",
        "g.fig.suptitle('Spark Stage Metrics', y=1.02, size=18)\n",
        "g.add_legend(loc='upper right', bbox_to_anchor=(1.2, 0.92))\n",
        "g.legend.set_title('Stage Names', prop={'weight': 'bold', 'size': 22})  # Add title and format it\n",
        "for text in g.legend.get_texts():\n",
        "    text.set_fontsize(18)  # Legend label font size# Set y-axis labels with units and add x-axis label\n",
        "for ax in g.axes.flat:\n",
        "    metric = ax.get_title()\n",
        "    if metric == 'peakExecutionMemory':\n",
        "        ax.set_ylabel('Peak Execution Memory (GB)', size=18)\n",
        "    elif metric in ('stageDuration', 'executorCpuTime'):\n",
        "        ax.set_ylabel('Time (minutes)', size=18)\n",
        "    else:\n",
        "        ax.set_ylabel(metric, size=18)\n",
        "\n",
        "    ax.set_xlabel('Stage Number', size=18)  # Change x-axis label to \"Stage Number\"\n",
        "\n",
        "g.set_yticklabels(fontsize=18)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8_ZLxdC5nDWG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}